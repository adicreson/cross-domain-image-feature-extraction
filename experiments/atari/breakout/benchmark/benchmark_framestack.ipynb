{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This file is for creating the benchmark, with stacked frames."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48ae7a254094dc4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# install relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2917de89eb849f74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!pip install 'gymnasium[atari]'\n",
    "#!pip install 'gymnasium[accept-rom-license]'\n",
    "#!pip install 'opencv-python'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:19.737630565Z",
     "start_time": "2024-03-14T19:10:19.735408790Z"
    }
   },
   "id": "c0ba8aa7682bc76e",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee775089098578a5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 20:10:22.055105: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 20:10:22.055145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 20:10:22.056413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 20:10:22.062893: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 20:10:23.363193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from feature_extraction.callbacks.wandb_reward_logging_callback import WandbRewardLoggingCallback\n",
    "from utils import evaluate_policy\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:25.466068733Z",
     "start_time": "2024-03-14T19:10:19.743060366Z"
    }
   },
   "id": "e9bbbd1ea019cb71",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29738fd8c353a5a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "progress_bar = False\n",
    "train_model = True\n",
    "eval_model = True\n",
    "save_name = \"a2c_breakout_benchmark_framestack\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:25.477519895Z",
     "start_time": "2024-03-14T19:10:25.469442619Z"
    }
   },
   "id": "83aa1191533c44b5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Login to wanb and create a project with config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb1c6636e0cfafaa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33madicreson\u001B[0m (\u001B[33mfeature_extraction\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/maliti/Prog/Global-Feature-Extraction/experiments/atari/breakout/benchmark/wandb/run-20240314_201026-gvwt5tgc</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/gvwt5tgc' target=\"_blank\">derby-cobbler-6</a></strong> to <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/gvwt5tgc' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/gvwt5tgc</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "config = dict(\n",
    "    env_id=\"ALE/Breakout-v5\",\n",
    "    algorithm='PPO',\n",
    "    #Hyperparams\n",
    "    policy=\"CnnPolicy\",\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    n_envs=8,\n",
    "    n_timesteps=10_000,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.1,\n",
    "    clip_range_vf=None,\n",
    "    normalize_advantage=True,\n",
    "    normalize=False,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    use_sde=False,\n",
    "    sde_sample_freq=-1,\n",
    "    rollout_buffer_class=None,\n",
    "    rollout_buffer_kwargs=None,\n",
    "    target_kl=None,\n",
    "    stats_window_size=100,\n",
    "    tensorboard_log=None,\n",
    "    policy_kwargs=None,\n",
    "    verbose=0,\n",
    "    seed=None,\n",
    "    device='auto',\n",
    "    _init_setup_model=True,\n",
    "    env_wrapper='stable_baselines3.common.atari_wrappers.AtariWrapper',\n",
    "    frame_stack=4,\n",
    ")\n",
    "\n",
    "wandb.init(project=save_name, config=config)\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:28.041067219Z",
     "start_time": "2024-03-14T19:10:25.486986345Z"
    }
   },
   "id": "25dc2fd081bc2f08",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create callbacks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b96f4e70a9af239e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "vec_eval_env = make_atari_env(config.env_id, n_envs=config.n_envs)\n",
    "vec_eval_env = VecFrameStack(vec_eval_env, n_stack=config.frame_stack)\n",
    "vec_eval_env = VecTransposeImage(vec_eval_env)\n",
    "\n",
    "# WandbCallback\n",
    "wandb_callback_after_eval = WandbRewardLoggingCallback()\n",
    "\n",
    "# Save best model\n",
    "eval_callback = EvalCallback(vec_eval_env, best_model_save_path=\"./logs/\",\n",
    "                             log_path=\"./logs/\", eval_freq=max(500 // config.n_envs, 1), callback_after_eval=wandb_callback_after_eval,\n",
    "                             deterministic=True, render=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:28.963594346Z",
     "start_time": "2024-03-14T19:10:28.047068512Z"
    }
   },
   "id": "3f47fb1c3e907b7a",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create vectorized env and stack frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b15b4e06d109cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vec_train_env = make_atari_env(config.env_id, n_envs=config.n_envs)\n",
    "# Frame-stacking with 4 frames\n",
    "vec_train_env = VecFrameStack(vec_train_env, n_stack=config.frame_stack)\n",
    "vec_train_env = VecTransposeImage(vec_train_env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:10:29.851356956Z",
     "start_time": "2024-03-14T19:10:29.073305643Z"
    }
   },
   "id": "6228fe1f1f1916f5",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create model, learn and save with wandb"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92e71946af36c557"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=496, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 251.60 +/- 20.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 252      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=992, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 251.20 +/- 23.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992      |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1488, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 217.00 +/- 34.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 1.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1488     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1984, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 244.20 +/- 48.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 2        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1984     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 228.40 +/- 46.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 1.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2976, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 251.20 +/- 16.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2976     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3472, episode_reward=1.80 +/- 1.47\n",
      "Episode length: 236.00 +/- 59.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3472     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3968, episode_reward=2.80 +/- 0.75\n",
      "Episode length: 278.20 +/- 41.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 278      |\n",
      "|    mean_reward     | 2.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3968     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4464, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 243.00 +/- 47.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | 2        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4464     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 254.00 +/- 21.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 254      |\n",
      "|    mean_reward     | 2.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5456, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 249.00 +/- 18.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5456     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5952, episode_reward=1.20 +/- 0.98\n",
      "Episode length: 210.00 +/- 35.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 210      |\n",
      "|    mean_reward     | 1.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5952     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6448, episode_reward=2.40 +/- 0.49\n",
      "Episode length: 253.20 +/- 29.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 253      |\n",
      "|    mean_reward     | 2.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6448     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6944, episode_reward=1.80 +/- 1.83\n",
      "Episode length: 238.00 +/- 79.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 238      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6944     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7440, episode_reward=1.80 +/- 0.98\n",
      "Episode length: 233.00 +/- 36.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 233      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7440     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7936, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 236.80 +/- 72.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7936     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8432, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 222.40 +/- 36.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 222      |\n",
      "|    mean_reward     | 1.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8432     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8928, episode_reward=2.00 +/- 1.79\n",
      "Episode length: 245.40 +/- 75.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 245      |\n",
      "|    mean_reward     | 2        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8928     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9424, episode_reward=2.00 +/- 1.26\n",
      "Episode length: 253.60 +/- 51.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 254      |\n",
      "|    mean_reward     | 2        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9424     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9920, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 219.80 +/- 43.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 220      |\n",
      "|    mean_reward     | 1.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9920     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10416, episode_reward=2.00 +/- 1.10\n",
      "Episode length: 245.60 +/- 47.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 246      |\n",
      "|    mean_reward     | 2        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10416    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10912, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 246.40 +/- 27.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 246      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11408, episode_reward=2.20 +/- 0.40\n",
      "Episode length: 252.20 +/- 19.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 252      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11408    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11904, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 226.00 +/- 54.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 1.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11904    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12400, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 217.40 +/- 30.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 1.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12400    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12896, episode_reward=2.60 +/- 0.49\n",
      "Episode length: 267.20 +/- 22.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 2.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12896    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13392, episode_reward=1.40 +/- 1.20\n",
      "Episode length: 223.20 +/- 47.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 1.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13888, episode_reward=1.80 +/- 1.60\n",
      "Episode length: 238.60 +/- 63.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 239      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13888    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14384, episode_reward=2.60 +/- 0.49\n",
      "Episode length: 276.00 +/- 23.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 276      |\n",
      "|    mean_reward     | 2.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14880, episode_reward=1.80 +/- 0.98\n",
      "Episode length: 241.20 +/- 40.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14880    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15376, episode_reward=1.80 +/- 0.98\n",
      "Episode length: 236.80 +/- 40.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15376    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15872, episode_reward=1.80 +/- 0.98\n",
      "Episode length: 237.40 +/- 37.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 1.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16368, episode_reward=1.00 +/- 1.26\n",
      "Episode length: 204.60 +/- 54.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 1        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16368    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 1.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    ppo_params_keys = [\n",
    "        'policy', 'learning_rate', 'n_steps', 'batch_size', 'n_epochs',\n",
    "        'gamma', 'gae_lambda', 'clip_range', 'clip_range_vf', 'normalize_advantage',\n",
    "        'ent_coef', 'vf_coef', 'max_grad_norm', 'use_sde', 'sde_sample_freq',\n",
    "        'rollout_buffer_class', 'rollout_buffer_kwargs', 'target_kl',\n",
    "        'stats_window_size', 'tensorboard_log', 'policy_kwargs', 'verbose',\n",
    "        'seed', 'device', '_init_setup_model'\n",
    "    ]   \n",
    "    \n",
    "    # Step 2: Filter the config dictionary to extract only the hyperparameters for PPO\n",
    "    ppo_hyperparams = {key: config[key] for key in ppo_params_keys if key in config}\n",
    "    \n",
    "    # Step 3: Unpack the filtered hyperparameters dictionary into the PPO constructor\n",
    "    model = PPO(**ppo_hyperparams, env=vec_train_env)\n",
    "    \n",
    "    model = PPO(config.policy, vec_train_env, verbose=1)\n",
    "    model.learn(total_timesteps=config.n_timesteps, callback=eval_callback, progress_bar=progress_bar)\n",
    "    model.save(save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:13:53.312642980Z",
     "start_time": "2024-03-14T19:10:29.858983116Z"
    }
   },
   "id": "28259fafc81561b7",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and evaluate Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "163eabb49c4390ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation\n",
      "1.5 1.5\n"
     ]
    }
   ],
   "source": [
    "if eval_model:\n",
    "    model = PPO.load(\"logs/best_model.zip\", env=vec_eval_env)\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=2, render=False, fps=30)\n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:13:54.159976665Z",
     "start_time": "2024-03-14T19:13:53.357345118Z"
    }
   },
   "id": "5c082cbec6e990a3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wrap up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0b7708e4c57b2c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2516e3af0c344cf788dd2d9c7ef80a7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▆▆▂▅▃▆▄█▅▆▆▂▆▄▄▄▃▅▅▃▅▆▆▃▃▇▃▄▇▄▄▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>1.0</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">derby-cobbler-6</strong> at: <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/gvwt5tgc' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/gvwt5tgc</a><br/> View job at <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0ODgxNDIzNg==/version_details/v2' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0ODgxNDIzNg==/version_details/v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240314_201026-gvwt5tgc/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T19:14:01.503694561Z",
     "start_time": "2024-03-14T19:13:54.159888199Z"
    }
   },
   "id": "9a4ee3b9c8621646",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
