{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This file is for creating the benchmark, with stacked frames."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48ae7a254094dc4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# import relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee775089098578a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, EveryNTimesteps\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "\n",
    "from feature_extraction.callbacks.wandb_reward_logging_callback import WandbRewardLoggingCallback\n",
    "from utils import evaluate_policy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:09:56.878480194Z",
     "start_time": "2024-03-14T20:09:56.821811814Z"
    }
   },
   "id": "e9bbbd1ea019cb71",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29738fd8c353a5a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "progress_bar = False\n",
    "train_model = True\n",
    "eval_model = True\n",
    "save_name = \"a2c_breakout_benchmark_framestack\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:09:56.881448407Z",
     "start_time": "2024-03-14T20:09:56.879037697Z"
    }
   },
   "id": "83aa1191533c44b5",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Login to wanb and create a project with config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb1c6636e0cfafaa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.4"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/maliti/Prog/Global-Feature-Extraction/experiments/atari/breakout/benchmark/wandb/run-20240314_210956-xx6wstke</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/xx6wstke' target=\"_blank\">raspberry-cake-7</a></strong> to <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/xx6wstke' target=\"_blank\">https://wandb.ai/feature_extraction/a2c_breakout_benchmark_framestack/runs/xx6wstke</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "config = dict(\n",
    "    env_id=\"ALE/Breakout-v5\",\n",
    "    algorithm='PPO',\n",
    "    #Hyperparams\n",
    "    policy=\"CnnPolicy\",\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    n_envs=8,\n",
    "    n_timesteps=10_000,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.1,\n",
    "    clip_range_vf=None,\n",
    "    normalize_advantage=True,\n",
    "    normalize=False,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    use_sde=False,\n",
    "    sde_sample_freq=-1,\n",
    "    rollout_buffer_class=None,\n",
    "    rollout_buffer_kwargs=None,\n",
    "    target_kl=None,\n",
    "    stats_window_size=100,\n",
    "    tensorboard_log=None,\n",
    "    policy_kwargs=None,\n",
    "    verbose=0,\n",
    "    seed=None,\n",
    "    device='auto',\n",
    "    _init_setup_model=True,\n",
    "    env_wrapper='stable_baselines3.common.atari_wrappers.AtariWrapper',\n",
    "    frame_stack=4,\n",
    ")\n",
    "\n",
    "wandb.init(project=save_name, config=config)\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:09:57.967427382Z",
     "start_time": "2024-03-14T20:09:56.896666496Z"
    }
   },
   "id": "25dc2fd081bc2f08",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create callbacks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b96f4e70a9af239e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vec_eval_env = make_atari_env(config.env_id, n_envs=config.n_envs)\n",
    "vec_eval_env = VecFrameStack(vec_eval_env, n_stack=config.frame_stack)\n",
    "vec_eval_env = VecTransposeImage(vec_eval_env)\n",
    "\n",
    "# WandbCallback\n",
    "wandb_callback_after_eval = WandbRewardLoggingCallback()\n",
    "\n",
    "# Save best model\n",
    "eval_callback = EvalCallback(vec_eval_env, best_model_save_path=\"./logs/\",\n",
    "                             log_path=\"./logs/\", eval_freq=max(500 // config.n_envs, 1), callback_after_eval=wandb_callback_after_eval,\n",
    "                             deterministic=True, render=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:09:58.906510802Z",
     "start_time": "2024-03-14T20:09:57.954520627Z"
    }
   },
   "id": "3f47fb1c3e907b7a",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create vectorized env and stack frames"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b15b4e06d109cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vec_train_env = make_atari_env(config.env_id, n_envs=config.n_envs)\n",
    "# Frame-stacking with 4 frames\n",
    "vec_train_env = VecFrameStack(vec_train_env, n_stack=config.frame_stack)\n",
    "vec_train_env = VecTransposeImage(vec_train_env)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:09:59.826748311Z",
     "start_time": "2024-03-14T20:09:59.023067952Z"
    }
   },
   "id": "6228fe1f1f1916f5",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create model, learn and save with wandb"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92e71946af36c557"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=496, episode_reward=2.20 +/- 0.75\n",
      "Episode length: 236.80 +/- 26.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 2.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 496      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=992, episode_reward=2.40 +/- 1.02\n",
      "Episode length: 5599.40 +/- 10700.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6e+03  |\n",
      "|    mean_reward     | 2.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 992      |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(config\u001B[38;5;241m.\u001B[39mpolicy, vec_train_env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     18\u001B[0m wandb\u001B[38;5;241m.\u001B[39mwatch(model\u001B[38;5;241m.\u001B[39mpolicy, log\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_timesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m model\u001B[38;5;241m.\u001B[39msave(save_name)\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001B[0m, in \u001B[0;36mPPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[1;32m    308\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    313\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    314\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[0;32m--> 315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 277\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:200\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n\u001B[1;32m    199\u001B[0m callback\u001B[38;5;241m.\u001B[39mupdate_locals(\u001B[38;5;28mlocals\u001B[39m())\n\u001B[0;32m--> 200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_info_buffer(infos)\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001B[0m, in \u001B[0;36mBaseCallback.on_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_calls \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnum_timesteps\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:460\u001B[0m, in \u001B[0;36mEvalCallback._on_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;66;03m# Reset success rate buffer\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_success_buffer \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 460\u001B[0m episode_rewards, episode_lengths \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_policy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    464\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrender\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_episode_rewards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_success_callback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    472\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(episode_rewards, \u001B[38;5;28mlist\u001B[39m)\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:94\u001B[0m, in \u001B[0;36mevaluate_policy\u001B[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m (episode_counts \u001B[38;5;241m<\u001B[39m episode_count_targets)\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m     88\u001B[0m     actions, states \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(\n\u001B[1;32m     89\u001B[0m         observations,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m     90\u001B[0m         state\u001B[38;5;241m=\u001B[39mstates,\n\u001B[1;32m     91\u001B[0m         episode_start\u001B[38;5;241m=\u001B[39mepisode_starts,\n\u001B[1;32m     92\u001B[0m         deterministic\u001B[38;5;241m=\u001B[39mdeterministic,\n\u001B[1;32m     93\u001B[0m     )\n\u001B[0;32m---> 94\u001B[0m     new_observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m     current_rewards \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rewards\n\u001B[1;32m     96\u001B[0m     current_lengths \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \n\u001B[1;32m    202\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[0;32m--> 206\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001B[0m, in \u001B[0;36mVecTransposeImage.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[0;32m---> 97\u001B[0m     observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;66;03m# Transpose the terminal observations\u001B[39;00m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, done \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dones):\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:33\u001B[0m, in \u001B[0;36mVecFrameStack.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     32\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]], np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mndarray, List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]],]:\n\u001B[0;32m---> 33\u001B[0m     observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     observations, infos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstacked_obs\u001B[38;5;241m.\u001B[39mupdate(observations, dones, infos)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observations, rewards, dones, infos\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 58\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/core.py:461\u001B[0m, in \u001B[0;36mWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[1;32m    459\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    460\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/core.py:555\u001B[0m, in \u001B[0;36mRewardWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[1;32m    553\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    554\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 555\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward(reward), terminated, truncated, info\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/core.py:522\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[1;32m    520\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    521\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/core.py:461\u001B[0m, in \u001B[0;36mWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[1;32m    459\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    460\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py:112\u001B[0m, in \u001B[0;36mEpisodicLifeEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AtariStepReturn:\n\u001B[0;32m--> 112\u001B[0m     obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwas_real_done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;66;03m# check current lives, make loss of life terminal,\u001B[39;00m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# then update lives to handle bonus lives\u001B[39;00m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/atari_wrappers.py:178\u001B[0m, in \u001B[0;36mMaxAndSkipEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    176\u001B[0m terminated \u001B[38;5;241m=\u001B[39m truncated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip):\n\u001B[0;32m--> 178\u001B[0m     obs, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m     done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_skip \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/core.py:461\u001B[0m, in \u001B[0;36mWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[1;32m    459\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    460\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 461\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001B[0m, in \u001B[0;36mMonitor.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneeds_reset:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to step environment that needs reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 94\u001B[0m observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(reward))\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/shimmy/atari_env.py:294\u001B[0m, in \u001B[0;36mAtariEnv.step\u001B[0;34m(self, action_ind)\u001B[0m\n\u001B[1;32m    292\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[0;32m--> 294\u001B[0m     reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43male\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    295\u001B[0m is_terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_over(with_truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    296\u001B[0m is_truncated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_truncated()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    ppo_params_keys = [\n",
    "        'policy', 'learning_rate', 'n_steps', 'batch_size', 'n_epochs',\n",
    "        'gamma', 'gae_lambda', 'clip_range', 'clip_range_vf', 'normalize_advantage',\n",
    "        'ent_coef', 'vf_coef', 'max_grad_norm', 'use_sde', 'sde_sample_freq',\n",
    "        'rollout_buffer_class', 'rollout_buffer_kwargs', 'target_kl',\n",
    "        'stats_window_size', 'tensorboard_log', 'policy_kwargs', 'verbose',\n",
    "        'seed', 'device', '_init_setup_model'\n",
    "    ]   \n",
    "    \n",
    "    # Step 2: Filter the config dictionary to extract only the hyperparameters for PPO\n",
    "    ppo_hyperparams = {key: config[key] for key in ppo_params_keys if key in config}\n",
    "    \n",
    "    # Step 3: Unpack the filtered hyperparameters dictionary into the PPO constructor\n",
    "    model = PPO(**ppo_hyperparams, env=vec_train_env)\n",
    "    \n",
    "    model = PPO(config.policy, vec_train_env, verbose=1)\n",
    "    wandb.watch(model.policy, log=\"all\", log_freq=10)\n",
    "    model.learn(total_timesteps=config.n_timesteps, callback=eval_callback, progress_bar=progress_bar)\n",
    "    model.save(save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T20:15:00.820356099Z",
     "start_time": "2024-03-14T20:09:59.830472818Z"
    }
   },
   "id": "28259fafc81561b7",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and evaluate Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "163eabb49c4390ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if eval_model:\n",
    "    model = PPO.load(\"logs/best_model.zip\", env=vec_eval_env)\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=2, render=False, fps=30)\n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T20:15:00.817654304Z"
    }
   },
   "id": "5c082cbec6e990a3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wrap up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0b7708e4c57b2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T20:15:00.817964345Z"
    }
   },
   "id": "9a4ee3b9c8621646",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export model to ONNX"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a56052a0317b554b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Example for creating an ONNX model (should be saved to wandb)\n",
    "dummy_input = torch.randn(1, 4, 84, 84)  # Batch size of 1\n",
    "\n",
    "torch.onnx.export(model.policy,             # Model's policy to export\n",
    "                  dummy_input,              # Example input for the model\n",
    "                  save_name + \".onnx\", # Path to save the ONNX model\n",
    "                  export_params=True,       # Export model parameters\n",
    "                  opset_version=11,         # Set the ONNX version\n",
    "                  do_constant_folding=True, # Optimization\n",
    "                  input_names=['input'],    # Naming the input layer\n",
    "                  output_names=['action_output', 'value_output'], # Naming the output layers\n",
    "                  dynamic_axes={'input': {0: 'batch_size'},    # Handling variable batch sizes\n",
    "                                'action_output': {0: 'batch_size'},\n",
    "                                'value_output': {0: 'batch_size'}})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T20:15:00.818150634Z"
    }
   },
   "id": "218d01884c66d338",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
