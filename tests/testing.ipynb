{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EveryNTimesteps, BaseCallback\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from feature_extraction.wrappers.feature_extraction_observation_wrapper import FeatureExtractionObservationWrapper\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "# wandb API = cceb2653e8e4543a510e4c872213e68ea45cb706\n",
    "# wandb.init(project=\"test-Block_based\", \n",
    "#            config={\"algorithm\": \"PPO\", \n",
    "#                    \"env\": \"ALE/Breakout-v5\", \n",
    "#                    \"feature_extractor\": \"Block-based\", \n",
    "#                    \"n_steps\": 10_000})\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        wandb.log({\"episode_reward\": self.locals[\"infos\"][0][\"episode\"][\"r\"]})\n",
    "        return True\n",
    "freq_checkpoint = EveryNTimesteps(n_steps=500, callback=CustomCallback())\n",
    "\n",
    "\n",
    "env_id = \"ALE/Breakout-v5\"  # Adjusted to a single-environment ID\n",
    "env = gym.make(env_id)\n",
    "env = FeatureExtractionObservationWrapper(env)\n",
    "print(\"Supposed observation space: \", env.observation_space)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "st = time.time()\n",
    "model.learn(total_timesteps=10_000)\n",
    "print(\"Time taken to train 10_000 timelaps: \", time.time() - st)\n",
    "\n",
    "model.save(\"stage1_test_Block\")\n",
    "# 17 minutes to train 10_000 timesteps with PPO(MlpPolicy) and stage feature extractor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "555b0728e42031d4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "756bd1f9600c53e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchinfo import torchinfo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class BlockFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.block_num = 3\n",
    "        stem_stages = 4\n",
    "        stage_num = 4\n",
    "        \n",
    "\n",
    "        # Extract layers up to the desired stage\n",
    "        stages = list(model.children())\n",
    "        print(\"len stages: \", len(stages))\n",
    "        self.stem = stages[:stem_stages]\n",
    "        print(\"len stem: \", len(self.stem))\n",
    "        self.custom_Stages = stages[stem_stages:stem_stages+stage_num]\n",
    "        print(\"len custom stages\", len(self.custom_Stages))\n",
    "        self.custom_Stages[-1] = self.custom_Stages[-1][:self.block_num]\n",
    "        \n",
    "        print(self.custom_Stages)\n",
    "        # # Access the desired stage\n",
    "        # self.stage = self.stages[:(stem_stages+stage_num)]\n",
    "        # \n",
    "        # self.stage[-1] = self.stage[-1][:self.block_num]\n",
    "        # print(\"PRINTING MODEL\")\n",
    "        \n",
    "        \n",
    "        # Access up to the desired bottleneck block within the stage\n",
    "        #self.blocks = nn.Sequential(*list(self.stage.children())[:block_num])\n",
    "        #print(list(*self.blocks))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for stages in self.stem:\n",
    "            x = stages(x)\n",
    "        \n",
    "        for layer in self.custom_Stages:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    block_num = 5 \n",
    "    block_feature_extractor = BlockFeatureExtractor(model)\n",
    "    rand_input = torch.rand(1, 3, 224, 224)\n",
    "    block_output = block_feature_extractor.forward(rand_input)\n",
    "\n",
    "    print(\"Shape of the output from block {}:\".format(block_num), block_output.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d03bf2754ab6b380",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "torchinfo.summary(models.resnet50(pretrained=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1fdd25b547955ac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import torchinfo\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from feature_extraction.feature_extractors.feature_extractor import FeatureExtractor\n",
    "\n",
    "\n",
    "class BlockFeatureExtractor(nn.Module, FeatureExtractor):\n",
    "    def __init__(self, model, num_blocks=1, num_stages=0):\n",
    "        # super(BlockFeatureExtractor, self).__init__()\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        self.Conv2d, self.BatchNorm2d, self.ReLU, self.MaxPool2d = list(model.children())[:4]\n",
    "        # Collecting the layers up to the specified stage and block\n",
    "        self.stages = self._get_block_features(model)\n",
    "        self.stage1, self.stage2, self.stage3, self.stage4 = self.stages\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.output_dim = self.output_dim()\n",
    "        self.freeze_params()\n",
    "\n",
    "\n",
    "    def freeze_params(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def output_dim(self):\n",
    "        dummy_input = torch.rand(1, 3, 224, 224)\n",
    "        output = self.extract_features(dummy_input)\n",
    "        return output.shape\n",
    "\n",
    "    def process_image(self, image):\n",
    "        image_processor = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return image_processor(image)\n",
    "\n",
    "    def _get_block_features(self, fe_model):\n",
    "        sequentials = [None] * 4\n",
    "        blocks = []\n",
    "        current_stages = 0\n",
    "        total_blocks = 0\n",
    "\n",
    "        # Traverse through the layers of the model until the specified stage and block\n",
    "        for name, module in fe_model.named_children():\n",
    "            if not self.num_blocks and not self.num_stages: break\n",
    "            if isinstance(module, nn.Sequential):\n",
    "                if current_stages < self.num_stages or len(module) <= self.num_blocks - total_blocks:\n",
    "                    sequentials[current_stages] = module\n",
    "                    current_stages += 1\n",
    "                    total_blocks += len(module)\n",
    "                    print(f\"Adding a whole stage, stage-{current_stages}, totalblocks: {total_blocks}\")\n",
    "                    continue\n",
    "\n",
    "                if self.num_blocks:\n",
    "                    for idx, block in enumerate(module):\n",
    "                        if total_blocks < self.num_blocks:\n",
    "                            blocks.append(block)\n",
    "                            total_blocks += 1\n",
    "                            print(\n",
    "                                f\"Adding block-{idx + 1} from stage-{current_stages + 1}, totalblocks: {total_blocks}\")\n",
    "\n",
    "                    if len(blocks):\n",
    "                        seq = nn.Sequential(*blocks)\n",
    "                        sequentials[current_stages] = seq\n",
    "\n",
    "                    current_stages += 1\n",
    "                break\n",
    "        return sequentials\n",
    "\n",
    "    # def _get_block_features(self, fe_model):\n",
    "    #     seqs = [None] * 4\n",
    "    #     sequentials = nn.Sequential(*list(model.children())[4:-6+self.num_stages])  # first stage\n",
    "    #     sequentials[-1] = nn.Sequential(*list(sequentials[-1].children())[:self.num_blocks])\n",
    "    #     for idx, seq in enumerate(sequentials):\n",
    "    #         seqs[idx] = seq\n",
    "    #     return seqs\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.Conv2d(x)\n",
    "            x = self.BatchNorm2d(x)\n",
    "            x = self.ReLU(x)\n",
    "            x = self.MaxPool2d(x)\n",
    "            for stage in self.stages[:self.num_stages]:\n",
    "                x = stage(x)\n",
    "        return x\n",
    "\n",
    "    def reduce_dim(self, features):\n",
    "        reduced_features = self.adaptive_avg_pool(features)\n",
    "        return reduced_features.view(features.size(0), -1)\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        processed_image = self.process_image(image)\n",
    "        feature_embeddings = self.forward(processed_image)\n",
    "        reduced_dim = self.reduce_dim(feature_embeddings)\n",
    "        return reduced_dim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = models.resnet50(weights='DEFAULT')\n",
    "    rand_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "    custom_model = nn.Sequential(*list(model.children())[:-4]) # first stage\n",
    "    feature_extractor = BlockFeatureExtractor(model, num_stages=2, num_blocks=7)\n",
    "\n",
    "    output1 = feature_extractor.extract_features(rand_input)\n",
    "    output2 = feature_extractor.reduce_dim(custom_model.forward(feature_extractor.process_image(rand_input)))\n",
    "    # output2 = feature_extractor.extract_features(torch.rand(1, 3, 224, 224))\n",
    "\n",
    "    print(output1.shape, output2.shape)\n",
    "    print(feature_extractor.output_dim)\n",
    "    torchinfo.summary(feature_extractor)\n",
    "\n",
    "    # Check if the outputs are the same\n",
    "    if torch.allclose(output1, output2):\n",
    "        print(\"Outputs of the models are the same.\")\n",
    "    else:\n",
    "        print(\"Outputs of the models are different.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d010f64eb790b311",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    model = models.resnet50(weights='DEFAULT')\n",
    "    \n",
    "    last_model = nn.Sequential(*list(model.children())[:-5]) # first stage\n",
    "    last_model[-1] = nn.Sequential(*list(last_model[-1].children())[:1]) # two blocks\n",
    "    torchinfo.summary(last_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d07a8e2e972f82",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder, DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "\n",
    "config = {\n",
    "    \"total_timesteps\": int(10000),\n",
    "    \"num_envs\": 4,\n",
    "}\n",
    "wandb.init(\n",
    "    project=\"sb3\",\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "    save_code=True,  # optional\n",
    ")\n",
    "\n",
    "env = make_atari_env('BreakoutNoFrameskip-v4', n_envs=config[\"num_envs\"], seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecVideoRecorder(env, \"videos\",\n",
    "    record_video_trigger=lambda x: x % 1000 == 0, video_length=200)  # record videos\n",
    "model = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    n_steps=128,\n",
    "    n_epochs=4,\n",
    "    learning_rate=lambda progression: 2.5e-4 * progression,\n",
    "    ent_coef=0.01,\n",
    "    clip_range=lambda progression: 0.1 * progression,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f\"runs\"\n",
    ")\n",
    "model.learn(\n",
    "    total_timesteps=int(10000),\n",
    "    callback=WandbCallback(\n",
    "        gradient_save_freq=500,\n",
    "        model_save_freq=2000,\n",
    "        model_save_path=f\"models\",\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1d49dbf024dce3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f55bf3e8dd930325"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
