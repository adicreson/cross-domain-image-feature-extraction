{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:18:04.564817311Z",
     "start_time": "2024-01-30T13:18:04.507025893Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!pip install 'gymnasium[atari]'\n",
    "#!pip install 'gymnasium[accept-rom-license]'\n",
    "#!pip install 'opencv-python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ccdfe4bd8ad3ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T00:38:10.881167507Z",
     "start_time": "2024-02-01T00:38:10.868694288Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3 import A2C\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb83b550b73061",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save model every n steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ece19ac2f7962a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:54:05.367038527Z",
     "start_time": "2024-01-30T13:52:53.310666352Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 237      |\n",
      "|    ep_rew_mean        | 1.88     |\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.132    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 236      |\n",
      "|    ep_rew_mean        | 1.9      |\n",
      "| time/                 |          |\n",
      "|    fps                | 204      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.591    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.158   |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 230      |\n",
      "|    ep_rew_mean        | 1.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 210      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.583    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m vec_env \u001B[38;5;241m=\u001B[39m VecFrameStack(vec_env, n_stack\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m)\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m A2C(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCnnPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, vec_env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevent_callback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma2c_breakout\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:201\u001B[0m, in \u001B[0;36mA2C.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[1;32m    194\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    199\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[0;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:299\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    296\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mrecord(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime/total_timesteps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps, exclude\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorboard\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    297\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdump(step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps)\n\u001B[0;32m--> 299\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    301\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:175\u001B[0m, in \u001B[0;36mA2C.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# Optimization step\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 175\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# Clip grad norm\u001B[39;00m\n\u001B[1;32m    178\u001B[0m th\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_grad_norm)\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
    "\n",
    "# this is equivalent to defining CheckpointCallback(save_freq=500)\n",
    "# checkpoint_callback will be triggered every 500 steps\n",
    "checkpoint_on_event = CheckpointCallback(save_freq=1, save_path=\"./logs/\")\n",
    "event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n",
    "\n",
    "vec_env = make_atari_env(\"ALE/Breakout-v5\", n_envs=8, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "vec_env = VecFrameStack(vec_env, n_stack=8)\n",
    "\n",
    "model = A2C(\"CnnPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=50000, callback=event_callback)\n",
    "\n",
    "model.save(\"a2c_breakout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d2f38fdc020283",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55bb6e4be58c44e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T08:15:58.022542710Z",
     "start_time": "2024-02-01T00:38:18.201115363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=2000, episode_reward=2.40 +/- 1.50\n",
      "Episode length: 260.60 +/- 62.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 261      |\n",
      "|    mean_reward        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00879  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 234      |\n",
      "|    ep_rew_mean     | 1.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=3.00 +/- 0.89\n",
      "Episode length: 286.40 +/- 47.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 286      |\n",
      "|    mean_reward        | 3        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0191   |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 240      |\n",
      "|    ep_rew_mean     | 1.99     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=1.60 +/- 1.36\n",
      "Episode length: 230.00 +/- 54.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 1.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00688  |\n",
      "|    value_loss         | 0.00795  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 252      |\n",
      "|    ep_rew_mean     | 2.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=0.80 +/- 0.98\n",
      "Episode length: 199.80 +/- 29.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 200      |\n",
      "|    mean_reward        | 0.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.717   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.00339  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 255      |\n",
      "|    ep_rew_mean     | 2.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 185      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=5.80 +/- 3.43\n",
      "Episode length: 5749.20 +/- 10625.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.75e+03 |\n",
      "|    mean_reward        | 5.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.308    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.205    |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 257      |\n",
      "|    ep_rew_mean     | 2.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=6.40 +/- 3.44\n",
      "Episode length: 361.80 +/- 100.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 362      |\n",
      "|    mean_reward        | 6.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0835  |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0546  |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 290      |\n",
      "|    ep_rew_mean     | 3.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=8.40 +/- 2.15\n",
      "Episode length: 473.40 +/- 94.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 473      |\n",
      "|    mean_reward        | 8.4      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.305   |\n",
      "|    explained_variance | 0.668    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00203  |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 330      |\n",
      "|    ep_rew_mean     | 4.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 93       |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=7.60 +/- 2.65\n",
      "Episode length: 387.00 +/- 59.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 387      |\n",
      "|    mean_reward        | 7.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.0015   |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 372      |\n",
      "|    ep_rew_mean     | 6.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=8.00 +/- 2.68\n",
      "Episode length: 384.20 +/- 57.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 384      |\n",
      "|    mean_reward        | 8        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.153   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.047    |\n",
      "|    value_loss         | 0.0222   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 405      |\n",
      "|    ep_rew_mean     | 7.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 105      |\n",
      "|    iterations      | 900      |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=8.00 +/- 2.45\n",
      "Episode length: 356.80 +/- 64.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 357      |\n",
      "|    mean_reward        | 8        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0344   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 416      |\n",
      "|    ep_rew_mean     | 7.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 110      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=6.60 +/- 3.61\n",
      "Episode length: 5762.60 +/- 10618.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.76e+03 |\n",
      "|    mean_reward        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.322   |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00329 |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 431      |\n",
      "|    ep_rew_mean     | 8.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 1100     |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=11.40 +/- 2.33\n",
      "Episode length: 529.20 +/- 41.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 529      |\n",
      "|    mean_reward        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0856  |\n",
      "|    value_loss         | 0.194    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 435      |\n",
      "|    ep_rew_mean     | 8.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=10.20 +/- 2.14\n",
      "Episode length: 512.40 +/- 59.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 512      |\n",
      "|    mean_reward        | 10.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.65    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.009    |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 449      |\n",
      "|    ep_rew_mean     | 8.7      |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 1300     |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=14.00 +/- 2.28\n",
      "Episode length: 596.60 +/- 24.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 597      |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0304  |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 472      |\n",
      "|    ep_rew_mean     | 9.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=11.40 +/- 2.80\n",
      "Episode length: 558.60 +/- 79.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 559      |\n",
      "|    mean_reward        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.508   |\n",
      "|    explained_variance | 0.476    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.234    |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 482      |\n",
      "|    ep_rew_mean     | 9.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 1500     |\n",
      "|    time_elapsed    | 325      |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=12.00 +/- 4.00\n",
      "Episode length: 586.60 +/- 110.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 587      |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.438   |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0877  |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 496      |\n",
      "|    ep_rew_mean     | 9.99     |\n",
      "| time/              |          |\n",
      "|    fps             | 95       |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=11.60 +/- 3.14\n",
      "Episode length: 561.40 +/- 70.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 561      |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.638   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0371   |\n",
      "|    value_loss         | 0.0921   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 496      |\n",
      "|    ep_rew_mean     | 10.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 98       |\n",
      "|    iterations      | 1700     |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=10.40 +/- 2.58\n",
      "Episode length: 508.40 +/- 51.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 508      |\n",
      "|    mean_reward        | 10.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0222  |\n",
      "|    value_loss         | 0.0505   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 490      |\n",
      "|    ep_rew_mean     | 9.85     |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 1800     |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=9.20 +/- 1.33\n",
      "Episode length: 490.60 +/- 41.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 491      |\n",
      "|    mean_reward        | 9.2      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0468   |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 485      |\n",
      "|    ep_rew_mean     | 9.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 1900     |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=11.60 +/- 2.58\n",
      "Episode length: 564.80 +/- 82.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 565      |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.551   |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0303  |\n",
      "|    value_loss         | 0.0839   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 479      |\n",
      "|    ep_rew_mean     | 9.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 105      |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=11.60 +/- 2.06\n",
      "Episode length: 527.60 +/- 63.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 528      |\n",
      "|    mean_reward        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.389   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0705  |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 489      |\n",
      "|    ep_rew_mean     | 9.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 108      |\n",
      "|    iterations      | 2100     |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=9.80 +/- 3.54\n",
      "Episode length: 5823.20 +/- 10588.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.82e+03 |\n",
      "|    mean_reward        | 9.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 499      |\n",
      "|    ep_rew_mean     | 10.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 93       |\n",
      "|    iterations      | 2200     |\n",
      "|    time_elapsed    | 470      |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=11.20 +/- 2.40\n",
      "Episode length: 548.20 +/- 50.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 548      |\n",
      "|    mean_reward        | 11.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.167   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.00822 |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 497      |\n",
      "|    ep_rew_mean     | 10       |\n",
      "| time/              |          |\n",
      "|    fps             | 95       |\n",
      "|    iterations      | 2300     |\n",
      "|    time_elapsed    | 479      |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=12.40 +/- 2.58\n",
      "Episode length: 584.40 +/- 79.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 584      |\n",
      "|    mean_reward        | 12.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.24    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.0198  |\n",
      "|    value_loss         | 0.0745   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | 10.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 97       |\n",
      "|    iterations      | 2400     |\n",
      "|    time_elapsed    | 489      |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=13.20 +/- 3.19\n",
      "Episode length: 595.00 +/- 68.41\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 595       |\n",
      "|    mean_reward        | 13.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.095    |\n",
      "|    explained_variance | 0.831     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -0.000673 |\n",
      "|    value_loss         | 0.213     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 524      |\n",
      "|    ep_rew_mean     | 10.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 99       |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=12.80 +/- 4.31\n",
      "Episode length: 542.60 +/- 44.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 543      |\n",
      "|    mean_reward        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0951  |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.000193 |\n",
      "|    value_loss         | 0.0715   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 547      |\n",
      "|    ep_rew_mean     | 11.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 101      |\n",
      "|    iterations      | 2600     |\n",
      "|    time_elapsed    | 511      |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=13.20 +/- 2.71\n",
      "Episode length: 593.40 +/- 62.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 593      |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.259   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 0.135    |\n",
      "|    value_loss         | 0.0526   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 537      |\n",
      "|    ep_rew_mean     | 11.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 103      |\n",
      "|    iterations      | 2700     |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=13.80 +/- 1.17\n",
      "Episode length: 632.40 +/- 67.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 632      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.195   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.206   |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 539      |\n",
      "|    ep_rew_mean     | 11.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 105      |\n",
      "|    iterations      | 2800     |\n",
      "|    time_elapsed    | 532      |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=12.00 +/- 4.60\n",
      "Episode length: 595.60 +/- 130.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 596      |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0971  |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.0156   |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 560      |\n",
      "|    ep_rew_mean     | 11.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 2900     |\n",
      "|    time_elapsed    | 542      |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=13.60 +/- 3.01\n",
      "Episode length: 583.20 +/- 111.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 583      |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0825  |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00586  |\n",
      "|    value_loss         | 0.0678   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 563      |\n",
      "|    ep_rew_mean     | 11.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 108      |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 553      |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=12.80 +/- 2.04\n",
      "Episode length: 602.00 +/- 27.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 602      |\n",
      "|    mean_reward        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.00955 |\n",
      "|    value_loss         | 0.0855   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 11.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 109      |\n",
      "|    iterations      | 3100     |\n",
      "|    time_elapsed    | 563      |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=14.60 +/- 2.87\n",
      "Episode length: 5960.20 +/- 10519.92\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.96e+03  |\n",
      "|    mean_reward        | 14.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0811   |\n",
      "|    explained_variance | 0.47      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -0.000857 |\n",
      "|    value_loss         | 0.0679    |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 572      |\n",
      "|    ep_rew_mean     | 12       |\n",
      "| time/              |          |\n",
      "|    fps             | 99       |\n",
      "|    iterations      | 3200     |\n",
      "|    time_elapsed    | 644      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=10.60 +/- 5.43\n",
      "Episode length: 5905.80 +/- 10547.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.91e+03 |\n",
      "|    mean_reward        | 10.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.248   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.00793 |\n",
      "|    value_loss         | 0.0285   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 12.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 90       |\n",
      "|    iterations      | 3300     |\n",
      "|    time_elapsed    | 726      |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=14.40 +/- 2.80\n",
      "Episode length: 696.80 +/- 83.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 697      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.111    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.335    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 597      |\n",
      "|    ep_rew_mean     | 12.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 92       |\n",
      "|    iterations      | 3400     |\n",
      "|    time_elapsed    | 737      |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=12.20 +/- 2.56\n",
      "Episode length: 589.80 +/- 69.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 590      |\n",
      "|    mean_reward        | 12.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0699  |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.004   |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 603      |\n",
      "|    ep_rew_mean     | 13       |\n",
      "| time/              |          |\n",
      "|    fps             | 93       |\n",
      "|    iterations      | 3500     |\n",
      "|    time_elapsed    | 747      |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=11.00 +/- 3.63\n",
      "Episode length: 5910.00 +/- 10545.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.91e+03 |\n",
      "|    mean_reward        | 11       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.206   |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0702   |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 13.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 3600     |\n",
      "|    time_elapsed    | 829      |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=13.40 +/- 3.72\n",
      "Episode length: 606.00 +/- 93.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 606      |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0743  |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00222  |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 624      |\n",
      "|    ep_rew_mean     | 13.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 3700     |\n",
      "|    time_elapsed    | 839      |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=14.20 +/- 3.76\n",
      "Episode length: 607.60 +/- 69.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 608      |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.204   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.0174   |\n",
      "|    value_loss         | 0.066    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 634      |\n",
      "|    ep_rew_mean     | 14.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 3800     |\n",
      "|    time_elapsed    | 849      |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=13.60 +/- 3.50\n",
      "Episode length: 611.80 +/- 30.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 612      |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.119   |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0172   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 640      |\n",
      "|    ep_rew_mean     | 14.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 90       |\n",
      "|    iterations      | 3900     |\n",
      "|    time_elapsed    | 860      |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=15.60 +/- 4.67\n",
      "Episode length: 5959.60 +/- 10520.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.00579  |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 646      |\n",
      "|    ep_rew_mean     | 14.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=12.40 +/- 2.58\n",
      "Episode length: 607.60 +/- 80.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 608       |\n",
      "|    mean_reward        | 12.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.131    |\n",
      "|    explained_variance | 0.718     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -0.000403 |\n",
      "|    value_loss         | 0.0421    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 645      |\n",
      "|    ep_rew_mean     | 14.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 4100     |\n",
      "|    time_elapsed    | 951      |\n",
      "|    total_timesteps | 82000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=13.80 +/- 2.48\n",
      "Episode length: 624.20 +/- 53.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 624      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.163   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.133    |\n",
      "|    value_loss         | 0.0872   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 652      |\n",
      "|    ep_rew_mean     | 14.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 4200     |\n",
      "|    time_elapsed    | 962      |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=13.20 +/- 2.48\n",
      "Episode length: 626.00 +/- 27.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 626      |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0789  |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.0262   |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 652      |\n",
      "|    ep_rew_mean     | 14.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 4300     |\n",
      "|    time_elapsed    | 972      |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=13.80 +/- 4.12\n",
      "Episode length: 5894.00 +/- 10553.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.89e+03 |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.0758   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 650      |\n",
      "|    ep_rew_mean     | 14.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 4400     |\n",
      "|    time_elapsed    | 1054     |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=18.20 +/- 2.93\n",
      "Episode length: 784.60 +/- 65.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 785      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.152   |\n",
      "|    explained_variance | 0.486    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.014   |\n",
      "|    value_loss         | 0.187    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 656      |\n",
      "|    ep_rew_mean     | 14.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 4500     |\n",
      "|    time_elapsed    | 1065     |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=14.40 +/- 2.94\n",
      "Episode length: 655.60 +/- 75.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 656      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0704  |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.00261  |\n",
      "|    value_loss         | 0.0247   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 668      |\n",
      "|    ep_rew_mean     | 15.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 4600     |\n",
      "|    time_elapsed    | 1076     |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=15.00 +/- 5.55\n",
      "Episode length: 656.00 +/- 140.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 656      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.24    |\n",
      "|    explained_variance | 0.497    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.179    |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 668      |\n",
      "|    ep_rew_mean     | 15.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 4700     |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=14.80 +/- 3.54\n",
      "Episode length: 628.00 +/- 105.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 628      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0361  |\n",
      "|    explained_variance | -0.165   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.00501  |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 14.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 4800     |\n",
      "|    time_elapsed    | 1098     |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=13.80 +/- 2.93\n",
      "Episode length: 629.40 +/- 59.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 629      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.225   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.0255  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 661      |\n",
      "|    ep_rew_mean     | 15       |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 4900     |\n",
      "|    time_elapsed    | 1108     |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=13.00 +/- 1.79\n",
      "Episode length: 624.20 +/- 32.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 624      |\n",
      "|    mean_reward        | 13       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.307   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.219    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 662      |\n",
      "|    ep_rew_mean     | 15.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 89       |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 1119     |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=16.00 +/- 7.40\n",
      "Episode length: 5981.80 +/- 10509.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 102000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.289   |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.0433   |\n",
      "|    value_loss         | 0.191    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 669      |\n",
      "|    ep_rew_mean     | 15.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 5100     |\n",
      "|    time_elapsed    | 1201     |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=14.80 +/- 4.02\n",
      "Episode length: 654.00 +/- 73.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 654      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0562  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.0659   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 5200     |\n",
      "|    time_elapsed    | 1210     |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=17.40 +/- 3.38\n",
      "Episode length: 697.20 +/- 36.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 697      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 106000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.266   |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0935   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 5300     |\n",
      "|    time_elapsed    | 1221     |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=14.80 +/- 2.32\n",
      "Episode length: 640.80 +/- 53.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 641      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.171    |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 5400     |\n",
      "|    time_elapsed    | 1232     |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=14.00 +/- 8.46\n",
      "Episode length: 5978.20 +/- 10511.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.091   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -0.00214 |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 16       |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 5500     |\n",
      "|    time_elapsed    | 1313     |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=15.00 +/- 2.61\n",
      "Episode length: 702.60 +/- 73.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 703      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.183   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.00201  |\n",
      "|    value_loss         | 0.057    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 16.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 5600     |\n",
      "|    time_elapsed    | 1324     |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=17.20 +/- 3.97\n",
      "Episode length: 678.60 +/- 73.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 679      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 114000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0405  |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 0.000672 |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 15.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 5700     |\n",
      "|    time_elapsed    | 1335     |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=15.60 +/- 3.01\n",
      "Episode length: 5962.80 +/- 10519.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.367   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0373  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 5800     |\n",
      "|    time_elapsed    | 1415     |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=15.20 +/- 1.33\n",
      "Episode length: 653.80 +/- 88.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 654      |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 118000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.356   |\n",
      "|    explained_variance | 0.505    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 0.0744   |\n",
      "|    value_loss         | 0.352    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 5900     |\n",
      "|    time_elapsed    | 1425     |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=14.40 +/- 3.32\n",
      "Episode length: 615.80 +/- 85.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 616      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -0.00675 |\n",
      "|    value_loss         | 0.0391   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 676      |\n",
      "|    ep_rew_mean     | 15.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 1436     |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=14.80 +/- 3.31\n",
      "Episode length: 671.40 +/- 78.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 671      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 122000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 0.0291   |\n",
      "|    value_loss         | 0.039    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 15.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 6100     |\n",
      "|    time_elapsed    | 1447     |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=14.20 +/- 2.93\n",
      "Episode length: 674.00 +/- 68.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 674      |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.149   |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.00329  |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 672      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 6200     |\n",
      "|    time_elapsed    | 1458     |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=16.80 +/- 3.87\n",
      "Episode length: 723.00 +/- 103.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 723      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 126000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.178   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.0366   |\n",
      "|    value_loss         | 0.0476   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 676      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 6300     |\n",
      "|    time_elapsed    | 1469     |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=13.80 +/- 2.23\n",
      "Episode length: 658.00 +/- 60.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 658      |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.00134 |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 15.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 6400     |\n",
      "|    time_elapsed    | 1480     |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=17.20 +/- 3.49\n",
      "Episode length: 705.60 +/- 89.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 130000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.178   |\n",
      "|    explained_variance | 0.637    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.0597   |\n",
      "|    value_loss         | 0.0586   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 6500     |\n",
      "|    time_elapsed    | 1491     |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=15.00 +/- 4.38\n",
      "Episode length: 647.80 +/- 125.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 648      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0621  |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.00866 |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 15.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 6600     |\n",
      "|    time_elapsed    | 1502     |\n",
      "|    total_timesteps | 132000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=14.20 +/- 3.87\n",
      "Episode length: 5978.00 +/- 10511.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 134000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.174   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 6700     |\n",
      "|    time_elapsed    | 1583     |\n",
      "|    total_timesteps | 134000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=14.80 +/- 3.06\n",
      "Episode length: 650.00 +/- 86.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 650      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0805  |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    value_loss         | 0.0391   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 681      |\n",
      "|    ep_rew_mean     | 15.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 6800     |\n",
      "|    time_elapsed    | 1593     |\n",
      "|    total_timesteps | 136000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=14.00 +/- 7.54\n",
      "Episode length: 5946.60 +/- 10527.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 138000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.247   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.0377  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 15.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 82       |\n",
      "|    iterations      | 6900     |\n",
      "|    time_elapsed    | 1674     |\n",
      "|    total_timesteps | 138000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=17.60 +/- 4.41\n",
      "Episode length: 695.80 +/- 71.07\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 696       |\n",
      "|    mean_reward        | 17.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00614  |\n",
      "|    explained_variance | 0.992     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -2.33e-05 |\n",
      "|    value_loss         | 0.00883   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 689      |\n",
      "|    ep_rew_mean     | 15.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 1685     |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=13.40 +/- 5.16\n",
      "Episode length: 619.00 +/- 154.15\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 619       |\n",
      "|    mean_reward        | 13.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 142000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0803   |\n",
      "|    explained_variance | 0.946     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -3.68e-05 |\n",
      "|    value_loss         | 0.0501    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 7100     |\n",
      "|    time_elapsed    | 1696     |\n",
      "|    total_timesteps | 142000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=12.40 +/- 2.73\n",
      "Episode length: 616.60 +/- 92.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 617      |\n",
      "|    mean_reward        | 12.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    value_loss         | 0.0257   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 7200     |\n",
      "|    time_elapsed    | 1707     |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=13.60 +/- 1.20\n",
      "Episode length: 683.40 +/- 32.67\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 683       |\n",
      "|    mean_reward        | 13.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 146000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0541   |\n",
      "|    explained_variance | 0.927     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -0.000231 |\n",
      "|    value_loss         | 0.0358    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 692      |\n",
      "|    ep_rew_mean     | 16.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 7300     |\n",
      "|    time_elapsed    | 1718     |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=16.80 +/- 4.26\n",
      "Episode length: 663.00 +/- 68.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 663      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0675  |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.000549 |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 692      |\n",
      "|    ep_rew_mean     | 16.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 7400     |\n",
      "|    time_elapsed    | 1729     |\n",
      "|    total_timesteps | 148000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=20.60 +/- 5.08\n",
      "Episode length: 730.00 +/- 57.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 730      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 150000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0785  |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    value_loss         | 0.123    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 16.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 7500     |\n",
      "|    time_elapsed    | 1741     |\n",
      "|    total_timesteps | 150000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=16.80 +/- 2.79\n",
      "Episode length: 702.40 +/- 113.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 702      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0483  |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -0.0269  |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 16.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 7600     |\n",
      "|    time_elapsed    | 1752     |\n",
      "|    total_timesteps | 152000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=16.80 +/- 2.40\n",
      "Episode length: 699.60 +/- 70.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 154000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.00722 |\n",
      "|    value_loss         | 0.033    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 706      |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 7700     |\n",
      "|    time_elapsed    | 1764     |\n",
      "|    total_timesteps | 154000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=16.40 +/- 5.46\n",
      "Episode length: 667.40 +/- 123.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 667      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0641  |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 0.192    |\n",
      "|    value_loss         | 0.0758   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 7800     |\n",
      "|    time_elapsed    | 1775     |\n",
      "|    total_timesteps | 156000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=12.20 +/- 3.87\n",
      "Episode length: 5940.60 +/- 10529.86\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.94e+03  |\n",
      "|    mean_reward        | 12.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 158000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0518   |\n",
      "|    explained_variance | 0.639     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -0.000656 |\n",
      "|    value_loss         | 0.0238    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 720      |\n",
      "|    ep_rew_mean     | 17.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 7900     |\n",
      "|    time_elapsed    | 1856     |\n",
      "|    total_timesteps | 158000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=14.40 +/- 4.08\n",
      "Episode length: 627.20 +/- 62.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 627      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0646  |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 0.00219  |\n",
      "|    value_loss         | 0.0712   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 718      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 8000     |\n",
      "|    time_elapsed    | 1866     |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=18.20 +/- 5.98\n",
      "Episode length: 657.00 +/- 97.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 657      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 162000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0723  |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 0.00927  |\n",
      "|    value_loss         | 0.222    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 8100     |\n",
      "|    time_elapsed    | 1877     |\n",
      "|    total_timesteps | 162000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=13.80 +/- 5.74\n",
      "Episode length: 5930.40 +/- 10534.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.93e+03 |\n",
      "|    mean_reward        | 13.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0751  |\n",
      "|    explained_variance | 0.845    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 724      |\n",
      "|    ep_rew_mean     | 18.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 8200     |\n",
      "|    time_elapsed    | 1958     |\n",
      "|    total_timesteps | 164000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=16.40 +/- 4.03\n",
      "Episode length: 714.80 +/- 99.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 166000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0587  |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.00182 |\n",
      "|    value_loss         | 0.0519   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 8300     |\n",
      "|    time_elapsed    | 1969     |\n",
      "|    total_timesteps | 166000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=15.60 +/- 3.93\n",
      "Episode length: 693.80 +/- 52.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 694      |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.335   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 0.077    |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 18.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 8400     |\n",
      "|    time_elapsed    | 1980     |\n",
      "|    total_timesteps | 168000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=14.40 +/- 3.67\n",
      "Episode length: 636.40 +/- 89.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.0556  |\n",
      "|    value_loss         | 0.0512   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 8500     |\n",
      "|    time_elapsed    | 1992     |\n",
      "|    total_timesteps | 170000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=16.60 +/- 6.22\n",
      "Episode length: 705.80 +/- 168.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.195   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | 0.047    |\n",
      "|    value_loss         | 0.0855   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 726      |\n",
      "|    ep_rew_mean     | 18.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 8600     |\n",
      "|    time_elapsed    | 2003     |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=15.40 +/- 2.73\n",
      "Episode length: 695.40 +/- 75.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 695      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 174000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0744  |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.00431  |\n",
      "|    value_loss         | 0.0836   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 8700     |\n",
      "|    time_elapsed    | 2015     |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=19.00 +/- 2.00\n",
      "Episode length: 706.20 +/- 46.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0902  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 0.00255  |\n",
      "|    value_loss         | 0.0488   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 8800     |\n",
      "|    time_elapsed    | 2026     |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=20.20 +/- 6.49\n",
      "Episode length: 771.20 +/- 61.07\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 771       |\n",
      "|    mean_reward        | 20.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 178000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.035    |\n",
      "|    explained_variance | 0.942     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -0.000594 |\n",
      "|    value_loss         | 0.0531    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 742      |\n",
      "|    ep_rew_mean     | 19       |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 8900     |\n",
      "|    time_elapsed    | 2038     |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=16.80 +/- 4.53\n",
      "Episode length: 701.80 +/- 96.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 702      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.00864  |\n",
      "|    value_loss         | 0.0226   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 9000     |\n",
      "|    time_elapsed    | 2049     |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=11.40 +/- 6.56\n",
      "Episode length: 5946.80 +/- 10526.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 182000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.204   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 0.0861   |\n",
      "|    value_loss         | 0.403    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 739      |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 9100     |\n",
      "|    time_elapsed    | 2131     |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=17.20 +/- 4.35\n",
      "Episode length: 754.20 +/- 116.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 754      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0966  |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 0.0525   |\n",
      "|    value_loss         | 0.329    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 9200     |\n",
      "|    time_elapsed    | 2142     |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=20.00 +/- 4.60\n",
      "Episode length: 723.80 +/- 40.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 724      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 186000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.00786 |\n",
      "|    value_loss         | 0.05     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 18.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 9300     |\n",
      "|    time_elapsed    | 2154     |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=13.60 +/- 8.01\n",
      "Episode length: 5937.80 +/- 10531.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.94e+03 |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.00258  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 17.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 9400     |\n",
      "|    time_elapsed    | 2236     |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=15.00 +/- 2.10\n",
      "Episode length: 698.80 +/- 47.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 699      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 190000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0693  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 0.000746 |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 17.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 84       |\n",
      "|    iterations      | 9500     |\n",
      "|    time_elapsed    | 2246     |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=16.20 +/- 3.87\n",
      "Episode length: 702.00 +/- 55.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 702      |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0929  |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | 0.00318  |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 9600     |\n",
      "|    time_elapsed    | 2258     |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=17.40 +/- 4.76\n",
      "Episode length: 690.80 +/- 144.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 691      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 194000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0694  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 0.0129   |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 708      |\n",
      "|    ep_rew_mean     | 17.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 9700     |\n",
      "|    time_elapsed    | 2269     |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=13.00 +/- 5.87\n",
      "Episode length: 11207.00 +/- 12895.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 13       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0577  |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -0.00377 |\n",
      "|    value_loss         | 0.0638   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 16.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 9800     |\n",
      "|    time_elapsed    | 2349     |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=15.00 +/- 1.79\n",
      "Episode length: 5951.00 +/- 10524.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 198000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00319 |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 4.39e-05 |\n",
      "|    value_loss         | 0.0738   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 698      |\n",
      "|    ep_rew_mean     | 16.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 9900     |\n",
      "|    time_elapsed    | 2430     |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=15.80 +/- 3.19\n",
      "Episode length: 696.20 +/- 55.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 696      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0798  |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 0.01     |\n",
      "|    value_loss         | 0.449    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 16.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 10000    |\n",
      "|    time_elapsed    | 2442     |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=16.80 +/- 5.38\n",
      "Episode length: 5974.00 +/- 10513.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 202000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0411  |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | 0.0128   |\n",
      "|    value_loss         | 0.0837   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10100    |\n",
      "|    time_elapsed    | 2522     |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=14.00 +/- 2.97\n",
      "Episode length: 641.00 +/- 69.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 641      |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0269  |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.00456 |\n",
      "|    value_loss         | 0.412    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 17.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10200    |\n",
      "|    time_elapsed    | 2533     |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=15.60 +/- 3.77\n",
      "Episode length: 682.60 +/- 110.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 683      |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 206000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0932  |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -0.00776 |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 17.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10300    |\n",
      "|    time_elapsed    | 2545     |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=14.40 +/- 2.33\n",
      "Episode length: 5974.00 +/- 10513.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0348  |\n",
      "|    explained_variance | 0.48     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 0.00286  |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 17.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 10400    |\n",
      "|    time_elapsed    | 2625     |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=16.60 +/- 4.59\n",
      "Episode length: 738.80 +/- 148.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 739      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 210000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | -0.0011  |\n",
      "|    value_loss         | 0.0765   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 718      |\n",
      "|    ep_rew_mean     | 17.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 10500    |\n",
      "|    time_elapsed    | 2636     |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=16.40 +/- 4.22\n",
      "Episode length: 646.80 +/- 107.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 647      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0837  |\n",
      "|    explained_variance | 0.736    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | -0.00948 |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 17.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10600    |\n",
      "|    time_elapsed    | 2646     |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=14.80 +/- 6.31\n",
      "Episode length: 656.60 +/- 127.37\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 657       |\n",
      "|    mean_reward        | 14.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 214000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.144    |\n",
      "|    explained_variance | 0.912     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | -0.000569 |\n",
      "|    value_loss         | 0.0342    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10700    |\n",
      "|    time_elapsed    | 2657     |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=14.80 +/- 2.14\n",
      "Episode length: 691.60 +/- 74.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | -0.00988 |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 18.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 10800    |\n",
      "|    time_elapsed    | 2669     |\n",
      "|    total_timesteps | 216000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=14.80 +/- 2.04\n",
      "Episode length: 660.80 +/- 65.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 661      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 218000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | 0.00969  |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 10900    |\n",
      "|    time_elapsed    | 2680     |\n",
      "|    total_timesteps | 218000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=17.00 +/- 5.69\n",
      "Episode length: 752.80 +/- 125.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 753      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.039   |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | 0.000959 |\n",
      "|    value_loss         | 0.0812   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 11000    |\n",
      "|    time_elapsed    | 2692     |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=16.80 +/- 5.49\n",
      "Episode length: 5982.20 +/- 10509.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 222000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0617  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | 0.00266  |\n",
      "|    value_loss         | 0.0432   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 720      |\n",
      "|    ep_rew_mean     | 18       |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 11100    |\n",
      "|    time_elapsed    | 2774     |\n",
      "|    total_timesteps | 222000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=14.40 +/- 8.01\n",
      "Episode length: 5998.80 +/- 10500.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.112   |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | 0.00219  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 18       |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 11200    |\n",
      "|    time_elapsed    | 2856     |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=17.60 +/- 5.24\n",
      "Episode length: 695.80 +/- 111.62\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 696       |\n",
      "|    mean_reward        | 17.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 226000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00121  |\n",
      "|    explained_variance | 0.845     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11299     |\n",
      "|    policy_loss        | -5.28e-06 |\n",
      "|    value_loss         | 0.0327    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 719      |\n",
      "|    ep_rew_mean     | 18       |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 11300    |\n",
      "|    time_elapsed    | 2867     |\n",
      "|    total_timesteps | 226000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=17.20 +/- 3.37\n",
      "Episode length: 701.40 +/- 43.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 701      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0782  |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -0.00241 |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 11400    |\n",
      "|    time_elapsed    | 2878     |\n",
      "|    total_timesteps | 228000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=18.20 +/- 3.97\n",
      "Episode length: 737.20 +/- 71.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 737      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 230000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0501  |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | -0.00017 |\n",
      "|    value_loss         | 0.0585   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 708      |\n",
      "|    ep_rew_mean     | 17.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 11500    |\n",
      "|    time_elapsed    | 2890     |\n",
      "|    total_timesteps | 230000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=18.00 +/- 6.81\n",
      "Episode length: 704.40 +/- 103.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 704      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | 0.00441  |\n",
      "|    value_loss         | 0.0446   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 17.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 11600    |\n",
      "|    time_elapsed    | 2901     |\n",
      "|    total_timesteps | 232000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=16.20 +/- 3.12\n",
      "Episode length: 644.00 +/- 95.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 644       |\n",
      "|    mean_reward        | 16.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 234000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0393   |\n",
      "|    explained_variance | 0.969     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -0.000373 |\n",
      "|    value_loss         | 0.0816    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 701      |\n",
      "|    ep_rew_mean     | 17.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 11700    |\n",
      "|    time_elapsed    | 2912     |\n",
      "|    total_timesteps | 234000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=18.60 +/- 5.00\n",
      "Episode length: 725.20 +/- 113.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 725      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0139  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 702      |\n",
      "|    ep_rew_mean     | 17.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 11800    |\n",
      "|    time_elapsed    | 2924     |\n",
      "|    total_timesteps | 236000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=18.40 +/- 7.06\n",
      "Episode length: 709.80 +/- 118.49\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 710       |\n",
      "|    mean_reward        | 18.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 238000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.113    |\n",
      "|    explained_variance | 0.979     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -6.64e-05 |\n",
      "|    value_loss         | 0.0196    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 17       |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 11900    |\n",
      "|    time_elapsed    | 2935     |\n",
      "|    total_timesteps | 238000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=20.20 +/- 4.71\n",
      "Episode length: 6055.80 +/- 10472.22\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.06e+03  |\n",
      "|    mean_reward        | 20.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 240000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.107    |\n",
      "|    explained_variance | 0.982     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | -0.000796 |\n",
      "|    value_loss         | 0.0266    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 12000    |\n",
      "|    time_elapsed    | 3015     |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=16.80 +/- 6.24\n",
      "Episode length: 699.60 +/- 118.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 242000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.165   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | -0.00393 |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 12100    |\n",
      "|    time_elapsed    | 3026     |\n",
      "|    total_timesteps | 242000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=18.40 +/- 5.71\n",
      "Episode length: 738.40 +/- 136.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 738      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0629  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 17.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 12200    |\n",
      "|    time_elapsed    | 3037     |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=26.40 +/- 2.42\n",
      "Episode length: 829.00 +/- 81.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 829      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 246000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0831  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -0.0113  |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 720      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 12300    |\n",
      "|    time_elapsed    | 3049     |\n",
      "|    total_timesteps | 246000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=21.40 +/- 3.61\n",
      "Episode length: 749.60 +/- 91.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 248000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0142  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | -0.00174 |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 726      |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 12400    |\n",
      "|    time_elapsed    | 3061     |\n",
      "|    total_timesteps | 248000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=19.40 +/- 5.99\n",
      "Episode length: 736.20 +/- 157.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 736      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0563  |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.0624  |\n",
      "|    value_loss         | 0.329    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 12500    |\n",
      "|    time_elapsed    | 3072     |\n",
      "|    total_timesteps | 250000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=8.60 +/- 3.38\n",
      "Episode length: 432.60 +/- 102.79\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 433       |\n",
      "|    mean_reward        | 8.6       |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 252000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00367  |\n",
      "|    explained_variance | 0.299     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -7.37e-05 |\n",
      "|    value_loss         | 0.205     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 81       |\n",
      "|    iterations      | 12600    |\n",
      "|    time_elapsed    | 3082     |\n",
      "|    total_timesteps | 252000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=8.60 +/- 7.14\n",
      "Episode length: 11183.60 +/- 12914.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 8.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 254000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0434  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | 0.00114  |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 701      |\n",
      "|    ep_rew_mean     | 17.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 12700    |\n",
      "|    time_elapsed    | 3162     |\n",
      "|    total_timesteps | 254000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=16.40 +/- 2.65\n",
      "Episode length: 752.00 +/- 62.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 752       |\n",
      "|    mean_reward        | 16.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 256000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0147   |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12799     |\n",
      "|    policy_loss        | -0.000362 |\n",
      "|    value_loss         | 0.035     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 708      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 12800    |\n",
      "|    time_elapsed    | 3174     |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=16.20 +/- 9.58\n",
      "Episode length: 11228.80 +/- 12877.21\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.12e+04  |\n",
      "|    mean_reward        | 16.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 258000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0152   |\n",
      "|    explained_variance | 0.808     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -9.95e-05 |\n",
      "|    value_loss         | 0.0555    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 12900    |\n",
      "|    time_elapsed    | 3253     |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=20.00 +/- 6.69\n",
      "Episode length: 731.00 +/- 114.76\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 731       |\n",
      "|    mean_reward        | 20        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 260000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0174   |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -0.000331 |\n",
      "|    value_loss         | 0.0928    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13000    |\n",
      "|    time_elapsed    | 3264     |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=19.20 +/- 5.64\n",
      "Episode length: 751.80 +/- 161.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 752      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 262000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0998  |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13100    |\n",
      "|    time_elapsed    | 3277     |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=13.20 +/- 4.96\n",
      "Episode length: 643.00 +/- 151.03\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 643       |\n",
      "|    mean_reward        | 13.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 264000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0265   |\n",
      "|    explained_variance | 0.989     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | -0.000105 |\n",
      "|    value_loss         | 0.0132    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 13200    |\n",
      "|    time_elapsed    | 3288     |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=16.40 +/- 3.98\n",
      "Episode length: 5966.00 +/- 10517.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 266000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0334  |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | -0.00109 |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 13300    |\n",
      "|    time_elapsed    | 3368     |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=20.00 +/- 4.77\n",
      "Episode length: 733.00 +/- 56.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 733      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 711      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13400    |\n",
      "|    time_elapsed    | 3380     |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=16.80 +/- 2.32\n",
      "Episode length: 722.00 +/- 61.30\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 722       |\n",
      "|    mean_reward        | 16.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 270000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0034   |\n",
      "|    explained_variance | 0.961     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -2.74e-05 |\n",
      "|    value_loss         | 0.0344    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 19       |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13500    |\n",
      "|    time_elapsed    | 3391     |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=15.60 +/- 4.63\n",
      "Episode length: 680.80 +/- 133.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 681       |\n",
      "|    mean_reward        | 15.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 272000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0725   |\n",
      "|    explained_variance | 0.571     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -6.03e-05 |\n",
      "|    value_loss         | 0.157     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13600    |\n",
      "|    time_elapsed    | 3402     |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=14.40 +/- 6.31\n",
      "Episode length: 5966.00 +/- 10517.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 274000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0677  |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0.00616 |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 13700    |\n",
      "|    time_elapsed    | 3484     |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=18.60 +/- 6.15\n",
      "Episode length: 709.20 +/- 100.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 709      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0707  |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -0.0358  |\n",
      "|    value_loss         | 0.0863   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 13800    |\n",
      "|    time_elapsed    | 3494     |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=15.00 +/- 3.58\n",
      "Episode length: 695.40 +/- 106.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 695      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 278000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0891  |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | -0.00789 |\n",
      "|    value_loss         | 0.0692   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 79       |\n",
      "|    iterations      | 13900    |\n",
      "|    time_elapsed    | 3505     |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=17.00 +/- 5.62\n",
      "Episode length: 6025.20 +/- 10487.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0418  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | 0.00621  |\n",
      "|    value_loss         | 0.0794   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 14000    |\n",
      "|    time_elapsed    | 3587     |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=18.20 +/- 5.67\n",
      "Episode length: 703.80 +/- 111.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 704      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 282000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | 0.0352   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 14100    |\n",
      "|    time_elapsed    | 3597     |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=18.60 +/- 3.07\n",
      "Episode length: 755.40 +/- 46.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 755      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.775    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | -0.101   |\n",
      "|    value_loss         | 0.526    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 18.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 14200    |\n",
      "|    time_elapsed    | 3610     |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=19.80 +/- 4.07\n",
      "Episode length: 723.80 +/- 120.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 724      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 286000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0582  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 0.00143  |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 739      |\n",
      "|    ep_rew_mean     | 18.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 78       |\n",
      "|    iterations      | 14300    |\n",
      "|    time_elapsed    | 3621     |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=15.60 +/- 4.32\n",
      "Episode length: 5975.40 +/- 10512.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0698  |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    value_loss         | 0.0967   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 724      |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 14400    |\n",
      "|    time_elapsed    | 3702     |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=15.00 +/- 3.35\n",
      "Episode length: 5938.80 +/- 10530.76\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.94e+03  |\n",
      "|    mean_reward        | 15        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 290000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.136    |\n",
      "|    explained_variance | 0.664     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -0.000131 |\n",
      "|    value_loss         | 0.278     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 17.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 14500    |\n",
      "|    time_elapsed    | 3783     |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=17.60 +/- 5.39\n",
      "Episode length: 694.40 +/- 70.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 694      |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0427  |\n",
      "|    explained_variance | 0.779    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 0.0554   |\n",
      "|    value_loss         | 0.0893   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 14600    |\n",
      "|    time_elapsed    | 3793     |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=15.40 +/- 4.67\n",
      "Episode length: 667.60 +/- 82.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 668      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 294000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.04    |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | 0.00628  |\n",
      "|    value_loss         | 0.173    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 18.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 14700    |\n",
      "|    time_elapsed    | 3805     |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=18.40 +/- 5.68\n",
      "Episode length: 6062.40 +/- 10468.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 0.00409  |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 14800    |\n",
      "|    time_elapsed    | 3885     |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=20.60 +/- 4.32\n",
      "Episode length: 5984.00 +/- 10508.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 298000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0704  |\n",
      "|    explained_variance | 0.806    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | -0.00404 |\n",
      "|    value_loss         | 0.0672   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 736      |\n",
      "|    ep_rew_mean     | 18.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 14900    |\n",
      "|    time_elapsed    | 3967     |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=16.00 +/- 2.28\n",
      "Episode length: 746.80 +/- 96.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 742      |\n",
      "|    ep_rew_mean     | 18.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15000    |\n",
      "|    time_elapsed    | 3979     |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=21.40 +/- 7.23\n",
      "Episode length: 725.40 +/- 140.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 725      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 302000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0652  |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15099    |\n",
      "|    policy_loss        | 0.000671 |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 19       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15100    |\n",
      "|    time_elapsed    | 3990     |\n",
      "|    total_timesteps | 302000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=19.20 +/- 4.45\n",
      "Episode length: 767.80 +/- 95.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 304000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.074   |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | -0.083   |\n",
      "|    value_loss         | 0.0554   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 19.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15200    |\n",
      "|    time_elapsed    | 4002     |\n",
      "|    total_timesteps | 304000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=17.60 +/- 4.03\n",
      "Episode length: 5996.00 +/- 10502.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 306000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0842  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | 0.0069   |\n",
      "|    value_loss         | 0.0602   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 15300    |\n",
      "|    time_elapsed    | 4083     |\n",
      "|    total_timesteps | 306000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=21.40 +/- 5.08\n",
      "Episode length: 781.00 +/- 68.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 781      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 308000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.112   |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | 0.0334   |\n",
      "|    value_loss         | 0.254    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 19.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15400    |\n",
      "|    time_elapsed    | 4094     |\n",
      "|    total_timesteps | 308000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=21.40 +/- 6.15\n",
      "Episode length: 782.00 +/- 106.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 310000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0131  |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | 2.25e-06 |\n",
      "|    value_loss         | 0.00839  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15500    |\n",
      "|    time_elapsed    | 4106     |\n",
      "|    total_timesteps | 310000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=21.60 +/- 5.75\n",
      "Episode length: 805.60 +/- 66.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 806      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 312000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0432  |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | 0.00017  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 15600    |\n",
      "|    time_elapsed    | 4118     |\n",
      "|    total_timesteps | 312000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=16.00 +/- 2.19\n",
      "Episode length: 696.20 +/- 46.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 696      |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 314000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0427  |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | 0.0011   |\n",
      "|    value_loss         | 0.0626   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 15700    |\n",
      "|    time_elapsed    | 4129     |\n",
      "|    total_timesteps | 314000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=18.60 +/- 3.72\n",
      "Episode length: 731.60 +/- 56.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 732      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 316000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.162   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | -0.00103 |\n",
      "|    value_loss         | 0.0861   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 753      |\n",
      "|    ep_rew_mean     | 19.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 15800    |\n",
      "|    time_elapsed    | 4141     |\n",
      "|    total_timesteps | 316000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=17.20 +/- 5.91\n",
      "Episode length: 711.20 +/- 105.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 711      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 318000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | -0.0636  |\n",
      "|    value_loss         | 0.0533   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 755      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 15900    |\n",
      "|    time_elapsed    | 4153     |\n",
      "|    total_timesteps | 318000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=15.20 +/- 6.97\n",
      "Episode length: 5961.00 +/- 10520.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.00296 |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 16000    |\n",
      "|    time_elapsed    | 4233     |\n",
      "|    total_timesteps | 320000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=20.40 +/- 4.76\n",
      "Episode length: 766.60 +/- 123.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 767      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 322000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | 0.009    |\n",
      "|    value_loss         | 0.0322   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 16100    |\n",
      "|    time_elapsed    | 4244     |\n",
      "|    total_timesteps | 322000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=20.40 +/- 9.31\n",
      "Episode length: 6011.60 +/- 10494.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 324000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.167   |\n",
      "|    explained_variance | 0.376    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.0285  |\n",
      "|    value_loss         | 0.0519   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 19.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 16200    |\n",
      "|    time_elapsed    | 4325     |\n",
      "|    total_timesteps | 324000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=15.00 +/- 10.33\n",
      "Episode length: 5982.00 +/- 10509.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 326000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.297   |\n",
      "|    explained_variance | 0.0172   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.298    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 16300    |\n",
      "|    time_elapsed    | 4407     |\n",
      "|    total_timesteps | 326000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=15.80 +/- 2.14\n",
      "Episode length: 702.80 +/- 37.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 703      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 328000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.0767  |\n",
      "|    value_loss         | 0.206    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 16400    |\n",
      "|    time_elapsed    | 4417     |\n",
      "|    total_timesteps | 328000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=13.40 +/- 2.24\n",
      "Episode length: 621.40 +/- 74.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 621      |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 330000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -0.0053  |\n",
      "|    value_loss         | 0.0631   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 16500    |\n",
      "|    time_elapsed    | 4427     |\n",
      "|    total_timesteps | 330000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=19.60 +/- 2.06\n",
      "Episode length: 807.40 +/- 63.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 807      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 332000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 19.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 16600    |\n",
      "|    time_elapsed    | 4439     |\n",
      "|    total_timesteps | 332000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=16.40 +/- 5.89\n",
      "Episode length: 694.60 +/- 167.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 695      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 334000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.165   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -0.0128  |\n",
      "|    value_loss         | 0.0624   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 16700    |\n",
      "|    time_elapsed    | 4451     |\n",
      "|    total_timesteps | 334000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=17.40 +/- 3.07\n",
      "Episode length: 722.00 +/- 113.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 336000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16799    |\n",
      "|    policy_loss        | -0.035   |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 18.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 16800    |\n",
      "|    time_elapsed    | 4464     |\n",
      "|    total_timesteps | 336000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=20.60 +/- 5.43\n",
      "Episode length: 787.40 +/- 83.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 338000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0443  |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | 0.00983  |\n",
      "|    value_loss         | 0.0939   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 18.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 16900    |\n",
      "|    time_elapsed    | 4476     |\n",
      "|    total_timesteps | 338000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=13.40 +/- 6.34\n",
      "Episode length: 5960.20 +/- 10520.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 13.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 340000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0296  |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | 0.000946 |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 739      |\n",
      "|    ep_rew_mean     | 18.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17000    |\n",
      "|    time_elapsed    | 4557     |\n",
      "|    total_timesteps | 340000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=20.40 +/- 2.87\n",
      "Episode length: 801.00 +/- 81.62\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 801       |\n",
      "|    mean_reward        | 20.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 342000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0229   |\n",
      "|    explained_variance | 0.996     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -9.03e-05 |\n",
      "|    value_loss         | 0.0087    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 18.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17100    |\n",
      "|    time_elapsed    | 4568     |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=18.00 +/- 3.46\n",
      "Episode length: 719.40 +/- 81.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 719      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 344000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0237  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 0.0407   |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 17200    |\n",
      "|    time_elapsed    | 4580     |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=14.60 +/- 4.41\n",
      "Episode length: 5977.60 +/- 10511.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 346000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0135  |\n",
      "|    explained_variance | 0.107    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | 9.75e-05 |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 19.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17300    |\n",
      "|    time_elapsed    | 4660     |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=22.00 +/- 2.83\n",
      "Episode length: 791.00 +/- 51.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 791      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 348000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.105   |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.226    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17400    |\n",
      "|    time_elapsed    | 4672     |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=17.00 +/- 5.37\n",
      "Episode length: 671.20 +/- 134.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 671      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | 0.0096   |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 18.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17500    |\n",
      "|    time_elapsed    | 4683     |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=13.20 +/- 5.11\n",
      "Episode length: 600.20 +/- 107.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 600      |\n",
      "|    mean_reward        | 13.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 352000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0659  |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.00235 |\n",
      "|    value_loss         | 0.361    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 19.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 17600    |\n",
      "|    time_elapsed    | 4694     |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=17.80 +/- 4.12\n",
      "Episode length: 728.80 +/- 104.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 354000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0882  |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | 0.0209   |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 17700    |\n",
      "|    time_elapsed    | 4705     |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=21.20 +/- 7.65\n",
      "Episode length: 756.80 +/- 121.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 757      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 356000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0763  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | 0.00897  |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 17800    |\n",
      "|    time_elapsed    | 4717     |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=19.00 +/- 2.97\n",
      "Episode length: 742.20 +/- 42.38\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 742       |\n",
      "|    mean_reward        | 19        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 358000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0243   |\n",
      "|    explained_variance | 0.967     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -0.000162 |\n",
      "|    value_loss         | 0.0753    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 17900    |\n",
      "|    time_elapsed    | 4729     |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=19.80 +/- 4.07\n",
      "Episode length: 802.80 +/- 62.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 803      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0453  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17999    |\n",
      "|    policy_loss        | 0.00138  |\n",
      "|    value_loss         | 0.0411   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18000    |\n",
      "|    time_elapsed    | 4741     |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=14.00 +/- 6.69\n",
      "Episode length: 5959.00 +/- 10521.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 14       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 362000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0039  |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18099    |\n",
      "|    policy_loss        | -5e-05   |\n",
      "|    value_loss         | 0.0582   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18100    |\n",
      "|    time_elapsed    | 4822     |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=18.00 +/- 5.02\n",
      "Episode length: 724.20 +/- 93.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 724      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 364000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00426 |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 0.00019  |\n",
      "|    value_loss         | 0.0804   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18200    |\n",
      "|    time_elapsed    | 4833     |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=15.60 +/- 5.39\n",
      "Episode length: 656.00 +/- 138.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 656      |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 366000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0708  |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18300    |\n",
      "|    time_elapsed    | 4844     |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=20.20 +/- 4.83\n",
      "Episode length: 782.20 +/- 79.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 368000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | 0.0193   |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18400    |\n",
      "|    time_elapsed    | 4856     |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=16.80 +/- 1.47\n",
      "Episode length: 722.00 +/- 14.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 370000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 0.202    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 768      |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18500    |\n",
      "|    time_elapsed    | 4868     |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=17.40 +/- 7.91\n",
      "Episode length: 5997.60 +/- 10501.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 372000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.0897   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 768      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18600    |\n",
      "|    time_elapsed    | 4950     |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=16.80 +/- 1.17\n",
      "Episode length: 742.20 +/- 50.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 742      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 374000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0309  |\n",
      "|    explained_variance | -0.563   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 0.00685  |\n",
      "|    value_loss         | 0.341    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18700    |\n",
      "|    time_elapsed    | 4960     |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=19.40 +/- 4.54\n",
      "Episode length: 737.20 +/- 82.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 737      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 376000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0944  |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 0.282    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18800    |\n",
      "|    time_elapsed    | 4973     |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=19.20 +/- 4.53\n",
      "Episode length: 777.00 +/- 114.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 777      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 378000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.083   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | -0.0151  |\n",
      "|    value_loss         | 0.284    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 18900    |\n",
      "|    time_elapsed    | 4984     |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=15.80 +/- 3.31\n",
      "Episode length: 716.40 +/- 94.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 716      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0495  |\n",
      "|    explained_variance | 0.467    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | 0.00722  |\n",
      "|    value_loss         | 0.633    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19000    |\n",
      "|    time_elapsed    | 4996     |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=23.40 +/- 4.63\n",
      "Episode length: 797.80 +/- 59.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 382000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0457  |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | -0.00131 |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19100    |\n",
      "|    time_elapsed    | 5008     |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=17.00 +/- 1.79\n",
      "Episode length: 684.40 +/- 50.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 684      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 384000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00222 |\n",
      "|    explained_variance | 0.786    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 7.81e-05 |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19200    |\n",
      "|    time_elapsed    | 5019     |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=17.40 +/- 2.58\n",
      "Episode length: 677.80 +/- 82.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 678      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 386000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0866  |\n",
      "|    explained_variance | 0.526    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.192    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19300    |\n",
      "|    time_elapsed    | 5031     |\n",
      "|    total_timesteps | 386000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=17.40 +/- 4.59\n",
      "Episode length: 726.20 +/- 19.59\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 726       |\n",
      "|    mean_reward        | 17.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 388000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0138   |\n",
      "|    explained_variance | 0.98      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | -0.000477 |\n",
      "|    value_loss         | 0.0277    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19400    |\n",
      "|    time_elapsed    | 5043     |\n",
      "|    total_timesteps | 388000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=18.00 +/- 6.90\n",
      "Episode length: 729.40 +/- 131.96\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 729       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 390000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00812  |\n",
      "|    explained_variance | 0.936     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -0.000172 |\n",
      "|    value_loss         | 0.0828    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 19500    |\n",
      "|    time_elapsed    | 5055     |\n",
      "|    total_timesteps | 390000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=17.20 +/- 9.99\n",
      "Episode length: 5954.80 +/- 10523.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 392000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 0.0196   |\n",
      "|    value_loss         | 0.0556   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19600    |\n",
      "|    time_elapsed    | 5136     |\n",
      "|    total_timesteps | 392000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=19.40 +/- 4.22\n",
      "Episode length: 790.40 +/- 127.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 790      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 394000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0173  |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | 0.00197  |\n",
      "|    value_loss         | 0.0948   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 19700    |\n",
      "|    time_elapsed    | 5148     |\n",
      "|    total_timesteps | 394000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=16.60 +/- 3.56\n",
      "Episode length: 6002.40 +/- 10499.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 396000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0385  |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | -0.00389 |\n",
      "|    value_loss         | 0.418    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 19800    |\n",
      "|    time_elapsed    | 5228     |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=21.60 +/- 2.33\n",
      "Episode length: 802.00 +/- 85.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 802      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 398000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0447  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.00216  |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 19900    |\n",
      "|    time_elapsed    | 5240     |\n",
      "|    total_timesteps | 398000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=20.80 +/- 8.26\n",
      "Episode length: 728.00 +/- 74.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 728      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.217   |\n",
      "|    explained_variance | 0.0186   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | -0.347   |\n",
      "|    value_loss         | 0.724    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20000    |\n",
      "|    time_elapsed    | 5252     |\n",
      "|    total_timesteps | 400000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=19.60 +/- 4.50\n",
      "Episode length: 739.60 +/- 82.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 740      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 402000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0223  |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | -0.00198 |\n",
      "|    value_loss         | 0.0999   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20100    |\n",
      "|    time_elapsed    | 5264     |\n",
      "|    total_timesteps | 402000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=17.00 +/- 2.76\n",
      "Episode length: 699.80 +/- 60.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 404000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0987  |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    value_loss         | 0.0338   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20200    |\n",
      "|    time_elapsed    | 5276     |\n",
      "|    total_timesteps | 404000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=12.00 +/- 6.81\n",
      "Episode length: 11199.80 +/- 12900.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 12       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 406000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0262  |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | 0.00175  |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 20300    |\n",
      "|    time_elapsed    | 5355     |\n",
      "|    total_timesteps | 406000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=21.60 +/- 4.13\n",
      "Episode length: 749.80 +/- 68.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 408000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0262  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | 0.00266  |\n",
      "|    value_loss         | 0.0358   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20400    |\n",
      "|    time_elapsed    | 5366     |\n",
      "|    total_timesteps | 408000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=20.20 +/- 6.08\n",
      "Episode length: 778.00 +/- 105.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 410000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0397  |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | -0.00354 |\n",
      "|    value_loss         | 0.0621   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20500    |\n",
      "|    time_elapsed    | 5378     |\n",
      "|    total_timesteps | 410000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=22.00 +/- 5.66\n",
      "Episode length: 812.00 +/- 153.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 812      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 412000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0351  |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | 0.00512  |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20600    |\n",
      "|    time_elapsed    | 5390     |\n",
      "|    total_timesteps | 412000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=19.20 +/- 4.49\n",
      "Episode length: 777.80 +/- 86.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 414000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0583  |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | 0.00871  |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20700    |\n",
      "|    time_elapsed    | 5402     |\n",
      "|    total_timesteps | 414000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=17.00 +/- 2.68\n",
      "Episode length: 711.40 +/- 63.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 711      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 416000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0497  |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 20800    |\n",
      "|    time_elapsed    | 5413     |\n",
      "|    total_timesteps | 416000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=418000, episode_reward=18.40 +/- 5.20\n",
      "Episode length: 744.80 +/- 145.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 745      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 418000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.143   |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | -0.0442  |\n",
      "|    value_loss         | 0.383    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 20900    |\n",
      "|    time_elapsed    | 5425     |\n",
      "|    total_timesteps | 418000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=21.40 +/- 3.83\n",
      "Episode length: 773.00 +/- 134.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 773      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 420000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0399  |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | 0.00229  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21000    |\n",
      "|    time_elapsed    | 5436     |\n",
      "|    total_timesteps | 420000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=19.00 +/- 6.03\n",
      "Episode length: 792.60 +/- 117.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 793      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 422000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0674  |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | -0.00996 |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21100    |\n",
      "|    time_elapsed    | 5448     |\n",
      "|    total_timesteps | 422000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=18.00 +/- 5.76\n",
      "Episode length: 714.00 +/- 98.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 714      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 424000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.06    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | -0.00108 |\n",
      "|    value_loss         | 0.0663   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21200    |\n",
      "|    time_elapsed    | 5459     |\n",
      "|    total_timesteps | 424000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=17.60 +/- 6.44\n",
      "Episode length: 6009.40 +/- 10496.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 426000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 0.00414  |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 21300    |\n",
      "|    time_elapsed    | 5540     |\n",
      "|    total_timesteps | 426000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=21.20 +/- 4.17\n",
      "Episode length: 757.80 +/- 96.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 758      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 428000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0245  |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | -0.00295 |\n",
      "|    value_loss         | 0.362    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21400    |\n",
      "|    time_elapsed    | 5551     |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=23.00 +/- 2.76\n",
      "Episode length: 802.00 +/- 51.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 802      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 430000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00849 |\n",
      "|    explained_variance | 0.646    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | 6.54e-05 |\n",
      "|    value_loss         | 0.229    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 19.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21500    |\n",
      "|    time_elapsed    | 5563     |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=23.40 +/- 5.39\n",
      "Episode length: 763.20 +/- 65.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 763      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 432000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0359  |\n",
      "|    explained_variance | 0.109    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | -0.00449 |\n",
      "|    value_loss         | 0.148    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 21600    |\n",
      "|    time_elapsed    | 5576     |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=14.60 +/- 8.48\n",
      "Episode length: 5948.40 +/- 10525.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 434000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00862 |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | 0.0199   |\n",
      "|    value_loss         | 0.0179   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 21700    |\n",
      "|    time_elapsed    | 5657     |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=20.40 +/- 9.05\n",
      "Episode length: 6077.60 +/- 10462.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 436000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0647  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -0.00816 |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 21800    |\n",
      "|    time_elapsed    | 5738     |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=15.20 +/- 4.87\n",
      "Episode length: 5943.60 +/- 10528.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.94e+03 |\n",
      "|    mean_reward        | 15.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 438000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.081   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | 0.00901  |\n",
      "|    value_loss         | 0.05     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 759      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 21900    |\n",
      "|    time_elapsed    | 5819     |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=21.80 +/- 4.96\n",
      "Episode length: 808.00 +/- 99.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 808      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0358  |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | -0.00495 |\n",
      "|    value_loss         | 0.0745   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22000    |\n",
      "|    time_elapsed    | 5829     |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=19.00 +/- 6.16\n",
      "Episode length: 734.00 +/- 101.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 442000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00134 |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | -1.4e-05 |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22100    |\n",
      "|    time_elapsed    | 5841     |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=16.80 +/- 3.66\n",
      "Episode length: 722.80 +/- 85.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 723      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 444000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.072   |\n",
      "|    explained_variance | 0.824    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.394    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 762      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22200    |\n",
      "|    time_elapsed    | 5852     |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=14.80 +/- 5.95\n",
      "Episode length: 11263.60 +/- 12848.77\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.13e+04  |\n",
      "|    mean_reward        | 14.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 446000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00226  |\n",
      "|    explained_variance | 0.969     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22299     |\n",
      "|    policy_loss        | -6.07e-06 |\n",
      "|    value_loss         | 0.0238    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22300    |\n",
      "|    time_elapsed    | 5931     |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=448000, episode_reward=15.40 +/- 2.06\n",
      "Episode length: 705.80 +/- 55.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 706      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 448000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.708    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 20.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22400    |\n",
      "|    time_elapsed    | 5942     |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=19.00 +/- 6.00\n",
      "Episode length: 6013.80 +/- 10493.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.01e+03  |\n",
      "|    mean_reward        | 19        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 450000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0595   |\n",
      "|    explained_variance | 0.899     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22499     |\n",
      "|    policy_loss        | -0.000248 |\n",
      "|    value_loss         | 0.11      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 22500    |\n",
      "|    time_elapsed    | 6022     |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=452000, episode_reward=19.40 +/- 4.03\n",
      "Episode length: 777.00 +/- 87.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 777      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 452000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0922  |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | -0.0063  |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 22600    |\n",
      "|    time_elapsed    | 6035     |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=21.80 +/- 6.97\n",
      "Episode length: 748.20 +/- 81.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 748      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 454000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.0566   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22700    |\n",
      "|    time_elapsed    | 6047     |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=17.20 +/- 2.71\n",
      "Episode length: 713.80 +/- 73.35\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 714       |\n",
      "|    mean_reward        | 17.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 456000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0255   |\n",
      "|    explained_variance | 0.642     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22799     |\n",
      "|    policy_loss        | -0.000354 |\n",
      "|    value_loss         | 0.29      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 22800    |\n",
      "|    time_elapsed    | 6058     |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=19.40 +/- 9.97\n",
      "Episode length: 5993.20 +/- 10504.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 458000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0914  |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -0.0971  |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 22900    |\n",
      "|    time_elapsed    | 6139     |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=20.80 +/- 4.49\n",
      "Episode length: 780.00 +/- 107.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 460000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0469  |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | 0.00238  |\n",
      "|    value_loss         | 0.0978   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23000    |\n",
      "|    time_elapsed    | 6150     |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=19.60 +/- 10.56\n",
      "Episode length: 697.00 +/- 193.75\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 697       |\n",
      "|    mean_reward        | 19.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 462000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0835   |\n",
      "|    explained_variance | 0.68      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | -0.000449 |\n",
      "|    value_loss         | 0.532     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23100    |\n",
      "|    time_elapsed    | 6160     |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=14.60 +/- 8.26\n",
      "Episode length: 6031.00 +/- 10484.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 464000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0503  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23200    |\n",
      "|    time_elapsed    | 6242     |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=16.40 +/- 3.72\n",
      "Episode length: 703.60 +/- 114.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 704      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 466000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00932 |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | 2.98e-06 |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23300    |\n",
      "|    time_elapsed    | 6252     |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=20.20 +/- 4.49\n",
      "Episode length: 769.40 +/- 81.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 769      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 468000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0969  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | -0.0403  |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23400    |\n",
      "|    time_elapsed    | 6263     |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=17.00 +/- 6.93\n",
      "Episode length: 6051.00 +/- 10474.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 470000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0237  |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 0.000563 |\n",
      "|    value_loss         | 0.056    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23500    |\n",
      "|    time_elapsed    | 6344     |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=15.00 +/- 2.28\n",
      "Episode length: 721.80 +/- 72.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 472000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0124  |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | 0.041    |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23600    |\n",
      "|    time_elapsed    | 6355     |\n",
      "|    total_timesteps | 472000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=19.00 +/- 5.76\n",
      "Episode length: 736.20 +/- 120.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 736      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 474000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00671 |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | 0.000159 |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23700    |\n",
      "|    time_elapsed    | 6366     |\n",
      "|    total_timesteps | 474000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=23.00 +/- 3.63\n",
      "Episode length: 799.60 +/- 73.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 800      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 476000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00159 |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | 7.37e-05 |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23800    |\n",
      "|    time_elapsed    | 6378     |\n",
      "|    total_timesteps | 476000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=18.40 +/- 5.95\n",
      "Episode length: 742.00 +/- 115.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 742      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 478000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 23900    |\n",
      "|    time_elapsed    | 6390     |\n",
      "|    total_timesteps | 478000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=16.40 +/- 7.86\n",
      "Episode length: 5989.20 +/- 10505.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0216  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | 0.000569 |\n",
      "|    value_loss         | 0.0183   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 24000    |\n",
      "|    time_elapsed    | 6470     |\n",
      "|    total_timesteps | 480000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=16.00 +/- 4.43\n",
      "Episode length: 5965.20 +/- 10518.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 482000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.192   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | -0.00103 |\n",
      "|    value_loss         | 0.0883   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24100    |\n",
      "|    time_elapsed    | 6550     |\n",
      "|    total_timesteps | 482000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=16.00 +/- 6.60\n",
      "Episode length: 6046.40 +/- 10477.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 484000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0404  |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | -0.00113 |\n",
      "|    value_loss         | 0.0399   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 24200    |\n",
      "|    time_elapsed    | 6630     |\n",
      "|    total_timesteps | 484000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=486000, episode_reward=19.40 +/- 5.16\n",
      "Episode length: 780.00 +/- 145.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 486000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0558  |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | 0.00331  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24300    |\n",
      "|    time_elapsed    | 6641     |\n",
      "|    total_timesteps | 486000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=23.40 +/- 6.28\n",
      "Episode length: 839.60 +/- 87.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 840      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 488000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00772 |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | 0.000561 |\n",
      "|    value_loss         | 0.224    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24400    |\n",
      "|    time_elapsed    | 6653     |\n",
      "|    total_timesteps | 488000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=16.00 +/- 3.22\n",
      "Episode length: 710.00 +/- 89.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 710      |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 490000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0979  |\n",
      "|    explained_variance | 0.396    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | 0.0249   |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 789      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24500    |\n",
      "|    time_elapsed    | 6665     |\n",
      "|    total_timesteps | 490000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=16.40 +/- 7.09\n",
      "Episode length: 700.00 +/- 120.49\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 700       |\n",
      "|    mean_reward        | 16.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 492000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0304   |\n",
      "|    explained_variance | 0.915     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24599     |\n",
      "|    policy_loss        | -0.000572 |\n",
      "|    value_loss         | 0.0702    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24600    |\n",
      "|    time_elapsed    | 6677     |\n",
      "|    total_timesteps | 492000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=17.80 +/- 3.43\n",
      "Episode length: 775.20 +/- 93.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 775      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 494000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0244  |\n",
      "|    explained_variance | 0.571    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24699    |\n",
      "|    policy_loss        | 0.00446  |\n",
      "|    value_loss         | 0.13     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 24700    |\n",
      "|    time_elapsed    | 6689     |\n",
      "|    total_timesteps | 494000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=18.00 +/- 7.35\n",
      "Episode length: 765.40 +/- 159.78\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 765       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 496000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0192   |\n",
      "|    explained_variance | 0.988     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24799     |\n",
      "|    policy_loss        | -0.000654 |\n",
      "|    value_loss         | 0.0429    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 24800    |\n",
      "|    time_elapsed    | 6700     |\n",
      "|    total_timesteps | 496000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=21.20 +/- 2.86\n",
      "Episode length: 823.40 +/- 104.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 823      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 498000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0252  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | -0.00212 |\n",
      "|    value_loss         | 0.0812   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 24900    |\n",
      "|    time_elapsed    | 6712     |\n",
      "|    total_timesteps | 498000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=15.60 +/- 4.84\n",
      "Episode length: 5928.80 +/- 10535.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.93e+03 |\n",
      "|    mean_reward        | 15.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0457  |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    value_loss         | 0.0418   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 25000    |\n",
      "|    time_elapsed    | 6792     |\n",
      "|    total_timesteps | 500000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=21.60 +/- 7.86\n",
      "Episode length: 841.20 +/- 175.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 502000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0416  |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25099    |\n",
      "|    policy_loss        | -0.00241 |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 25100    |\n",
      "|    time_elapsed    | 6806     |\n",
      "|    total_timesteps | 502000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=18.00 +/- 3.85\n",
      "Episode length: 726.80 +/- 88.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 727      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 504000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0798  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | -0.00209 |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 25200    |\n",
      "|    time_elapsed    | 6818     |\n",
      "|    total_timesteps | 504000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=20.00 +/- 7.62\n",
      "Episode length: 752.60 +/- 117.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 753      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 506000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0379  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | 0.00109  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25300    |\n",
      "|    time_elapsed    | 6830     |\n",
      "|    total_timesteps | 506000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=508000, episode_reward=19.20 +/- 5.34\n",
      "Episode length: 754.40 +/- 117.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 754      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 508000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00839 |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 0.000306 |\n",
      "|    value_loss         | 0.0325   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25400    |\n",
      "|    time_elapsed    | 6842     |\n",
      "|    total_timesteps | 508000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=20.20 +/- 4.49\n",
      "Episode length: 757.60 +/- 90.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 758      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 510000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00493 |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25499    |\n",
      "|    policy_loss        | -0.0001  |\n",
      "|    value_loss         | 0.0152   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25500    |\n",
      "|    time_elapsed    | 6853     |\n",
      "|    total_timesteps | 510000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=19.60 +/- 5.92\n",
      "Episode length: 743.00 +/- 151.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 743      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 512000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0391  |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25599    |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25600    |\n",
      "|    time_elapsed    | 6865     |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=19.60 +/- 3.83\n",
      "Episode length: 725.40 +/- 120.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 725      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 514000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.247    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25700    |\n",
      "|    time_elapsed    | 6876     |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=20.20 +/- 2.14\n",
      "Episode length: 809.80 +/- 45.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 810      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 516000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0755  |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | -0.00361 |\n",
      "|    value_loss         | 0.0735   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25800    |\n",
      "|    time_elapsed    | 6889     |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=23.40 +/- 7.53\n",
      "Episode length: 6072.80 +/- 10464.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.07e+03  |\n",
      "|    mean_reward        | 23.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 518000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0251   |\n",
      "|    explained_variance | 0.949     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25899     |\n",
      "|    policy_loss        | -0.000758 |\n",
      "|    value_loss         | 0.0566    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 25900    |\n",
      "|    time_elapsed    | 6968     |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=19.40 +/- 4.22\n",
      "Episode length: 789.00 +/- 109.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 520000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.045   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 25999    |\n",
      "|    policy_loss        | 0.000779 |\n",
      "|    value_loss         | 0.0185   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 26000    |\n",
      "|    time_elapsed    | 6980     |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=19.40 +/- 5.20\n",
      "Episode length: 721.80 +/- 79.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 522000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0605  |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 26100    |\n",
      "|    time_elapsed    | 6991     |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=25.40 +/- 3.77\n",
      "Episode length: 903.60 +/- 127.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 904       |\n",
      "|    mean_reward        | 25.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 524000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000685 |\n",
      "|    explained_variance | 0.985     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26199     |\n",
      "|    policy_loss        | 4.04e-06  |\n",
      "|    value_loss         | 0.0364    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 26200    |\n",
      "|    time_elapsed    | 7004     |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=21.20 +/- 3.66\n",
      "Episode length: 794.60 +/- 95.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 526000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0362  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | -0.0028  |\n",
      "|    value_loss         | 0.0475   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 26300    |\n",
      "|    time_elapsed    | 7017     |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=26.00 +/- 5.59\n",
      "Episode length: 823.00 +/- 120.94\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 823       |\n",
      "|    mean_reward        | 26        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 528000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00998  |\n",
      "|    explained_variance | 0.882     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26399     |\n",
      "|    policy_loss        | -5.97e-05 |\n",
      "|    value_loss         | 0.0339    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26400    |\n",
      "|    time_elapsed    | 7029     |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=16.20 +/- 5.00\n",
      "Episode length: 716.80 +/- 118.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 717      |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 530000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0345  |\n",
      "|    explained_variance | 0.589    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26500    |\n",
      "|    time_elapsed    | 7041     |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=20.80 +/- 7.49\n",
      "Episode length: 763.00 +/- 153.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 763      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 532000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0568  |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.0599   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26600    |\n",
      "|    time_elapsed    | 7053     |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=17.40 +/- 3.61\n",
      "Episode length: 734.20 +/- 107.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 534000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0145  |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | 0.000248 |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26700    |\n",
      "|    time_elapsed    | 7065     |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=18.40 +/- 5.00\n",
      "Episode length: 719.00 +/- 89.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 719      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 536000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.031   |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | 0.00199  |\n",
      "|    value_loss         | 0.35     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26800    |\n",
      "|    time_elapsed    | 7077     |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=16.40 +/- 5.31\n",
      "Episode length: 674.00 +/- 115.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 674      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 538000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0338  |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | 0.00345  |\n",
      "|    value_loss         | 0.0484   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 26900    |\n",
      "|    time_elapsed    | 7088     |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=20.00 +/- 4.60\n",
      "Episode length: 774.40 +/- 67.19\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 774       |\n",
      "|    mean_reward        | 20        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 540000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98e-05 |\n",
      "|    explained_variance | 0.991     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26999     |\n",
      "|    policy_loss        | -4.43e-09 |\n",
      "|    value_loss         | 0.0112    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27000    |\n",
      "|    time_elapsed    | 7100     |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=542000, episode_reward=18.80 +/- 4.66\n",
      "Episode length: 701.40 +/- 91.13\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 701       |\n",
      "|    mean_reward        | 18.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 542000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00722  |\n",
      "|    explained_variance | 0.898     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27099     |\n",
      "|    policy_loss        | -2.72e-05 |\n",
      "|    value_loss         | 0.0366    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27100    |\n",
      "|    time_elapsed    | 7112     |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=19.20 +/- 4.02\n",
      "Episode length: 719.20 +/- 34.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 719      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 544000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | -0.0512  |\n",
      "|    value_loss         | 0.0708   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27200    |\n",
      "|    time_elapsed    | 7124     |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=17.80 +/- 1.94\n",
      "Episode length: 756.40 +/- 53.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 756      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 546000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0439  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | 0.00253  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27300    |\n",
      "|    time_elapsed    | 7136     |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=17.40 +/- 3.72\n",
      "Episode length: 703.80 +/- 42.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 704      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 548000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0207  |\n",
      "|    explained_variance | 0.515    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | 0.000534 |\n",
      "|    value_loss         | 0.496    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27400    |\n",
      "|    time_elapsed    | 7147     |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=21.00 +/- 6.90\n",
      "Episode length: 792.00 +/- 138.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 550000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | 0.00495  |\n",
      "|    value_loss         | 0.0799   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27500    |\n",
      "|    time_elapsed    | 7159     |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=21.40 +/- 3.20\n",
      "Episode length: 792.80 +/- 96.46\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 793       |\n",
      "|    mean_reward        | 21.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 552000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0169   |\n",
      "|    explained_variance | 0.986     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27599     |\n",
      "|    policy_loss        | -0.000315 |\n",
      "|    value_loss         | 0.0339    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27600    |\n",
      "|    time_elapsed    | 7171     |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=19.20 +/- 4.07\n",
      "Episode length: 781.60 +/- 69.94\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 782       |\n",
      "|    mean_reward        | 19.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 554000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00118  |\n",
      "|    explained_variance | 0.914     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27699     |\n",
      "|    policy_loss        | -3.48e-05 |\n",
      "|    value_loss         | 0.0926    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 27700    |\n",
      "|    time_elapsed    | 7183     |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=18.00 +/- 11.88\n",
      "Episode length: 726.20 +/- 238.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 726      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 556000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.149   |\n",
      "|    explained_variance | 0.211    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | 0.0338   |\n",
      "|    value_loss         | 0.266    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 27800    |\n",
      "|    time_elapsed    | 7194     |\n",
      "|    total_timesteps | 556000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=18.40 +/- 2.73\n",
      "Episode length: 6015.00 +/- 10492.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 558000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0665  |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27899    |\n",
      "|    policy_loss        | -0.0333  |\n",
      "|    value_loss         | 0.508    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 27900    |\n",
      "|    time_elapsed    | 7274     |\n",
      "|    total_timesteps | 558000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=20.40 +/- 5.46\n",
      "Episode length: 791.40 +/- 120.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 791      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 560000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0812  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 27999    |\n",
      "|    policy_loss        | 0.0161   |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 28000    |\n",
      "|    time_elapsed    | 7285     |\n",
      "|    total_timesteps | 560000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=20.60 +/- 6.05\n",
      "Episode length: 794.20 +/- 141.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 794      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 562000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00991 |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | 0.000314 |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 19.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28100    |\n",
      "|    time_elapsed    | 7297     |\n",
      "|    total_timesteps | 562000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=17.40 +/- 1.96\n",
      "Episode length: 769.60 +/- 50.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 564000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00348 |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | 0.000244 |\n",
      "|    value_loss         | 0.0967   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 20.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28200    |\n",
      "|    time_elapsed    | 7309     |\n",
      "|    total_timesteps | 564000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=19.20 +/- 4.40\n",
      "Episode length: 817.80 +/- 148.24\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 818       |\n",
      "|    mean_reward        | 19.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 566000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0176   |\n",
      "|    explained_variance | 0.95      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28299     |\n",
      "|    policy_loss        | -0.000236 |\n",
      "|    value_loss         | 0.022     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28300    |\n",
      "|    time_elapsed    | 7321     |\n",
      "|    total_timesteps | 566000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=20.60 +/- 1.85\n",
      "Episode length: 799.80 +/- 75.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 800      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 568000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.021   |\n",
      "|    explained_variance | 0.897    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28399    |\n",
      "|    policy_loss        | 0.00127  |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 790      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28400    |\n",
      "|    time_elapsed    | 7334     |\n",
      "|    total_timesteps | 568000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=17.20 +/- 3.49\n",
      "Episode length: 692.40 +/- 39.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 692      |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 570000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0303  |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | 0.000419 |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28500    |\n",
      "|    time_elapsed    | 7345     |\n",
      "|    total_timesteps | 570000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=25.80 +/- 7.93\n",
      "Episode length: 806.80 +/- 114.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 807       |\n",
      "|    mean_reward        | 25.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 572000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000325 |\n",
      "|    explained_variance | 0.937     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28599     |\n",
      "|    policy_loss        | -1.38e-05 |\n",
      "|    value_loss         | 0.122     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28600    |\n",
      "|    time_elapsed    | 7358     |\n",
      "|    total_timesteps | 572000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=23.40 +/- 7.00\n",
      "Episode length: 787.60 +/- 85.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 574000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | 0.0034   |\n",
      "|    value_loss         | 0.0886   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28700    |\n",
      "|    time_elapsed    | 7370     |\n",
      "|    total_timesteps | 574000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=576000, episode_reward=20.00 +/- 4.98\n",
      "Episode length: 6057.40 +/- 10471.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 576000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0328  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | -0.00796 |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 28800    |\n",
      "|    time_elapsed    | 7451     |\n",
      "|    total_timesteps | 576000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=16.60 +/- 6.71\n",
      "Episode length: 5951.00 +/- 10524.95\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.95e+03  |\n",
      "|    mean_reward        | 16.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 578000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.81e-05 |\n",
      "|    explained_variance | 0.982     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28899     |\n",
      "|    policy_loss        | -1.54e-08 |\n",
      "|    value_loss         | 0.0191    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 28900    |\n",
      "|    time_elapsed    | 7531     |\n",
      "|    total_timesteps | 578000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=21.80 +/- 6.27\n",
      "Episode length: 749.60 +/- 163.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 580000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0895  |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.273    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29000    |\n",
      "|    time_elapsed    | 7542     |\n",
      "|    total_timesteps | 580000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=19.60 +/- 3.20\n",
      "Episode length: 798.40 +/- 48.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 582000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0567  |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | -0.00323 |\n",
      "|    value_loss         | 0.43     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 789      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 29100    |\n",
      "|    time_elapsed    | 7554     |\n",
      "|    total_timesteps | 582000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=21.60 +/- 4.54\n",
      "Episode length: 815.60 +/- 85.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 816      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 584000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0269  |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | -0.0527  |\n",
      "|    value_loss         | 0.16     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 29200    |\n",
      "|    time_elapsed    | 7567     |\n",
      "|    total_timesteps | 584000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=17.20 +/- 3.19\n",
      "Episode length: 5951.20 +/- 10524.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 586000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0737  |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | -0.00921 |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29300    |\n",
      "|    time_elapsed    | 7647     |\n",
      "|    total_timesteps | 586000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=20.40 +/- 4.96\n",
      "Episode length: 756.80 +/- 69.78\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 757       |\n",
      "|    mean_reward        | 20.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 588000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.51e-05 |\n",
      "|    explained_variance | 0.536     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29399     |\n",
      "|    policy_loss        | 1.61e-06  |\n",
      "|    value_loss         | 0.294     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29400    |\n",
      "|    time_elapsed    | 7658     |\n",
      "|    total_timesteps | 588000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=21.80 +/- 8.42\n",
      "Episode length: 777.20 +/- 146.81\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 777       |\n",
      "|    mean_reward        | 21.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 590000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00309  |\n",
      "|    explained_variance | 0.68      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29499     |\n",
      "|    policy_loss        | -5.14e-05 |\n",
      "|    value_loss         | 0.0697    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29500    |\n",
      "|    time_elapsed    | 7669     |\n",
      "|    total_timesteps | 590000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=18.00 +/- 7.59\n",
      "Episode length: 700.40 +/- 137.07\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 700       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 592000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000783 |\n",
      "|    explained_variance | 0.933     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29599     |\n",
      "|    policy_loss        | -2.05e-06 |\n",
      "|    value_loss         | 0.0155    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 29600    |\n",
      "|    time_elapsed    | 7681     |\n",
      "|    total_timesteps | 592000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=21.40 +/- 3.88\n",
      "Episode length: 838.40 +/- 87.03\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 838       |\n",
      "|    mean_reward        | 21.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 594000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00478  |\n",
      "|    explained_variance | 0.991     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29699     |\n",
      "|    policy_loss        | -8.73e-06 |\n",
      "|    value_loss         | 0.0319    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 77       |\n",
      "|    iterations      | 29700    |\n",
      "|    time_elapsed    | 7694     |\n",
      "|    total_timesteps | 594000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=15.00 +/- 9.27\n",
      "Episode length: 6013.20 +/- 10493.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 596000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0596  |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | 0.0389   |\n",
      "|    value_loss         | 0.0965   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29800    |\n",
      "|    time_elapsed    | 7775     |\n",
      "|    total_timesteps | 596000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=15.00 +/- 6.42\n",
      "Episode length: 11294.60 +/- 12823.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 598000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0316  |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | 0.109    |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 29900    |\n",
      "|    time_elapsed    | 7855     |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=24.80 +/- 7.44\n",
      "Episode length: 811.40 +/- 199.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 811      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0377  |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | 0.00414  |\n",
      "|    value_loss         | 0.0814   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 30000    |\n",
      "|    time_elapsed    | 7867     |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=18.20 +/- 5.60\n",
      "Episode length: 5952.80 +/- 10523.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 602000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00695 |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30099    |\n",
      "|    policy_loss        | 3.82e-05 |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30100    |\n",
      "|    time_elapsed    | 7947     |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=17.40 +/- 2.50\n",
      "Episode length: 729.20 +/- 127.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 729      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 604000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0672  |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30199    |\n",
      "|    policy_loss        | -0.0097  |\n",
      "|    value_loss         | 0.276    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30200    |\n",
      "|    time_elapsed    | 7957     |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=15.00 +/- 4.60\n",
      "Episode length: 5955.60 +/- 10522.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 606000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0107  |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30299    |\n",
      "|    policy_loss        | -0.00107 |\n",
      "|    value_loss         | 0.363    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30300    |\n",
      "|    time_elapsed    | 8037     |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=20.00 +/- 5.59\n",
      "Episode length: 734.20 +/- 116.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 608000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0379  |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30399    |\n",
      "|    policy_loss        | 0.000834 |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30400    |\n",
      "|    time_elapsed    | 8049     |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=19.80 +/- 5.84\n",
      "Episode length: 770.20 +/- 123.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 610000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0821  |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30499    |\n",
      "|    policy_loss        | -0.0287  |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30500    |\n",
      "|    time_elapsed    | 8060     |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=21.00 +/- 2.97\n",
      "Episode length: 768.40 +/- 104.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 612000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30599    |\n",
      "|    policy_loss        | 0.000435 |\n",
      "|    value_loss         | 0.0575   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30600    |\n",
      "|    time_elapsed    | 8072     |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=18.80 +/- 4.35\n",
      "Episode length: 760.00 +/- 106.34\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 760       |\n",
      "|    mean_reward        | 18.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 614000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00018  |\n",
      "|    explained_variance | 0.326     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30699     |\n",
      "|    policy_loss        | -6.38e-06 |\n",
      "|    value_loss         | 0.249     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 20.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30700    |\n",
      "|    time_elapsed    | 8084     |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=18.00 +/- 8.51\n",
      "Episode length: 11294.20 +/- 12823.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 616000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0952  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30799    |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0564   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30800    |\n",
      "|    time_elapsed    | 8163     |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=17.40 +/- 1.62\n",
      "Episode length: 717.80 +/- 72.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 718      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 618000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30899    |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 30900    |\n",
      "|    time_elapsed    | 8174     |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=17.00 +/- 4.43\n",
      "Episode length: 699.60 +/- 112.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 700      |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 620000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0346  |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 30999    |\n",
      "|    policy_loss        | -0.0349  |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31000    |\n",
      "|    time_elapsed    | 8186     |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=14.40 +/- 8.26\n",
      "Episode length: 5956.80 +/- 10522.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 622000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.085   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31099    |\n",
      "|    policy_loss        | 0.00362  |\n",
      "|    value_loss         | 0.042    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31100    |\n",
      "|    time_elapsed    | 8267     |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=21.00 +/- 4.05\n",
      "Episode length: 6027.40 +/- 10486.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.03e+03  |\n",
      "|    mean_reward        | 21        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 624000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.019    |\n",
      "|    explained_variance | 0.928     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31199     |\n",
      "|    policy_loss        | -0.000859 |\n",
      "|    value_loss         | 0.056     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 31200    |\n",
      "|    time_elapsed    | 8347     |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=24.40 +/- 7.14\n",
      "Episode length: 900.40 +/- 104.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 900      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 626000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31299    |\n",
      "|    policy_loss        | -0.00792 |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 31300    |\n",
      "|    time_elapsed    | 8358     |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=19.80 +/- 4.58\n",
      "Episode length: 790.80 +/- 85.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 791      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 628000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0643  |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | 0.00637  |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31400    |\n",
      "|    time_elapsed    | 8370     |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=24.60 +/- 10.07\n",
      "Episode length: 857.60 +/- 208.15\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 858       |\n",
      "|    mean_reward        | 24.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 630000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00767  |\n",
      "|    explained_variance | 0.879     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31499     |\n",
      "|    policy_loss        | -0.000371 |\n",
      "|    value_loss         | 0.0922    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31500    |\n",
      "|    time_elapsed    | 8381     |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=632000, episode_reward=26.40 +/- 5.78\n",
      "Episode length: 913.40 +/- 154.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 913      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 632000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0938  |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31599    |\n",
      "|    policy_loss        | 0.00498  |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31600    |\n",
      "|    time_elapsed    | 8394     |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=17.40 +/- 2.24\n",
      "Episode length: 802.80 +/- 51.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 803      |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 634000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00737 |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31699    |\n",
      "|    policy_loss        | 0.000605 |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31700    |\n",
      "|    time_elapsed    | 8407     |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=22.00 +/- 5.55\n",
      "Episode length: 802.40 +/- 105.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 802      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 636000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0867  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31799    |\n",
      "|    policy_loss        | 0.00128  |\n",
      "|    value_loss         | 0.0612   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31800    |\n",
      "|    time_elapsed    | 8419     |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=14.20 +/- 8.23\n",
      "Episode length: 11255.20 +/- 12855.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 14.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 638000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0418  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31899    |\n",
      "|    policy_loss        | -0.00206 |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 31900    |\n",
      "|    time_elapsed    | 8498     |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=21.80 +/- 2.64\n",
      "Episode length: 787.60 +/- 74.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 640000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.495    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31999    |\n",
      "|    policy_loss        | -0.22    |\n",
      "|    value_loss         | 1.35     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32000    |\n",
      "|    time_elapsed    | 8509     |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=17.60 +/- 3.38\n",
      "Episode length: 754.00 +/- 107.68\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 754       |\n",
      "|    mean_reward        | 17.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 642000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0807   |\n",
      "|    explained_variance | 0.375     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32099     |\n",
      "|    policy_loss        | -2.34e-06 |\n",
      "|    value_loss         | 0.14      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32100    |\n",
      "|    time_elapsed    | 8521     |\n",
      "|    total_timesteps | 642000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=18.20 +/- 1.94\n",
      "Episode length: 771.20 +/- 63.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 771      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 644000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0425  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32199    |\n",
      "|    policy_loss        | -0.00764 |\n",
      "|    value_loss         | 0.0496   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32200    |\n",
      "|    time_elapsed    | 8533     |\n",
      "|    total_timesteps | 644000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=19.60 +/- 2.94\n",
      "Episode length: 824.40 +/- 121.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 824      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 646000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0514  |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32299    |\n",
      "|    policy_loss        | -0.0252  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32300    |\n",
      "|    time_elapsed    | 8546     |\n",
      "|    total_timesteps | 646000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=20.20 +/- 3.76\n",
      "Episode length: 765.80 +/- 108.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 648000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0794  |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32399    |\n",
      "|    policy_loss        | -0.0285  |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32400    |\n",
      "|    time_elapsed    | 8558     |\n",
      "|    total_timesteps | 648000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=20.20 +/- 5.64\n",
      "Episode length: 763.80 +/- 146.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 764      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 650000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.138   |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32499    |\n",
      "|    policy_loss        | -0.00268 |\n",
      "|    value_loss         | 0.0987   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32500    |\n",
      "|    time_elapsed    | 8570     |\n",
      "|    total_timesteps | 650000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=18.60 +/- 4.50\n",
      "Episode length: 790.00 +/- 97.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 790      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 652000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.094   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32599    |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 32600    |\n",
      "|    time_elapsed    | 8582     |\n",
      "|    total_timesteps | 652000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=19.00 +/- 3.35\n",
      "Episode length: 785.60 +/- 85.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 654000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0653  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32699    |\n",
      "|    policy_loss        | 0.000659 |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 32700    |\n",
      "|    time_elapsed    | 8594     |\n",
      "|    total_timesteps | 654000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=19.20 +/- 4.83\n",
      "Episode length: 770.00 +/- 106.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 656000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0175  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32799    |\n",
      "|    policy_loss        | 0.000156 |\n",
      "|    value_loss         | 0.0588   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 32800    |\n",
      "|    time_elapsed    | 8606     |\n",
      "|    total_timesteps | 656000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=20.60 +/- 4.41\n",
      "Episode length: 789.00 +/- 69.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 658000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0549  |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32899    |\n",
      "|    policy_loss        | -0.0407  |\n",
      "|    value_loss         | 0.0428   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 32900    |\n",
      "|    time_elapsed    | 8619     |\n",
      "|    total_timesteps | 658000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=20.00 +/- 3.95\n",
      "Episode length: 795.00 +/- 98.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 660000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00436 |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 32999    |\n",
      "|    policy_loss        | 0.000106 |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 20.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33000    |\n",
      "|    time_elapsed    | 8631     |\n",
      "|    total_timesteps | 660000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=20.60 +/- 3.07\n",
      "Episode length: 841.00 +/- 52.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 662000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0465  |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33099    |\n",
      "|    policy_loss        | 0.00847  |\n",
      "|    value_loss         | 0.0821   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33100    |\n",
      "|    time_elapsed    | 8643     |\n",
      "|    total_timesteps | 662000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=14.40 +/- 4.36\n",
      "Episode length: 5952.80 +/- 10523.91\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.95e+03  |\n",
      "|    mean_reward        | 14.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 664000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0139   |\n",
      "|    explained_variance | 0.994     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33199     |\n",
      "|    policy_loss        | -6.23e-05 |\n",
      "|    value_loss         | 0.00945   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33200    |\n",
      "|    time_elapsed    | 8724     |\n",
      "|    total_timesteps | 664000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=666000, episode_reward=18.60 +/- 4.88\n",
      "Episode length: 730.80 +/- 104.80\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 731       |\n",
      "|    mean_reward        | 18.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 666000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0374   |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33299     |\n",
      "|    policy_loss        | -0.000826 |\n",
      "|    value_loss         | 0.114     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33300    |\n",
      "|    time_elapsed    | 8735     |\n",
      "|    total_timesteps | 666000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=19.60 +/- 3.77\n",
      "Episode length: 796.00 +/- 37.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 796      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 668000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0476  |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33399    |\n",
      "|    policy_loss        | 0.00132  |\n",
      "|    value_loss         | 0.0358   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33400    |\n",
      "|    time_elapsed    | 8747     |\n",
      "|    total_timesteps | 668000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=18.40 +/- 1.74\n",
      "Episode length: 6009.20 +/- 10495.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 670000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0706  |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33499    |\n",
      "|    policy_loss        | 0.00407  |\n",
      "|    value_loss         | 0.292    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 33500    |\n",
      "|    time_elapsed    | 8826     |\n",
      "|    total_timesteps | 670000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=19.00 +/- 7.16\n",
      "Episode length: 768.80 +/- 137.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 769      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 672000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33599    |\n",
      "|    policy_loss        | 0.00453  |\n",
      "|    value_loss         | 0.0731   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 33600    |\n",
      "|    time_elapsed    | 8838     |\n",
      "|    total_timesteps | 672000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=18.80 +/- 6.62\n",
      "Episode length: 6059.60 +/- 10470.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 674000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0443  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33699    |\n",
      "|    policy_loss        | -0.0172  |\n",
      "|    value_loss         | 0.125    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 33700    |\n",
      "|    time_elapsed    | 8918     |\n",
      "|    total_timesteps | 674000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=19.20 +/- 4.92\n",
      "Episode length: 756.00 +/- 105.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 756      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 676000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0425  |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33799    |\n",
      "|    policy_loss        | -0.00687 |\n",
      "|    value_loss         | 0.0599   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 33800    |\n",
      "|    time_elapsed    | 8929     |\n",
      "|    total_timesteps | 676000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=18.00 +/- 6.03\n",
      "Episode length: 755.20 +/- 125.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 755      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 678000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0433  |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33899    |\n",
      "|    policy_loss        | -0.028   |\n",
      "|    value_loss         | 0.172    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 33900    |\n",
      "|    time_elapsed    | 8941     |\n",
      "|    total_timesteps | 678000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=22.80 +/- 2.14\n",
      "Episode length: 793.40 +/- 109.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 793      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 680000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.035   |\n",
      "|    explained_variance | 0.785    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | 0.0123   |\n",
      "|    value_loss         | 0.601    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 34000    |\n",
      "|    time_elapsed    | 8952     |\n",
      "|    total_timesteps | 680000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=19.20 +/- 7.63\n",
      "Episode length: 749.40 +/- 99.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 749      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 682000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0275  |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34099    |\n",
      "|    policy_loss        | -0.00624 |\n",
      "|    value_loss         | 0.355    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34100    |\n",
      "|    time_elapsed    | 8964     |\n",
      "|    total_timesteps | 682000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=19.00 +/- 4.98\n",
      "Episode length: 770.80 +/- 82.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 771      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 684000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | -0.00273 |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34200    |\n",
      "|    time_elapsed    | 8976     |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=19.60 +/- 5.43\n",
      "Episode length: 738.00 +/- 111.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 738      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 686000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0657  |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34299    |\n",
      "|    policy_loss        | 0.00416  |\n",
      "|    value_loss         | 0.0936   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34300    |\n",
      "|    time_elapsed    | 8988     |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=19.20 +/- 6.52\n",
      "Episode length: 786.60 +/- 155.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 688000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0735  |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34399    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.109    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34400    |\n",
      "|    time_elapsed    | 9001     |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=24.00 +/- 6.32\n",
      "Episode length: 862.00 +/- 177.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 690000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34499    |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34500    |\n",
      "|    time_elapsed    | 9014     |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=18.00 +/- 3.95\n",
      "Episode length: 775.60 +/- 78.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 692000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.15    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34599    |\n",
      "|    policy_loss        | 0.0589   |\n",
      "|    value_loss         | 0.217    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34600    |\n",
      "|    time_elapsed    | 9026     |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=19.80 +/- 7.78\n",
      "Episode length: 6001.20 +/- 10500.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6e+03    |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 694000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0245  |\n",
      "|    explained_variance | 0.593    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34699    |\n",
      "|    policy_loss        | 0.000582 |\n",
      "|    value_loss         | 0.608    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 34700    |\n",
      "|    time_elapsed    | 9107     |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=16.00 +/- 6.42\n",
      "Episode length: 6046.80 +/- 10476.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 696000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34799    |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 34800    |\n",
      "|    time_elapsed    | 9189     |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=18.60 +/- 10.93\n",
      "Episode length: 6039.40 +/- 10480.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 698000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0564  |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34899    |\n",
      "|    policy_loss        | -0.00394 |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 34900    |\n",
      "|    time_elapsed    | 9270     |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=19.40 +/- 4.84\n",
      "Episode length: 754.20 +/- 137.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 754      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0775  |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 34999    |\n",
      "|    policy_loss        | 0.000364 |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35000    |\n",
      "|    time_elapsed    | 9281     |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=23.00 +/- 4.98\n",
      "Episode length: 861.20 +/- 142.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 861      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 702000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00343 |\n",
      "|    explained_variance | 0.704    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35099    |\n",
      "|    policy_loss        | 6.79e-05 |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35100    |\n",
      "|    time_elapsed    | 9294     |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=23.00 +/- 2.83\n",
      "Episode length: 6093.80 +/- 10453.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 704000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0319  |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35199    |\n",
      "|    policy_loss        | 3.56e-05 |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35200    |\n",
      "|    time_elapsed    | 9373     |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=19.40 +/- 4.50\n",
      "Episode length: 796.20 +/- 84.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 796      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 706000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0816  |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35299    |\n",
      "|    policy_loss        | 0.0251   |\n",
      "|    value_loss         | 0.0575   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35300    |\n",
      "|    time_elapsed    | 9384     |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=20.60 +/- 4.22\n",
      "Episode length: 785.60 +/- 89.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 708000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0698  |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35399    |\n",
      "|    policy_loss        | -0.134   |\n",
      "|    value_loss         | 0.368    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35400    |\n",
      "|    time_elapsed    | 9396     |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=19.40 +/- 7.34\n",
      "Episode length: 6048.20 +/- 10475.97\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.05e+03  |\n",
      "|    mean_reward        | 19.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 710000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.32e-05 |\n",
      "|    explained_variance | 0.897     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35499     |\n",
      "|    policy_loss        | -1.65e-08 |\n",
      "|    value_loss         | 0.0476    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 35500    |\n",
      "|    time_elapsed    | 9477     |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=712000, episode_reward=18.80 +/- 3.87\n",
      "Episode length: 844.00 +/- 112.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 844      |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 712000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35599    |\n",
      "|    policy_loss        | 0.0219   |\n",
      "|    value_loss         | 0.629    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35600    |\n",
      "|    time_elapsed    | 9488     |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=21.60 +/- 4.41\n",
      "Episode length: 819.00 +/- 137.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 714000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0378  |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35699    |\n",
      "|    policy_loss        | 0.0725   |\n",
      "|    value_loss         | 0.818    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35700    |\n",
      "|    time_elapsed    | 9500     |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=19.60 +/- 3.26\n",
      "Episode length: 760.80 +/- 77.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 716000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.696    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35799    |\n",
      "|    policy_loss        | -0.0128  |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35800    |\n",
      "|    time_elapsed    | 9512     |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=25.40 +/- 5.57\n",
      "Episode length: 878.60 +/- 122.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 879      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 718000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.105   |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35899    |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    value_loss         | 0.211    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 35900    |\n",
      "|    time_elapsed    | 9525     |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=18.80 +/- 10.63\n",
      "Episode length: 6079.80 +/- 10460.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.11    |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 35999    |\n",
      "|    policy_loss        | -0.0333  |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 36000    |\n",
      "|    time_elapsed    | 9607     |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=21.00 +/- 5.06\n",
      "Episode length: 773.00 +/- 88.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 773      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 722000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00229 |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | 8.03e-05 |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36100    |\n",
      "|    time_elapsed    | 9619     |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=20.60 +/- 9.33\n",
      "Episode length: 796.20 +/- 220.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 796      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 724000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | -0.214   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36199    |\n",
      "|    policy_loss        | 0.0969   |\n",
      "|    value_loss         | 0.17     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36200    |\n",
      "|    time_elapsed    | 9630     |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=23.00 +/- 7.35\n",
      "Episode length: 864.00 +/- 161.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 864      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 726000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0546  |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36299    |\n",
      "|    policy_loss        | 0.00983  |\n",
      "|    value_loss         | 0.0954   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36300    |\n",
      "|    time_elapsed    | 9642     |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=22.60 +/- 4.36\n",
      "Episode length: 822.80 +/- 146.93\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 823       |\n",
      "|    mean_reward        | 22.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 728000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00551  |\n",
      "|    explained_variance | 0.701     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36399     |\n",
      "|    policy_loss        | -2.43e-05 |\n",
      "|    value_loss         | 0.225     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36400    |\n",
      "|    time_elapsed    | 9654     |\n",
      "|    total_timesteps | 728000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=21.60 +/- 5.61\n",
      "Episode length: 742.00 +/- 85.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 742      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 730000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0169  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36499    |\n",
      "|    policy_loss        | 0.0059   |\n",
      "|    value_loss         | 0.0633   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36500    |\n",
      "|    time_elapsed    | 9666     |\n",
      "|    total_timesteps | 730000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=20.00 +/- 5.93\n",
      "Episode length: 750.00 +/- 118.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 750      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 732000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0508  |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | 0.061    |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36600    |\n",
      "|    time_elapsed    | 9678     |\n",
      "|    total_timesteps | 732000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=21.20 +/- 9.13\n",
      "Episode length: 809.00 +/- 159.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 809       |\n",
      "|    mean_reward        | 21.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 734000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000432 |\n",
      "|    explained_variance | 0.966     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36699     |\n",
      "|    policy_loss        | -6.22e-06 |\n",
      "|    value_loss         | 0.0623    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36700    |\n",
      "|    time_elapsed    | 9690     |\n",
      "|    total_timesteps | 734000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=22.20 +/- 7.47\n",
      "Episode length: 789.40 +/- 138.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 736000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.064   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36799    |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36800    |\n",
      "|    time_elapsed    | 9702     |\n",
      "|    total_timesteps | 736000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=21.60 +/- 5.89\n",
      "Episode length: 827.80 +/- 121.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 738000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.08    |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36899    |\n",
      "|    policy_loss        | 0.0286   |\n",
      "|    value_loss         | 0.0608   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 36900    |\n",
      "|    time_elapsed    | 9714     |\n",
      "|    total_timesteps | 738000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=19.00 +/- 4.69\n",
      "Episode length: 11311.60 +/- 12809.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 740000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0751  |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36999    |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37000    |\n",
      "|    time_elapsed    | 9792     |\n",
      "|    total_timesteps | 740000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=19.00 +/- 5.80\n",
      "Episode length: 778.00 +/- 145.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 742000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0454  |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37099    |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.17     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37100    |\n",
      "|    time_elapsed    | 9804     |\n",
      "|    total_timesteps | 742000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=20.80 +/- 5.74\n",
      "Episode length: 810.60 +/- 157.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 811      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 744000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0587  |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37199    |\n",
      "|    policy_loss        | -0.00737 |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37200    |\n",
      "|    time_elapsed    | 9816     |\n",
      "|    total_timesteps | 744000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=746000, episode_reward=22.40 +/- 8.40\n",
      "Episode length: 799.40 +/- 137.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 799       |\n",
      "|    mean_reward        | 22.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 746000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000712 |\n",
      "|    explained_variance | 0.852     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37299     |\n",
      "|    policy_loss        | 3.83e-05  |\n",
      "|    value_loss         | 0.236     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37300    |\n",
      "|    time_elapsed    | 9829     |\n",
      "|    total_timesteps | 746000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=25.20 +/- 6.88\n",
      "Episode length: 905.40 +/- 156.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 905      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 748000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37399    |\n",
      "|    policy_loss        | 0.00519  |\n",
      "|    value_loss         | 0.0483   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 37400    |\n",
      "|    time_elapsed    | 9841     |\n",
      "|    total_timesteps | 748000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=27.20 +/- 5.56\n",
      "Episode length: 892.40 +/- 90.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 892      |\n",
      "|    mean_reward        | 27.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 750000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0414  |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37499    |\n",
      "|    policy_loss        | -0.00562 |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 37500    |\n",
      "|    time_elapsed    | 9854     |\n",
      "|    total_timesteps | 750000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=18.60 +/- 4.63\n",
      "Episode length: 772.20 +/- 73.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 772      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 752000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0252  |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37599    |\n",
      "|    policy_loss        | -0.00275 |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 37600    |\n",
      "|    time_elapsed    | 9867     |\n",
      "|    total_timesteps | 752000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=19.20 +/- 3.25\n",
      "Episode length: 680.00 +/- 68.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 680      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 754000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0981  |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37699    |\n",
      "|    policy_loss        | 0.00263  |\n",
      "|    value_loss         | 0.0804   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 37700    |\n",
      "|    time_elapsed    | 9878     |\n",
      "|    total_timesteps | 754000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=19.40 +/- 9.56\n",
      "Episode length: 6037.60 +/- 10481.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 756000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0545  |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37799    |\n",
      "|    policy_loss        | 0.0262   |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37800    |\n",
      "|    time_elapsed    | 9960     |\n",
      "|    total_timesteps | 756000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=18.60 +/- 4.27\n",
      "Episode length: 6067.20 +/- 10466.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 758000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.042   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | 0.0233   |\n",
      "|    value_loss         | 0.0552   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 21.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 37900    |\n",
      "|    time_elapsed    | 10042    |\n",
      "|    total_timesteps | 758000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=22.40 +/- 5.20\n",
      "Episode length: 862.40 +/- 124.59\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 862       |\n",
      "|    mean_reward        | 22.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 760000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00354  |\n",
      "|    explained_variance | -0.668    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37999     |\n",
      "|    policy_loss        | -9.77e-05 |\n",
      "|    value_loss         | 0.224     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38000    |\n",
      "|    time_elapsed    | 10055    |\n",
      "|    total_timesteps | 760000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=23.20 +/- 5.27\n",
      "Episode length: 824.60 +/- 62.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 825      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 762000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.144   |\n",
      "|    explained_variance | 0.66     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38099    |\n",
      "|    policy_loss        | 0.000644 |\n",
      "|    value_loss         | 0.257    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38100    |\n",
      "|    time_elapsed    | 10067    |\n",
      "|    total_timesteps | 762000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=25.20 +/- 3.87\n",
      "Episode length: 871.60 +/- 86.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 872      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 764000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0346  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | -0.00292 |\n",
      "|    value_loss         | 0.064    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38200    |\n",
      "|    time_elapsed    | 10079    |\n",
      "|    total_timesteps | 764000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=25.20 +/- 8.93\n",
      "Episode length: 798.40 +/- 113.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 766000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0458  |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38299    |\n",
      "|    policy_loss        | 0.000967 |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38300    |\n",
      "|    time_elapsed    | 10092    |\n",
      "|    total_timesteps | 766000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=19.80 +/- 10.51\n",
      "Episode length: 6092.60 +/- 10455.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 768000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0676  |\n",
      "|    explained_variance | 0.515    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38399    |\n",
      "|    policy_loss        | 0.00858  |\n",
      "|    value_loss         | 0.0677   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38400    |\n",
      "|    time_elapsed    | 10173    |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=27.20 +/- 2.93\n",
      "Episode length: 923.00 +/- 111.14\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 923       |\n",
      "|    mean_reward        | 27.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 770000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.117    |\n",
      "|    explained_variance | 0.98      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38499     |\n",
      "|    policy_loss        | -0.000578 |\n",
      "|    value_loss         | 0.0118    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38500    |\n",
      "|    time_elapsed    | 10186    |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=19.40 +/- 4.45\n",
      "Episode length: 782.40 +/- 97.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 782      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 772000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0152  |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38599    |\n",
      "|    policy_loss        | 0.00106  |\n",
      "|    value_loss         | 0.291    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38600    |\n",
      "|    time_elapsed    | 10198    |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=16.60 +/- 2.94\n",
      "Episode length: 789.20 +/- 93.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 774000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0634  |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38699    |\n",
      "|    policy_loss        | 0.0767   |\n",
      "|    value_loss         | 0.0653   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38700    |\n",
      "|    time_elapsed    | 10210    |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=22.20 +/- 4.62\n",
      "Episode length: 796.80 +/- 53.88\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 797      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 776000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0259  |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38799    |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    value_loss         | 0.0908   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 38800    |\n",
      "|    time_elapsed    | 10223    |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=19.40 +/- 7.00\n",
      "Episode length: 761.00 +/- 156.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 778000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0547  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38899    |\n",
      "|    policy_loss        | -0.00598 |\n",
      "|    value_loss         | 0.0553   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 38900    |\n",
      "|    time_elapsed    | 10236    |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=23.20 +/- 7.63\n",
      "Episode length: 841.60 +/- 73.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 842      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 780000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00124 |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 38999    |\n",
      "|    policy_loss        | -3e-05   |\n",
      "|    value_loss         | 0.0517   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 20.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39000    |\n",
      "|    time_elapsed    | 10248    |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=22.40 +/- 7.34\n",
      "Episode length: 846.00 +/- 166.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 846      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 782000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0729  |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39099    |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 0.0672   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39100    |\n",
      "|    time_elapsed    | 10260    |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=19.80 +/- 4.92\n",
      "Episode length: 764.40 +/- 121.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 764      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 784000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0731  |\n",
      "|    explained_variance | -0.618   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39199    |\n",
      "|    policy_loss        | 0.0353   |\n",
      "|    value_loss         | 0.656    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 21.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39200    |\n",
      "|    time_elapsed    | 10272    |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=22.20 +/- 7.08\n",
      "Episode length: 835.80 +/- 183.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 786000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0881  |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | 0.0392   |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39300    |\n",
      "|    time_elapsed    | 10283    |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=21.20 +/- 5.74\n",
      "Episode length: 750.40 +/- 112.12\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 750       |\n",
      "|    mean_reward        | 21.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 788000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000278 |\n",
      "|    explained_variance | 0.98      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39399     |\n",
      "|    policy_loss        | 2.22e-06  |\n",
      "|    value_loss         | 0.0331    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39400    |\n",
      "|    time_elapsed    | 10295    |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=22.80 +/- 4.12\n",
      "Episode length: 724.20 +/- 86.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 724      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 790000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.037   |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39499    |\n",
      "|    policy_loss        | 0.000475 |\n",
      "|    value_loss         | 0.0656   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39500    |\n",
      "|    time_elapsed    | 10306    |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=792000, episode_reward=21.20 +/- 7.83\n",
      "Episode length: 776.00 +/- 119.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 792000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0369  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39599    |\n",
      "|    policy_loss        | 0.0208   |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39600    |\n",
      "|    time_elapsed    | 10319    |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=18.00 +/- 5.18\n",
      "Episode length: 761.20 +/- 110.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 794000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.084   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39699    |\n",
      "|    policy_loss        | 0.0372   |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39700    |\n",
      "|    time_elapsed    | 10331    |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=21.20 +/- 1.33\n",
      "Episode length: 814.40 +/- 106.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 814      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 796000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0717  |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | -0.00183 |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39800    |\n",
      "|    time_elapsed    | 10344    |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=19.20 +/- 6.21\n",
      "Episode length: 6057.40 +/- 10471.88\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.06e+03  |\n",
      "|    mean_reward        | 19.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 798000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.02     |\n",
      "|    explained_variance | 0.775     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39899     |\n",
      "|    policy_loss        | -0.000623 |\n",
      "|    value_loss         | 0.142     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 39900    |\n",
      "|    time_elapsed    | 10424    |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=23.00 +/- 2.97\n",
      "Episode length: 820.00 +/- 106.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 820      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0546  |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 39999    |\n",
      "|    policy_loss        | 0.0138   |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40000    |\n",
      "|    time_elapsed    | 10435    |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=19.00 +/- 2.10\n",
      "Episode length: 790.00 +/- 27.17\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 790       |\n",
      "|    mean_reward        | 19        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 802000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.37e-05 |\n",
      "|    explained_variance | 0.972     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40099     |\n",
      "|    policy_loss        | 6.49e-07  |\n",
      "|    value_loss         | 0.0378    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40100    |\n",
      "|    time_elapsed    | 10447    |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=23.40 +/- 15.15\n",
      "Episode length: 6170.00 +/- 10416.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 804000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0947  |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | -0.00813 |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40200    |\n",
      "|    time_elapsed    | 10528    |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=18.00 +/- 6.96\n",
      "Episode length: 694.20 +/- 136.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 694      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 806000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0664  |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | 0.031    |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40300    |\n",
      "|    time_elapsed    | 10538    |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=15.80 +/- 4.62\n",
      "Episode length: 711.60 +/- 76.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 712      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 808000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0966  |\n",
      "|    explained_variance | 0.526    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40399    |\n",
      "|    policy_loss        | 0.00862  |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40400    |\n",
      "|    time_elapsed    | 10550    |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=24.20 +/- 5.64\n",
      "Episode length: 844.20 +/- 116.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 844      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 810000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0886  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40499    |\n",
      "|    policy_loss        | 0.00128  |\n",
      "|    value_loss         | 0.0848   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40500    |\n",
      "|    time_elapsed    | 10563    |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=22.60 +/- 5.95\n",
      "Episode length: 886.00 +/- 164.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 886      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 812000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0279  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40599    |\n",
      "|    policy_loss        | -0.029   |\n",
      "|    value_loss         | 0.086    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40600    |\n",
      "|    time_elapsed    | 10575    |\n",
      "|    total_timesteps | 812000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=25.60 +/- 6.44\n",
      "Episode length: 876.00 +/- 108.89\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 876       |\n",
      "|    mean_reward        | 25.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 814000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000479 |\n",
      "|    explained_variance | 0.965     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40699     |\n",
      "|    policy_loss        | 1.69e-05  |\n",
      "|    value_loss         | 0.0391    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40700    |\n",
      "|    time_elapsed    | 10587    |\n",
      "|    total_timesteps | 814000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=16.60 +/- 6.12\n",
      "Episode length: 6013.20 +/- 10493.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 16.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 816000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0111  |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40799    |\n",
      "|    policy_loss        | 0.00183  |\n",
      "|    value_loss         | 0.0483   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40800    |\n",
      "|    time_elapsed    | 10668    |\n",
      "|    total_timesteps | 816000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=21.40 +/- 10.46\n",
      "Episode length: 759.00 +/- 143.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 759      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 818000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40899    |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 40900    |\n",
      "|    time_elapsed    | 10679    |\n",
      "|    total_timesteps | 818000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=27.00 +/- 2.83\n",
      "Episode length: 972.40 +/- 119.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 972      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 820000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0637  |\n",
      "|    explained_variance | -0.345   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 40999    |\n",
      "|    policy_loss        | 0.0277   |\n",
      "|    value_loss         | 0.356    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41000    |\n",
      "|    time_elapsed    | 10692    |\n",
      "|    total_timesteps | 820000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=22.40 +/- 7.66\n",
      "Episode length: 868.00 +/- 154.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 822000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41099    |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41100    |\n",
      "|    time_elapsed    | 10704    |\n",
      "|    total_timesteps | 822000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=25.80 +/- 7.55\n",
      "Episode length: 840.60 +/- 181.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 824000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0203  |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41199    |\n",
      "|    policy_loss        | 0.000802 |\n",
      "|    value_loss         | 0.0409   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41200    |\n",
      "|    time_elapsed    | 10717    |\n",
      "|    total_timesteps | 824000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=826000, episode_reward=19.00 +/- 13.84\n",
      "Episode length: 6121.60 +/- 10441.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 826000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.026   |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41299    |\n",
      "|    policy_loss        | 0.00294  |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41300    |\n",
      "|    time_elapsed    | 10798    |\n",
      "|    total_timesteps | 826000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=31.20 +/- 12.06\n",
      "Episode length: 892.60 +/- 155.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 893      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 828000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0997  |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41399    |\n",
      "|    policy_loss        | -0.00881 |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41400    |\n",
      "|    time_elapsed    | 10811    |\n",
      "|    total_timesteps | 828000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=17.20 +/- 8.03\n",
      "Episode length: 5960.00 +/- 10520.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 830000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0768  |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41499    |\n",
      "|    policy_loss        | -0.0836  |\n",
      "|    value_loss         | 0.0641   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41500    |\n",
      "|    time_elapsed    | 10893    |\n",
      "|    total_timesteps | 830000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=23.40 +/- 4.32\n",
      "Episode length: 831.40 +/- 16.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 832000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0656  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41599    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.041    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 41600    |\n",
      "|    time_elapsed    | 10904    |\n",
      "|    total_timesteps | 832000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=17.00 +/- 7.62\n",
      "Episode length: 6009.00 +/- 10495.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 834000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41699    |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 41700    |\n",
      "|    time_elapsed    | 10985    |\n",
      "|    total_timesteps | 834000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=19.80 +/- 5.27\n",
      "Episode length: 6041.80 +/- 10479.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 836000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0998  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | -0.00415 |\n",
      "|    value_loss         | 0.0586   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 41800    |\n",
      "|    time_elapsed    | 11065    |\n",
      "|    total_timesteps | 836000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=14.20 +/- 2.93\n",
      "Episode length: 5920.00 +/- 10540.38\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 5.92e+03  |\n",
      "|    mean_reward        | 14.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 838000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00262  |\n",
      "|    explained_variance | 0.809     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41899     |\n",
      "|    policy_loss        | -3.73e-05 |\n",
      "|    value_loss         | 0.0808    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 41900    |\n",
      "|    time_elapsed    | 11145    |\n",
      "|    total_timesteps | 838000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=22.80 +/- 4.96\n",
      "Episode length: 866.80 +/- 66.87\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 867       |\n",
      "|    mean_reward        | 22.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 840000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000836 |\n",
      "|    explained_variance | 0.172     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41999     |\n",
      "|    policy_loss        | -3.93e-06 |\n",
      "|    value_loss         | 0.347     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 21.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42000    |\n",
      "|    time_elapsed    | 11157    |\n",
      "|    total_timesteps | 840000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=20.40 +/- 5.54\n",
      "Episode length: 880.20 +/- 162.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 880      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 842000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00435 |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | 0.000197 |\n",
      "|    value_loss         | 0.0332   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42100    |\n",
      "|    time_elapsed    | 11169    |\n",
      "|    total_timesteps | 842000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=18.80 +/- 12.98\n",
      "Episode length: 6080.00 +/- 10461.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 844000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0858  |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42199    |\n",
      "|    policy_loss        | -0.00771 |\n",
      "|    value_loss         | 0.26     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42200    |\n",
      "|    time_elapsed    | 11251    |\n",
      "|    total_timesteps | 844000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=26.80 +/- 9.81\n",
      "Episode length: 880.20 +/- 153.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 880      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 846000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0562  |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42299    |\n",
      "|    policy_loss        | 0.0194   |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42300    |\n",
      "|    time_elapsed    | 11262    |\n",
      "|    total_timesteps | 846000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=848000, episode_reward=20.60 +/- 2.87\n",
      "Episode length: 771.60 +/- 49.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 772      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 848000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0547  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42399    |\n",
      "|    policy_loss        | -0.0034  |\n",
      "|    value_loss         | 0.0732   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42400    |\n",
      "|    time_elapsed    | 11274    |\n",
      "|    total_timesteps | 848000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=21.40 +/- 4.45\n",
      "Episode length: 824.40 +/- 175.78\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 824       |\n",
      "|    mean_reward        | 21.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 850000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.053    |\n",
      "|    explained_variance | 0.903     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42499     |\n",
      "|    policy_loss        | -0.000918 |\n",
      "|    value_loss         | 0.258     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42500    |\n",
      "|    time_elapsed    | 11286    |\n",
      "|    total_timesteps | 850000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=22.40 +/- 11.67\n",
      "Episode length: 6125.60 +/- 10437.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.13e+03 |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 852000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0229  |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42599    |\n",
      "|    policy_loss        | 0.00551  |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 42600    |\n",
      "|    time_elapsed    | 11366    |\n",
      "|    total_timesteps | 852000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=24.20 +/- 5.15\n",
      "Episode length: 932.60 +/- 110.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 933      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 854000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0287  |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42699    |\n",
      "|    policy_loss        | -0.00393 |\n",
      "|    value_loss         | 0.0772   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42700    |\n",
      "|    time_elapsed    | 11377    |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=20.00 +/- 3.03\n",
      "Episode length: 755.20 +/- 53.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 755      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 856000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0228  |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42799    |\n",
      "|    policy_loss        | 0.000156 |\n",
      "|    value_loss         | 0.00734  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42800    |\n",
      "|    time_elapsed    | 11389    |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=25.60 +/- 4.22\n",
      "Episode length: 905.40 +/- 121.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 905      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 858000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0703  |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42899    |\n",
      "|    policy_loss        | 0.0389   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 42900    |\n",
      "|    time_elapsed    | 11401    |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=18.00 +/- 7.87\n",
      "Episode length: 6053.40 +/- 10473.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 860000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0265  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 42999    |\n",
      "|    policy_loss        | 0.00291  |\n",
      "|    value_loss         | 0.0608   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43000    |\n",
      "|    time_elapsed    | 11482    |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=21.20 +/- 7.78\n",
      "Episode length: 6081.60 +/- 10459.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 862000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0457  |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43099    |\n",
      "|    policy_loss        | 0.00222  |\n",
      "|    value_loss         | 0.0474   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43100    |\n",
      "|    time_elapsed    | 11562    |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=20.80 +/- 4.21\n",
      "Episode length: 817.80 +/- 130.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 818      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 864000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0248  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43199    |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    value_loss         | 0.0572   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43200    |\n",
      "|    time_elapsed    | 11573    |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=20.20 +/- 5.42\n",
      "Episode length: 795.00 +/- 132.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 866000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0078  |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | 0.000446 |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43300    |\n",
      "|    time_elapsed    | 11585    |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=20.00 +/- 4.77\n",
      "Episode length: 819.40 +/- 105.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 868000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0888  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43399    |\n",
      "|    policy_loss        | 0.00499  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43400    |\n",
      "|    time_elapsed    | 11597    |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=21.80 +/- 5.11\n",
      "Episode length: 809.60 +/- 84.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 810      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 870000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0206  |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43499    |\n",
      "|    policy_loss        | -0.0012  |\n",
      "|    value_loss         | 0.132    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 43500    |\n",
      "|    time_elapsed    | 11610    |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=23.60 +/- 8.31\n",
      "Episode length: 840.60 +/- 141.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 872000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00764 |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | 1.23e-05 |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 43600    |\n",
      "|    time_elapsed    | 11623    |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=20.60 +/- 8.59\n",
      "Episode length: 748.80 +/- 137.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 749      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 874000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00135 |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43699    |\n",
      "|    policy_loss        | -2.1e-05 |\n",
      "|    value_loss         | 0.0634   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 43700    |\n",
      "|    time_elapsed    | 11634    |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=23.40 +/- 6.47\n",
      "Episode length: 829.20 +/- 121.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 829      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 876000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0634  |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43799    |\n",
      "|    policy_loss        | 0.041    |\n",
      "|    value_loss         | 0.306    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 43800    |\n",
      "|    time_elapsed    | 11647    |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=24.60 +/- 3.93\n",
      "Episode length: 812.00 +/- 88.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 812      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 878000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0201  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43899    |\n",
      "|    policy_loss        | -0.00181 |\n",
      "|    value_loss         | 0.118    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 43900    |\n",
      "|    time_elapsed    | 11659    |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=18.00 +/- 4.00\n",
      "Episode length: 5984.60 +/- 10508.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 880000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0879  |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43999    |\n",
      "|    policy_loss        | -0.176   |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 44000    |\n",
      "|    time_elapsed    | 11740    |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=882000, episode_reward=25.80 +/- 8.47\n",
      "Episode length: 920.20 +/- 157.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 920      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 882000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | -0.00603 |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44100    |\n",
      "|    time_elapsed    | 11750    |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=24.80 +/- 8.35\n",
      "Episode length: 803.40 +/- 95.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 803      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 884000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0412  |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44199    |\n",
      "|    policy_loss        | -0.0021  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44200    |\n",
      "|    time_elapsed    | 11762    |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=25.20 +/- 7.39\n",
      "Episode length: 6108.60 +/- 10446.63\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.11e+03  |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 886000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0267   |\n",
      "|    explained_variance | 0.199     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44299     |\n",
      "|    policy_loss        | -0.000781 |\n",
      "|    value_loss         | 0.117     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 44300    |\n",
      "|    time_elapsed    | 11842    |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=24.20 +/- 13.61\n",
      "Episode length: 792.20 +/- 284.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 888000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0258  |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44399    |\n",
      "|    policy_loss        | 0.00126  |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 44400    |\n",
      "|    time_elapsed    | 11854    |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=25.00 +/- 5.37\n",
      "Episode length: 950.40 +/- 153.71\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 950       |\n",
      "|    mean_reward        | 25        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 890000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0221   |\n",
      "|    explained_variance | 0.958     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44499     |\n",
      "|    policy_loss        | -3.62e-05 |\n",
      "|    value_loss         | 0.0515    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 44500    |\n",
      "|    time_elapsed    | 11867    |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=21.40 +/- 3.83\n",
      "Episode length: 827.00 +/- 136.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 827      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 892000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0797  |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44599    |\n",
      "|    policy_loss        | -0.00611 |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44600    |\n",
      "|    time_elapsed    | 11879    |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=20.80 +/- 3.92\n",
      "Episode length: 733.80 +/- 55.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 894000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0797  |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44699    |\n",
      "|    policy_loss        | 0.00966  |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44700    |\n",
      "|    time_elapsed    | 11891    |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=23.60 +/- 10.13\n",
      "Episode length: 879.20 +/- 175.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 879      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 896000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0487  |\n",
      "|    explained_variance | -2.52    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44799    |\n",
      "|    policy_loss        | 0.0906   |\n",
      "|    value_loss         | 0.376    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44800    |\n",
      "|    time_elapsed    | 11903    |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=21.40 +/- 5.78\n",
      "Episode length: 861.60 +/- 135.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 898000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.043   |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 44899    |\n",
      "|    policy_loss        | -0.00125 |\n",
      "|    value_loss         | 0.552    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 44900    |\n",
      "|    time_elapsed    | 11916    |\n",
      "|    total_timesteps | 898000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=20.00 +/- 8.29\n",
      "Episode length: 6073.80 +/- 10463.16\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.07e+03  |\n",
      "|    mean_reward        | 20        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 900000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0129   |\n",
      "|    explained_variance | 0.886     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44999     |\n",
      "|    policy_loss        | -0.000397 |\n",
      "|    value_loss         | 0.086     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45000    |\n",
      "|    time_elapsed    | 11996    |\n",
      "|    total_timesteps | 900000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=19.40 +/- 4.84\n",
      "Episode length: 775.80 +/- 92.50\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 776       |\n",
      "|    mean_reward        | 19.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 902000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00256  |\n",
      "|    explained_variance | 0.983     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45099     |\n",
      "|    policy_loss        | -7.74e-05 |\n",
      "|    value_loss         | 0.0362    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45100    |\n",
      "|    time_elapsed    | 12008    |\n",
      "|    total_timesteps | 902000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=23.60 +/- 6.47\n",
      "Episode length: 820.40 +/- 111.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 820      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 904000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0201  |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45199    |\n",
      "|    policy_loss        | 0.00126  |\n",
      "|    value_loss         | 0.293    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45200    |\n",
      "|    time_elapsed    | 12019    |\n",
      "|    total_timesteps | 904000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=22.00 +/- 3.63\n",
      "Episode length: 788.20 +/- 74.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 906000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45299    |\n",
      "|    policy_loss        | -0.0653  |\n",
      "|    value_loss         | 0.0378   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45300    |\n",
      "|    time_elapsed    | 12032    |\n",
      "|    total_timesteps | 906000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=27.60 +/- 6.65\n",
      "Episode length: 862.20 +/- 179.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 908000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0557  |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45399    |\n",
      "|    policy_loss        | 0.00331  |\n",
      "|    value_loss         | 0.218    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45400    |\n",
      "|    time_elapsed    | 12043    |\n",
      "|    total_timesteps | 908000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=29.00 +/- 13.84\n",
      "Episode length: 978.20 +/- 270.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 978      |\n",
      "|    mean_reward        | 29       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 910000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0754  |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45499    |\n",
      "|    policy_loss        | -0.00494 |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45500    |\n",
      "|    time_elapsed    | 12056    |\n",
      "|    total_timesteps | 910000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=20.20 +/- 13.33\n",
      "Episode length: 6120.80 +/- 10441.22\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.12e+03  |\n",
      "|    mean_reward        | 20.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 912000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0236   |\n",
      "|    explained_variance | 0.991     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45599     |\n",
      "|    policy_loss        | -0.000432 |\n",
      "|    value_loss         | 0.0099    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 45600    |\n",
      "|    time_elapsed    | 12137    |\n",
      "|    total_timesteps | 912000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=17.00 +/- 9.32\n",
      "Episode length: 6056.20 +/- 10472.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 914000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45699    |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 0.0933   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 45700    |\n",
      "|    time_elapsed    | 12220    |\n",
      "|    total_timesteps | 914000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=916000, episode_reward=19.00 +/- 5.83\n",
      "Episode length: 773.80 +/- 128.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 774      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 916000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0234  |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45799    |\n",
      "|    policy_loss        | -0.00171 |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 22.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 45800    |\n",
      "|    time_elapsed    | 12232    |\n",
      "|    total_timesteps | 916000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=21.00 +/- 6.42\n",
      "Episode length: 771.00 +/- 127.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 771      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 918000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0806  |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 45899    |\n",
      "|    policy_loss        | -0.00777 |\n",
      "|    value_loss         | 0.0661   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 45900    |\n",
      "|    time_elapsed    | 12244    |\n",
      "|    total_timesteps | 918000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=19.80 +/- 5.00\n",
      "Episode length: 812.40 +/- 112.45\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 812       |\n",
      "|    mean_reward        | 19.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 920000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00273  |\n",
      "|    explained_variance | 0.945     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45999     |\n",
      "|    policy_loss        | -2.79e-05 |\n",
      "|    value_loss         | 0.0358    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46000    |\n",
      "|    time_elapsed    | 12256    |\n",
      "|    total_timesteps | 920000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=22.00 +/- 5.48\n",
      "Episode length: 825.80 +/- 117.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 826      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 922000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.061   |\n",
      "|    explained_variance | 0.406    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46099    |\n",
      "|    policy_loss        | -0.172   |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46100    |\n",
      "|    time_elapsed    | 12268    |\n",
      "|    total_timesteps | 922000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=28.20 +/- 10.93\n",
      "Episode length: 946.00 +/- 144.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 946      |\n",
      "|    mean_reward        | 28.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 924000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0605  |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46199    |\n",
      "|    policy_loss        | -0.0159  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 22.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46200    |\n",
      "|    time_elapsed    | 12280    |\n",
      "|    total_timesteps | 924000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=18.80 +/- 2.56\n",
      "Episode length: 671.20 +/- 130.22\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 671      |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 926000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0738  |\n",
      "|    explained_variance | 0.795    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46299    |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.403    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46300    |\n",
      "|    time_elapsed    | 12292    |\n",
      "|    total_timesteps | 926000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=19.40 +/- 6.47\n",
      "Episode length: 780.40 +/- 167.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 928000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0518  |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46399    |\n",
      "|    policy_loss        | -0.005   |\n",
      "|    value_loss         | 0.0784   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46400    |\n",
      "|    time_elapsed    | 12303    |\n",
      "|    total_timesteps | 928000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=23.60 +/- 5.89\n",
      "Episode length: 783.20 +/- 148.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 783      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 930000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00421 |\n",
      "|    explained_variance | 0.734    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46499    |\n",
      "|    policy_loss        | 0.000146 |\n",
      "|    value_loss         | 0.088    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 22.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46500    |\n",
      "|    time_elapsed    | 12315    |\n",
      "|    total_timesteps | 930000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=22.60 +/- 5.00\n",
      "Episode length: 851.20 +/- 163.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 851      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 932000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.034   |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46599    |\n",
      "|    policy_loss        | 0.00519  |\n",
      "|    value_loss         | 0.173    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46600    |\n",
      "|    time_elapsed    | 12327    |\n",
      "|    total_timesteps | 932000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=21.60 +/- 5.89\n",
      "Episode length: 845.20 +/- 156.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 934000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0211  |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46699    |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.0645   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46700    |\n",
      "|    time_elapsed    | 12339    |\n",
      "|    total_timesteps | 934000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=16.40 +/- 9.65\n",
      "Episode length: 6015.40 +/- 10494.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 936000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0204  |\n",
      "|    explained_variance | 0.0691   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46799    |\n",
      "|    policy_loss        | -0.021   |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46800    |\n",
      "|    time_elapsed    | 12420    |\n",
      "|    total_timesteps | 936000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=30.00 +/- 6.66\n",
      "Episode length: 948.40 +/- 104.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 948      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 938000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | -0.00362 |\n",
      "|    value_loss         | 0.07     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 46900    |\n",
      "|    time_elapsed    | 12431    |\n",
      "|    total_timesteps | 938000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=23.60 +/- 6.53\n",
      "Episode length: 850.80 +/- 104.43\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 851       |\n",
      "|    mean_reward        | 23.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 940000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00184  |\n",
      "|    explained_variance | 0.99      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46999     |\n",
      "|    policy_loss        | -5.22e-05 |\n",
      "|    value_loss         | 0.0298    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47000    |\n",
      "|    time_elapsed    | 12443    |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=19.60 +/- 3.01\n",
      "Episode length: 790.00 +/- 86.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 790      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 942000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0263  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47099    |\n",
      "|    policy_loss        | 0.000427 |\n",
      "|    value_loss         | 0.0447   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47100    |\n",
      "|    time_elapsed    | 12456    |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=23.00 +/- 7.24\n",
      "Episode length: 830.00 +/- 164.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 830      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 944000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0965  |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47199    |\n",
      "|    policy_loss        | 0.00338  |\n",
      "|    value_loss         | 0.189    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47200    |\n",
      "|    time_elapsed    | 12468    |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=21.20 +/- 5.34\n",
      "Episode length: 783.00 +/- 66.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 783      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 946000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0477  |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47299    |\n",
      "|    policy_loss        | -0.00212 |\n",
      "|    value_loss         | 0.0828   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 23       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47300    |\n",
      "|    time_elapsed    | 12480    |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=20.80 +/- 5.04\n",
      "Episode length: 842.20 +/- 139.70\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 842       |\n",
      "|    mean_reward        | 20.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 948000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00598  |\n",
      "|    explained_variance | 0.976     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47399     |\n",
      "|    policy_loss        | -1.63e-05 |\n",
      "|    value_loss         | 0.0856    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47400    |\n",
      "|    time_elapsed    | 12492    |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=22.60 +/- 4.32\n",
      "Episode length: 760.60 +/- 77.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 761      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 950000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0397  |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47499    |\n",
      "|    policy_loss        | 0.00551  |\n",
      "|    value_loss         | 0.279    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47500    |\n",
      "|    time_elapsed    | 12504    |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=21.20 +/- 3.66\n",
      "Episode length: 789.40 +/- 97.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 952000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0243  |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47599    |\n",
      "|    policy_loss        | 0.000986 |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 47600    |\n",
      "|    time_elapsed    | 12516    |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=18.40 +/- 3.72\n",
      "Episode length: 777.60 +/- 78.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 954000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47699    |\n",
      "|    policy_loss        | 0.0217   |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 47700    |\n",
      "|    time_elapsed    | 12528    |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=16.40 +/- 1.96\n",
      "Episode length: 736.00 +/- 67.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 736      |\n",
      "|    mean_reward        | 16.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 956000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0709  |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47799    |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.0895   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 47800    |\n",
      "|    time_elapsed    | 12540    |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=19.60 +/- 13.69\n",
      "Episode length: 6098.60 +/- 10452.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 958000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0122  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47899    |\n",
      "|    policy_loss        | 0.000593 |\n",
      "|    value_loss         | 0.0734   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 47900    |\n",
      "|    time_elapsed    | 12621    |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=25.20 +/- 8.16\n",
      "Episode length: 828.40 +/- 135.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 47999    |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.12     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48000    |\n",
      "|    time_elapsed    | 12634    |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=962000, episode_reward=19.60 +/- 9.09\n",
      "Episode length: 6069.80 +/- 10465.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 962000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0682  |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48099    |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.0879   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48100    |\n",
      "|    time_elapsed    | 12715    |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=25.40 +/- 6.83\n",
      "Episode length: 879.80 +/- 133.87\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 880       |\n",
      "|    mean_reward        | 25.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 964000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0205   |\n",
      "|    explained_variance | 0.897     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48199     |\n",
      "|    policy_loss        | -0.000119 |\n",
      "|    value_loss         | 0.164     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48200    |\n",
      "|    time_elapsed    | 12726    |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=26.60 +/- 6.74\n",
      "Episode length: 941.40 +/- 99.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 941      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 966000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48299    |\n",
      "|    policy_loss        | 0.00833  |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48300    |\n",
      "|    time_elapsed    | 12739    |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=25.80 +/- 7.76\n",
      "Episode length: 909.00 +/- 181.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 909      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 968000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.189   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48399    |\n",
      "|    policy_loss        | 0.00181  |\n",
      "|    value_loss         | 0.0717   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48400    |\n",
      "|    time_elapsed    | 12751    |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=15.80 +/- 9.24\n",
      "Episode length: 5963.20 +/- 10518.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.96e+03 |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 970000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0346  |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | -0.0163  |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48500    |\n",
      "|    time_elapsed    | 12832    |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=17.80 +/- 7.05\n",
      "Episode length: 6039.20 +/- 10481.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 972000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48599    |\n",
      "|    policy_loss        | 0.0433   |\n",
      "|    value_loss         | 0.0779   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48600    |\n",
      "|    time_elapsed    | 12912    |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=974000, episode_reward=20.60 +/- 6.95\n",
      "Episode length: 805.00 +/- 126.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 805      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 974000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0883  |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48699    |\n",
      "|    policy_loss        | 0.0184   |\n",
      "|    value_loss         | 0.472    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48700    |\n",
      "|    time_elapsed    | 12924    |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=21.60 +/- 6.68\n",
      "Episode length: 826.40 +/- 106.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 826      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 976000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0724  |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | -0.071   |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48800    |\n",
      "|    time_elapsed    | 12937    |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=27.00 +/- 1.90\n",
      "Episode length: 791.20 +/- 57.62\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 791       |\n",
      "|    mean_reward        | 27        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 978000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0156   |\n",
      "|    explained_variance | 0.989     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48899     |\n",
      "|    policy_loss        | -9.71e-05 |\n",
      "|    value_loss         | 0.0502    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 22.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 48900    |\n",
      "|    time_elapsed    | 12949    |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=25.60 +/- 5.68\n",
      "Episode length: 818.40 +/- 100.72\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 818       |\n",
      "|    mean_reward        | 25.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 980000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0108   |\n",
      "|    explained_variance | 0.996     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48999     |\n",
      "|    policy_loss        | -0.000259 |\n",
      "|    value_loss         | 0.0218    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49000    |\n",
      "|    time_elapsed    | 12962    |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=17.40 +/- 7.86\n",
      "Episode length: 6011.40 +/- 10494.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 982000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0908  |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49099    |\n",
      "|    policy_loss        | -0.00671 |\n",
      "|    value_loss         | 0.216    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49100    |\n",
      "|    time_elapsed    | 13043    |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=20.20 +/- 3.31\n",
      "Episode length: 715.40 +/- 92.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 715      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 984000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0211  |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49199    |\n",
      "|    policy_loss        | -0.00254 |\n",
      "|    value_loss         | 0.377    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49200    |\n",
      "|    time_elapsed    | 13053    |\n",
      "|    total_timesteps | 984000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=19.60 +/- 10.23\n",
      "Episode length: 6137.40 +/- 10431.74\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.14e+03  |\n",
      "|    mean_reward        | 19.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 986000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0306   |\n",
      "|    explained_variance | 0.934     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49299     |\n",
      "|    policy_loss        | -0.000815 |\n",
      "|    value_loss         | 0.0992    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49300    |\n",
      "|    time_elapsed    | 13135    |\n",
      "|    total_timesteps | 986000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=20.60 +/- 4.76\n",
      "Episode length: 803.20 +/- 95.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 803      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 988000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0166  |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | 0.00105  |\n",
      "|    value_loss         | 0.201    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49400    |\n",
      "|    time_elapsed    | 13146    |\n",
      "|    total_timesteps | 988000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=25.60 +/- 9.18\n",
      "Episode length: 869.20 +/- 130.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 869      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 990000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0548  |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49499    |\n",
      "|    policy_loss        | 0.00354  |\n",
      "|    value_loss         | 0.317    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49500    |\n",
      "|    time_elapsed    | 13159    |\n",
      "|    total_timesteps | 990000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=19.80 +/- 7.39\n",
      "Episode length: 780.20 +/- 101.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 992000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00148 |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49599    |\n",
      "|    policy_loss        | 7.3e-05  |\n",
      "|    value_loss         | 0.0648   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49600    |\n",
      "|    time_elapsed    | 13171    |\n",
      "|    total_timesteps | 992000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=25.00 +/- 3.63\n",
      "Episode length: 837.40 +/- 121.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 837      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 994000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00785 |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | 0.000507 |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49700    |\n",
      "|    time_elapsed    | 13183    |\n",
      "|    total_timesteps | 994000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=21.00 +/- 8.17\n",
      "Episode length: 6120.00 +/- 10440.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 996000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0668  |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49799    |\n",
      "|    policy_loss        | -0.363   |\n",
      "|    value_loss         | 0.263    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49800    |\n",
      "|    time_elapsed    | 13262    |\n",
      "|    total_timesteps | 996000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=25.20 +/- 6.65\n",
      "Episode length: 931.60 +/- 153.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 932      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 998000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0116  |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49899    |\n",
      "|    policy_loss        | 0.00074  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 49900    |\n",
      "|    time_elapsed    | 13273    |\n",
      "|    total_timesteps | 998000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=22.60 +/- 9.00\n",
      "Episode length: 6104.00 +/- 10449.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0409  |\n",
      "|    explained_variance | 0.831    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 49999    |\n",
      "|    policy_loss        | 0.00571  |\n",
      "|    value_loss         | 0.337    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 50000    |\n",
      "|    time_elapsed    | 13352    |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1002000, episode_reward=21.40 +/- 7.28\n",
      "Episode length: 847.00 +/- 128.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 847      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1002000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0394  |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50099    |\n",
      "|    policy_loss        | 0.0134   |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 50100    |\n",
      "|    time_elapsed    | 13364    |\n",
      "|    total_timesteps | 1002000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1004000, episode_reward=18.00 +/- 2.19\n",
      "Episode length: 768.20 +/- 76.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1004000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0606  |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50199    |\n",
      "|    policy_loss        | 0.0337   |\n",
      "|    value_loss         | 0.172    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50200    |\n",
      "|    time_elapsed    | 13376    |\n",
      "|    total_timesteps | 1004000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1006000, episode_reward=22.60 +/- 8.14\n",
      "Episode length: 831.40 +/- 196.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1006000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0702  |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50299    |\n",
      "|    policy_loss        | 0.00848  |\n",
      "|    value_loss         | 0.0835   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50300    |\n",
      "|    time_elapsed    | 13388    |\n",
      "|    total_timesteps | 1006000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1008000, episode_reward=21.00 +/- 7.01\n",
      "Episode length: 801.40 +/- 151.63\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 801      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1008000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0199  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50399    |\n",
      "|    policy_loss        | 0.00139  |\n",
      "|    value_loss         | 0.051    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50400    |\n",
      "|    time_elapsed    | 13400    |\n",
      "|    total_timesteps | 1008000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=21.40 +/- 6.92\n",
      "Episode length: 813.00 +/- 134.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 813      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0247  |\n",
      "|    explained_variance | 0.559    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50499    |\n",
      "|    policy_loss        | 0.00584  |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50500    |\n",
      "|    time_elapsed    | 13412    |\n",
      "|    total_timesteps | 1010000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1012000, episode_reward=21.20 +/- 4.45\n",
      "Episode length: 770.20 +/- 98.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 770      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1012000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00561 |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50599    |\n",
      "|    policy_loss        | -8e-05   |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50600    |\n",
      "|    time_elapsed    | 13424    |\n",
      "|    total_timesteps | 1012000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1014000, episode_reward=18.60 +/- 3.93\n",
      "Episode length: 746.80 +/- 86.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 747      |\n",
      "|    mean_reward        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1014000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.09    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | 0.00899  |\n",
      "|    value_loss         | 0.0205   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50700    |\n",
      "|    time_elapsed    | 13436    |\n",
      "|    total_timesteps | 1014000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1016000, episode_reward=20.80 +/- 6.88\n",
      "Episode length: 883.00 +/- 176.35\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 883       |\n",
      "|    mean_reward        | 20.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1016000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00528  |\n",
      "|    explained_variance | 0.982     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50799     |\n",
      "|    policy_loss        | -0.000102 |\n",
      "|    value_loss         | 0.0136    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50800    |\n",
      "|    time_elapsed    | 13448    |\n",
      "|    total_timesteps | 1016000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1018000, episode_reward=23.20 +/- 6.27\n",
      "Episode length: 833.40 +/- 190.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 833      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1018000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00431 |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50899    |\n",
      "|    policy_loss        | 0.000171 |\n",
      "|    value_loss         | 0.151    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 50900    |\n",
      "|    time_elapsed    | 13460    |\n",
      "|    total_timesteps | 1018000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=21.40 +/- 5.82\n",
      "Episode length: 761.60 +/- 99.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 762      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0883  |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 50999    |\n",
      "|    policy_loss        | 0.00233  |\n",
      "|    value_loss         | 0.249    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51000    |\n",
      "|    time_elapsed    | 13472    |\n",
      "|    total_timesteps | 1020000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1022000, episode_reward=17.40 +/- 10.25\n",
      "Episode length: 6027.60 +/- 10487.11\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.03e+03  |\n",
      "|    mean_reward        | 17.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1022000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0042   |\n",
      "|    explained_variance | 0.752     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51099     |\n",
      "|    policy_loss        | -1.76e-05 |\n",
      "|    value_loss         | 0.0896    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51100    |\n",
      "|    time_elapsed    | 13554    |\n",
      "|    total_timesteps | 1022000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1024000, episode_reward=20.80 +/- 7.98\n",
      "Episode length: 804.00 +/- 181.15\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 804       |\n",
      "|    mean_reward        | 20.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1024000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000594 |\n",
      "|    explained_variance | 0.888     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51199     |\n",
      "|    policy_loss        | 9.61e-06  |\n",
      "|    value_loss         | 0.0276    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51200    |\n",
      "|    time_elapsed    | 13565    |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1026000, episode_reward=24.80 +/- 6.88\n",
      "Episode length: 903.80 +/- 166.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 904      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1026000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0666  |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51299    |\n",
      "|    policy_loss        | 0.00155  |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51300    |\n",
      "|    time_elapsed    | 13577    |\n",
      "|    total_timesteps | 1026000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1028000, episode_reward=21.00 +/- 7.04\n",
      "Episode length: 722.20 +/- 94.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 722      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1028000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51399    |\n",
      "|    policy_loss        | -0.0889  |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51400    |\n",
      "|    time_elapsed    | 13590    |\n",
      "|    total_timesteps | 1028000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=22.60 +/- 6.74\n",
      "Episode length: 794.00 +/- 102.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 794      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0132  |\n",
      "|    explained_variance | 0.822    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51499    |\n",
      "|    policy_loss        | 0.000369 |\n",
      "|    value_loss         | 0.0793   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51500    |\n",
      "|    time_elapsed    | 13602    |\n",
      "|    total_timesteps | 1030000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1032000, episode_reward=16.80 +/- 9.95\n",
      "Episode length: 6057.00 +/- 10471.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1032000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.071   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51599    |\n",
      "|    policy_loss        | 0.00341  |\n",
      "|    value_loss         | 0.0751   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51600    |\n",
      "|    time_elapsed    | 13683    |\n",
      "|    total_timesteps | 1032000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1034000, episode_reward=20.00 +/- 1.90\n",
      "Episode length: 815.60 +/- 34.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 816      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1034000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0682  |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51699    |\n",
      "|    policy_loss        | 0.00797  |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51700    |\n",
      "|    time_elapsed    | 13695    |\n",
      "|    total_timesteps | 1034000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1036000, episode_reward=19.20 +/- 3.54\n",
      "Episode length: 766.00 +/- 47.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1036000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0111  |\n",
      "|    explained_variance | -1.07    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51799    |\n",
      "|    policy_loss        | 0.000867 |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51800    |\n",
      "|    time_elapsed    | 13707    |\n",
      "|    total_timesteps | 1036000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1038000, episode_reward=23.20 +/- 6.88\n",
      "Episode length: 810.20 +/- 141.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 810      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1038000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0767  |\n",
      "|    explained_variance | -0.058   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51899    |\n",
      "|    policy_loss        | -0.00285 |\n",
      "|    value_loss         | 0.417    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 51900    |\n",
      "|    time_elapsed    | 13719    |\n",
      "|    total_timesteps | 1038000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=25.80 +/- 6.68\n",
      "Episode length: 894.20 +/- 110.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 894      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0638  |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 51999    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.0298   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52000    |\n",
      "|    time_elapsed    | 13732    |\n",
      "|    total_timesteps | 1040000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1042000, episode_reward=26.20 +/- 5.27\n",
      "Episode length: 937.80 +/- 81.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 938      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1042000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0495  |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | 0.000882 |\n",
      "|    value_loss         | 0.226    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52100    |\n",
      "|    time_elapsed    | 13745    |\n",
      "|    total_timesteps | 1042000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1044000, episode_reward=23.40 +/- 7.31\n",
      "Episode length: 823.20 +/- 113.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 823      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1044000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0287  |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | -0.00692 |\n",
      "|    value_loss         | 0.316    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52200    |\n",
      "|    time_elapsed    | 13757    |\n",
      "|    total_timesteps | 1044000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1046000, episode_reward=22.40 +/- 7.94\n",
      "Episode length: 832.60 +/- 145.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 833      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1046000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.017   |\n",
      "|    explained_variance | 0.673    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52299    |\n",
      "|    policy_loss        | -0.0282  |\n",
      "|    value_loss         | 0.0473   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52300    |\n",
      "|    time_elapsed    | 13769    |\n",
      "|    total_timesteps | 1046000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1048000, episode_reward=25.80 +/- 6.49\n",
      "Episode length: 854.20 +/- 148.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 854      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1048000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0264  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52399    |\n",
      "|    policy_loss        | -0.00316 |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 52400    |\n",
      "|    time_elapsed    | 13781    |\n",
      "|    total_timesteps | 1048000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=25.80 +/- 5.27\n",
      "Episode length: 863.00 +/- 95.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 863       |\n",
      "|    mean_reward        | 25.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1050000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00971  |\n",
      "|    explained_variance | 0.671     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52499     |\n",
      "|    policy_loss        | -2.58e-05 |\n",
      "|    value_loss         | 0.0747    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 52500    |\n",
      "|    time_elapsed    | 13794    |\n",
      "|    total_timesteps | 1050000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1052000, episode_reward=16.80 +/- 7.73\n",
      "Episode length: 5947.60 +/- 10527.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.95e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1052000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.033   |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52599    |\n",
      "|    policy_loss        | 0.00602  |\n",
      "|    value_loss         | 0.31     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52600    |\n",
      "|    time_elapsed    | 13874    |\n",
      "|    total_timesteps | 1052000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1054000, episode_reward=22.20 +/- 5.53\n",
      "Episode length: 827.60 +/- 140.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1054000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0596  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52699    |\n",
      "|    policy_loss        | -0.00419 |\n",
      "|    value_loss         | 0.0679   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52700    |\n",
      "|    time_elapsed    | 13887    |\n",
      "|    total_timesteps | 1054000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1056000, episode_reward=20.60 +/- 9.75\n",
      "Episode length: 6047.80 +/- 10477.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.05e+03  |\n",
      "|    mean_reward        | 20.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1056000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00763  |\n",
      "|    explained_variance | 0.893     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52799     |\n",
      "|    policy_loss        | -0.000628 |\n",
      "|    value_loss         | 0.552     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52800    |\n",
      "|    time_elapsed    | 13966    |\n",
      "|    total_timesteps | 1056000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1058000, episode_reward=17.00 +/- 8.72\n",
      "Episode length: 6062.40 +/- 10468.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1058000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0269  |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52899    |\n",
      "|    policy_loss        | 0.00458  |\n",
      "|    value_loss         | 0.041    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 52900    |\n",
      "|    time_elapsed    | 14047    |\n",
      "|    total_timesteps | 1058000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=19.00 +/- 2.45\n",
      "Episode length: 797.80 +/- 63.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0229  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 52999    |\n",
      "|    policy_loss        | 0.000132 |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53000    |\n",
      "|    time_elapsed    | 14059    |\n",
      "|    total_timesteps | 1060000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1062000, episode_reward=18.40 +/- 4.67\n",
      "Episode length: 707.80 +/- 69.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 708      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1062000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0911  |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53099    |\n",
      "|    policy_loss        | 0.0479   |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53100    |\n",
      "|    time_elapsed    | 14071    |\n",
      "|    total_timesteps | 1062000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1064000, episode_reward=25.40 +/- 8.24\n",
      "Episode length: 780.00 +/- 109.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1064000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0318  |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53199    |\n",
      "|    policy_loss        | -0.00309 |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53200    |\n",
      "|    time_elapsed    | 14083    |\n",
      "|    total_timesteps | 1064000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1066000, episode_reward=18.00 +/- 5.87\n",
      "Episode length: 727.20 +/- 132.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 727      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1066000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0522  |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53299    |\n",
      "|    policy_loss        | -0.00218 |\n",
      "|    value_loss         | 0.2      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53300    |\n",
      "|    time_elapsed    | 14094    |\n",
      "|    total_timesteps | 1066000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1068000, episode_reward=23.60 +/- 6.28\n",
      "Episode length: 875.00 +/- 170.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1068000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.124   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53399    |\n",
      "|    policy_loss        | -0.0135  |\n",
      "|    value_loss         | 0.226    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53400    |\n",
      "|    time_elapsed    | 14107    |\n",
      "|    total_timesteps | 1068000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=25.60 +/- 11.46\n",
      "Episode length: 792.80 +/- 190.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 793      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0515  |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53499    |\n",
      "|    policy_loss        | -0.0366  |\n",
      "|    value_loss         | 0.547    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53500    |\n",
      "|    time_elapsed    | 14119    |\n",
      "|    total_timesteps | 1070000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1072000, episode_reward=27.40 +/- 5.89\n",
      "Episode length: 909.00 +/- 114.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 909      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1072000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0928  |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53599    |\n",
      "|    policy_loss        | -0.017   |\n",
      "|    value_loss         | 0.24     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53600    |\n",
      "|    time_elapsed    | 14132    |\n",
      "|    total_timesteps | 1072000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1074000, episode_reward=20.80 +/- 4.71\n",
      "Episode length: 6106.40 +/- 10447.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.11e+03 |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1074000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0355  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53699    |\n",
      "|    policy_loss        | -0.00286 |\n",
      "|    value_loss         | 0.0281   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53700    |\n",
      "|    time_elapsed    | 14212    |\n",
      "|    total_timesteps | 1074000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1076000, episode_reward=23.00 +/- 9.86\n",
      "Episode length: 802.00 +/- 128.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 802      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1076000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00427 |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53799    |\n",
      "|    policy_loss        | 6.53e-05 |\n",
      "|    value_loss         | 0.0616   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53800    |\n",
      "|    time_elapsed    | 14223    |\n",
      "|    total_timesteps | 1076000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1078000, episode_reward=19.00 +/- 2.28\n",
      "Episode length: 779.20 +/- 59.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 779      |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1078000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53899    |\n",
      "|    policy_loss        | 0.00412  |\n",
      "|    value_loss         | 0.0323   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 53900    |\n",
      "|    time_elapsed    | 14236    |\n",
      "|    total_timesteps | 1078000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=24.40 +/- 7.50\n",
      "Episode length: 821.40 +/- 138.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 821      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0349  |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 53999    |\n",
      "|    policy_loss        | 0.0101   |\n",
      "|    value_loss         | 0.0868   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54000    |\n",
      "|    time_elapsed    | 14248    |\n",
      "|    total_timesteps | 1080000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1082000, episode_reward=20.60 +/- 7.45\n",
      "Episode length: 6010.20 +/- 10495.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1082000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0623  |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54099    |\n",
      "|    policy_loss        | -0.00493 |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54100    |\n",
      "|    time_elapsed    | 14327    |\n",
      "|    total_timesteps | 1082000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1084000, episode_reward=26.60 +/- 8.48\n",
      "Episode length: 918.00 +/- 190.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 918      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1084000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0683  |\n",
      "|    explained_variance | 0.815    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | -0.0157  |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54200    |\n",
      "|    time_elapsed    | 14340    |\n",
      "|    total_timesteps | 1084000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1086000, episode_reward=21.80 +/- 5.15\n",
      "Episode length: 842.20 +/- 140.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 842      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1086000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0227  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54299    |\n",
      "|    policy_loss        | 0.000632 |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54300    |\n",
      "|    time_elapsed    | 14352    |\n",
      "|    total_timesteps | 1086000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1088000, episode_reward=19.20 +/- 10.24\n",
      "Episode length: 6082.20 +/- 10459.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1088000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54399    |\n",
      "|    policy_loss        | -0.00642 |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54400    |\n",
      "|    time_elapsed    | 14434    |\n",
      "|    total_timesteps | 1088000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=21.80 +/- 8.06\n",
      "Episode length: 6096.20 +/- 10452.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00846 |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54499    |\n",
      "|    policy_loss        | 0.000208 |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54500    |\n",
      "|    time_elapsed    | 14516    |\n",
      "|    total_timesteps | 1090000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1092000, episode_reward=25.40 +/- 9.56\n",
      "Episode length: 922.20 +/- 187.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 922      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1092000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0134  |\n",
      "|    explained_variance | 0.356    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54599    |\n",
      "|    policy_loss        | -0.00677 |\n",
      "|    value_loss         | 0.653    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54600    |\n",
      "|    time_elapsed    | 14529    |\n",
      "|    total_timesteps | 1092000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1094000, episode_reward=18.00 +/- 6.00\n",
      "Episode length: 712.00 +/- 162.79\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 712       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1094000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0056   |\n",
      "|    explained_variance | 0.899     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54699     |\n",
      "|    policy_loss        | -0.000174 |\n",
      "|    value_loss         | 0.0555    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 54700    |\n",
      "|    time_elapsed    | 14541    |\n",
      "|    total_timesteps | 1094000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1096000, episode_reward=15.40 +/- 8.19\n",
      "Episode length: 6018.00 +/- 10491.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1096000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0413  |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54799    |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.241    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 54800    |\n",
      "|    time_elapsed    | 14623    |\n",
      "|    total_timesteps | 1096000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1098000, episode_reward=21.20 +/- 7.86\n",
      "Episode length: 6115.80 +/- 10442.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1098000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54899    |\n",
      "|    policy_loss        | 0.00501  |\n",
      "|    value_loss         | 0.116    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 54900    |\n",
      "|    time_elapsed    | 14703    |\n",
      "|    total_timesteps | 1098000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=27.00 +/- 8.79\n",
      "Episode length: 817.00 +/- 160.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 817      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0437  |\n",
      "|    explained_variance | 0.843    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 54999    |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    value_loss         | 0.188    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55000    |\n",
      "|    time_elapsed    | 14714    |\n",
      "|    total_timesteps | 1100000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1102000, episode_reward=20.40 +/- 10.69\n",
      "Episode length: 6113.40 +/- 10445.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.11e+03 |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1102000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0466  |\n",
      "|    explained_variance | 0.805    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55099    |\n",
      "|    policy_loss        | 0.00532  |\n",
      "|    value_loss         | 0.23     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55100    |\n",
      "|    time_elapsed    | 14794    |\n",
      "|    total_timesteps | 1102000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1104000, episode_reward=22.20 +/- 2.79\n",
      "Episode length: 797.80 +/- 66.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1104000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0897  |\n",
      "|    explained_variance | 0.527    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55199    |\n",
      "|    policy_loss        | -0.0236  |\n",
      "|    value_loss         | 0.384    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55200    |\n",
      "|    time_elapsed    | 14805    |\n",
      "|    total_timesteps | 1104000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1106000, episode_reward=23.80 +/- 9.99\n",
      "Episode length: 848.40 +/- 220.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 848      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1106000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0135  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55299    |\n",
      "|    policy_loss        | 0.000645 |\n",
      "|    value_loss         | 0.0673   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55300    |\n",
      "|    time_elapsed    | 14818    |\n",
      "|    total_timesteps | 1106000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1108000, episode_reward=22.80 +/- 6.97\n",
      "Episode length: 829.60 +/- 167.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 830      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1108000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0671  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55399    |\n",
      "|    policy_loss        | 0.00305  |\n",
      "|    value_loss         | 0.051    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55400    |\n",
      "|    time_elapsed    | 14832    |\n",
      "|    total_timesteps | 1108000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=23.80 +/- 9.64\n",
      "Episode length: 6053.80 +/- 10474.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0318  |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55499    |\n",
      "|    policy_loss        | -0.0945  |\n",
      "|    value_loss         | 0.0887   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55500    |\n",
      "|    time_elapsed    | 14912    |\n",
      "|    total_timesteps | 1110000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1112000, episode_reward=27.40 +/- 4.63\n",
      "Episode length: 6198.80 +/- 10401.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.2e+03  |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1112000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.176   |\n",
      "|    explained_variance | 0.166    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.851    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55600    |\n",
      "|    time_elapsed    | 14994    |\n",
      "|    total_timesteps | 1112000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1114000, episode_reward=25.00 +/- 6.36\n",
      "Episode length: 899.20 +/- 143.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 899      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1114000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00779 |\n",
      "|    explained_variance | 0.381    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | -0.00016 |\n",
      "|    value_loss         | 0.24     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55700    |\n",
      "|    time_elapsed    | 15007    |\n",
      "|    total_timesteps | 1114000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1116000, episode_reward=19.20 +/- 5.98\n",
      "Episode length: 742.80 +/- 177.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 743      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1116000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0186  |\n",
      "|    explained_variance | 0.32     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55799    |\n",
      "|    policy_loss        | -8.4e-05 |\n",
      "|    value_loss         | 0.225    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55800    |\n",
      "|    time_elapsed    | 15019    |\n",
      "|    total_timesteps | 1116000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1118000, episode_reward=26.80 +/- 11.29\n",
      "Episode length: 892.60 +/- 196.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 893      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1118000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0351  |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55899    |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    value_loss         | 0.285    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 55900    |\n",
      "|    time_elapsed    | 15032    |\n",
      "|    total_timesteps | 1118000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=25.80 +/- 6.49\n",
      "Episode length: 938.20 +/- 192.49\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 938      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0528  |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 55999    |\n",
      "|    policy_loss        | -0.00556 |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56000    |\n",
      "|    time_elapsed    | 15046    |\n",
      "|    total_timesteps | 1120000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1122000, episode_reward=30.80 +/- 5.27\n",
      "Episode length: 948.20 +/- 156.77\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 948       |\n",
      "|    mean_reward        | 30.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1122000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0187   |\n",
      "|    explained_variance | 0.989     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56099     |\n",
      "|    policy_loss        | -0.000958 |\n",
      "|    value_loss         | 0.0241    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56100    |\n",
      "|    time_elapsed    | 15060    |\n",
      "|    total_timesteps | 1122000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1124000, episode_reward=24.20 +/- 5.67\n",
      "Episode length: 907.80 +/- 137.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1124000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56199    |\n",
      "|    policy_loss        | -0.0488  |\n",
      "|    value_loss         | 0.153    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56200    |\n",
      "|    time_elapsed    | 15073    |\n",
      "|    total_timesteps | 1124000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1126000, episode_reward=21.00 +/- 5.22\n",
      "Episode length: 839.40 +/- 151.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 839      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1126000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0174  |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56299    |\n",
      "|    policy_loss        | 0.00055  |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56300    |\n",
      "|    time_elapsed    | 15086    |\n",
      "|    total_timesteps | 1126000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1128000, episode_reward=20.80 +/- 5.27\n",
      "Episode length: 721.20 +/- 127.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 721       |\n",
      "|    mean_reward        | 20.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1128000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00586  |\n",
      "|    explained_variance | 0.927     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56399     |\n",
      "|    policy_loss        | -0.000116 |\n",
      "|    value_loss         | 0.429     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56400    |\n",
      "|    time_elapsed    | 15097    |\n",
      "|    total_timesteps | 1128000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=17.20 +/- 1.94\n",
      "Episode length: 697.80 +/- 96.42\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 698       |\n",
      "|    mean_reward        | 17.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1130000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000679 |\n",
      "|    explained_variance | 0.989     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56499     |\n",
      "|    policy_loss        | 1.41e-05  |\n",
      "|    value_loss         | 0.0255    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56500    |\n",
      "|    time_elapsed    | 15109    |\n",
      "|    total_timesteps | 1130000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1132000, episode_reward=16.00 +/- 6.60\n",
      "Episode length: 5941.40 +/- 10529.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.94e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1132000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0588  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | 0.00716  |\n",
      "|    value_loss         | 0.0647   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56600    |\n",
      "|    time_elapsed    | 15190    |\n",
      "|    total_timesteps | 1132000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1134000, episode_reward=22.40 +/- 4.80\n",
      "Episode length: 805.00 +/- 98.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 805      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1134000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0389  |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56699    |\n",
      "|    policy_loss        | 0.008    |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56700    |\n",
      "|    time_elapsed    | 15202    |\n",
      "|    total_timesteps | 1134000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1136000, episode_reward=26.20 +/- 6.05\n",
      "Episode length: 858.20 +/- 58.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 858      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1136000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0448  |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56799    |\n",
      "|    policy_loss        | -0.00615 |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56800    |\n",
      "|    time_elapsed    | 15215    |\n",
      "|    total_timesteps | 1136000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1138000, episode_reward=22.60 +/- 7.89\n",
      "Episode length: 792.00 +/- 139.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1138000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0233  |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56899    |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    value_loss         | 0.0278   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 56900    |\n",
      "|    time_elapsed    | 15228    |\n",
      "|    total_timesteps | 1138000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=24.00 +/- 12.46\n",
      "Episode length: 6070.60 +/- 10465.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0508  |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | -0.0105  |\n",
      "|    value_loss         | 0.0846   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57000    |\n",
      "|    time_elapsed    | 15309    |\n",
      "|    total_timesteps | 1140000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1142000, episode_reward=21.60 +/- 6.47\n",
      "Episode length: 787.40 +/- 125.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1142000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0119  |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57099    |\n",
      "|    policy_loss        | 0.000526 |\n",
      "|    value_loss         | 0.217    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57100    |\n",
      "|    time_elapsed    | 15321    |\n",
      "|    total_timesteps | 1142000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1144000, episode_reward=24.80 +/- 5.31\n",
      "Episode length: 851.00 +/- 112.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 851      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1144000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57199    |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57200    |\n",
      "|    time_elapsed    | 15333    |\n",
      "|    total_timesteps | 1144000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1146000, episode_reward=22.80 +/- 6.88\n",
      "Episode length: 849.60 +/- 182.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 850      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1146000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0481  |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57299    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.261    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57300    |\n",
      "|    time_elapsed    | 15345    |\n",
      "|    total_timesteps | 1146000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1148000, episode_reward=25.60 +/- 5.85\n",
      "Episode length: 777.80 +/- 84.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1148000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0619  |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57399    |\n",
      "|    policy_loss        | -0.00666 |\n",
      "|    value_loss         | 0.054    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57400    |\n",
      "|    time_elapsed    | 15358    |\n",
      "|    total_timesteps | 1148000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=31.00 +/- 9.36\n",
      "Episode length: 1017.00 +/- 198.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.02e+03 |\n",
      "|    mean_reward        | 31       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0615  |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57499    |\n",
      "|    policy_loss        | 0.0157   |\n",
      "|    value_loss         | 0.0796   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57500    |\n",
      "|    time_elapsed    | 15371    |\n",
      "|    total_timesteps | 1150000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1152000, episode_reward=17.40 +/- 9.29\n",
      "Episode length: 6058.80 +/- 10470.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1152000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0676  |\n",
      "|    explained_variance | -8.62    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57599    |\n",
      "|    policy_loss        | 0.109    |\n",
      "|    value_loss         | 0.549    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57600    |\n",
      "|    time_elapsed    | 15452    |\n",
      "|    total_timesteps | 1152000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1154000, episode_reward=24.60 +/- 8.85\n",
      "Episode length: 907.00 +/- 160.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 907      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1154000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.033   |\n",
      "|    explained_variance | 0.837    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57699    |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 0.082    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57700    |\n",
      "|    time_elapsed    | 15464    |\n",
      "|    total_timesteps | 1154000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1156000, episode_reward=24.80 +/- 5.19\n",
      "Episode length: 935.20 +/- 112.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 935      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1156000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57799    |\n",
      "|    policy_loss        | 0.00193  |\n",
      "|    value_loss         | 0.0907   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57800    |\n",
      "|    time_elapsed    | 15477    |\n",
      "|    total_timesteps | 1156000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1158000, episode_reward=28.00 +/- 4.29\n",
      "Episode length: 911.40 +/- 89.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 911      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1158000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0422  |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57899    |\n",
      "|    policy_loss        | 0.00137  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 57900    |\n",
      "|    time_elapsed    | 15490    |\n",
      "|    total_timesteps | 1158000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=25.00 +/- 6.03\n",
      "Episode length: 879.20 +/- 100.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 879      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.051   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 57999    |\n",
      "|    policy_loss        | 0.000552 |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58000    |\n",
      "|    time_elapsed    | 15503    |\n",
      "|    total_timesteps | 1160000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1162000, episode_reward=18.20 +/- 10.65\n",
      "Episode length: 5996.60 +/- 10502.10\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6e+03     |\n",
      "|    mean_reward        | 18.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1162000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.91e-05 |\n",
      "|    explained_variance | 0.707     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58099     |\n",
      "|    policy_loss        | -2.51e-06 |\n",
      "|    value_loss         | 0.34      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58100    |\n",
      "|    time_elapsed    | 15585    |\n",
      "|    total_timesteps | 1162000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1164000, episode_reward=23.40 +/- 5.39\n",
      "Episode length: 791.60 +/- 81.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 792      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1164000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0548  |\n",
      "|    explained_variance | -0.703   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58199    |\n",
      "|    policy_loss        | -0.0036  |\n",
      "|    value_loss         | 2.42     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58200    |\n",
      "|    time_elapsed    | 15595    |\n",
      "|    total_timesteps | 1164000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1166000, episode_reward=21.60 +/- 3.93\n",
      "Episode length: 836.40 +/- 108.04\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 836       |\n",
      "|    mean_reward        | 21.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1166000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00499  |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58299     |\n",
      "|    policy_loss        | -4.58e-06 |\n",
      "|    value_loss         | 0.316     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58300    |\n",
      "|    time_elapsed    | 15608    |\n",
      "|    total_timesteps | 1166000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1168000, episode_reward=28.00 +/- 4.15\n",
      "Episode length: 905.80 +/- 161.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1168000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0446  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58399    |\n",
      "|    policy_loss        | -0.00213 |\n",
      "|    value_loss         | 0.0329   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58400    |\n",
      "|    time_elapsed    | 15620    |\n",
      "|    total_timesteps | 1168000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=24.40 +/- 6.65\n",
      "Episode length: 6047.40 +/- 10476.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0582  |\n",
      "|    explained_variance | 0.761    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58499    |\n",
      "|    policy_loss        | 0.0284   |\n",
      "|    value_loss         | 0.0959   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58500    |\n",
      "|    time_elapsed    | 15700    |\n",
      "|    total_timesteps | 1170000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1172000, episode_reward=20.00 +/- 5.25\n",
      "Episode length: 777.60 +/- 92.91\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 778       |\n",
      "|    mean_reward        | 20        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1172000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000564 |\n",
      "|    explained_variance | 0.893     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58599     |\n",
      "|    policy_loss        | -2.27e-05 |\n",
      "|    value_loss         | 0.481     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58600    |\n",
      "|    time_elapsed    | 15712    |\n",
      "|    total_timesteps | 1172000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1174000, episode_reward=26.60 +/- 5.75\n",
      "Episode length: 864.20 +/- 82.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 864      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1174000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0288  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58699    |\n",
      "|    policy_loss        | -0.0033  |\n",
      "|    value_loss         | 0.0683   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58700    |\n",
      "|    time_elapsed    | 15725    |\n",
      "|    total_timesteps | 1174000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1176000, episode_reward=25.20 +/- 5.19\n",
      "Episode length: 849.20 +/- 36.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 849      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1176000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.017   |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | 0.00276  |\n",
      "|    value_loss         | 0.0732   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58800    |\n",
      "|    time_elapsed    | 15737    |\n",
      "|    total_timesteps | 1176000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1178000, episode_reward=20.60 +/- 3.38\n",
      "Episode length: 734.80 +/- 31.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 735      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1178000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0781  |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58899    |\n",
      "|    policy_loss        | -0.0492  |\n",
      "|    value_loss         | 0.0459   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 58900    |\n",
      "|    time_elapsed    | 15749    |\n",
      "|    total_timesteps | 1178000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=24.60 +/- 6.83\n",
      "Episode length: 927.60 +/- 170.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 928      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0423  |\n",
      "|    explained_variance | 0.379    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 58999    |\n",
      "|    policy_loss        | 0.00246  |\n",
      "|    value_loss         | 0.321    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59000    |\n",
      "|    time_elapsed    | 15762    |\n",
      "|    total_timesteps | 1180000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1182000, episode_reward=28.80 +/- 8.98\n",
      "Episode length: 830.00 +/- 161.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 830      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1182000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0246  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59099    |\n",
      "|    policy_loss        | 0.00212  |\n",
      "|    value_loss         | 0.0516   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59100    |\n",
      "|    time_elapsed    | 15775    |\n",
      "|    total_timesteps | 1182000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1184000, episode_reward=20.00 +/- 4.34\n",
      "Episode length: 743.80 +/- 104.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 744      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1184000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00379 |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | 1.89e-05 |\n",
      "|    value_loss         | 0.0786   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59200    |\n",
      "|    time_elapsed    | 15786    |\n",
      "|    total_timesteps | 1184000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1186000, episode_reward=18.00 +/- 3.35\n",
      "Episode length: 751.20 +/- 77.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 751      |\n",
      "|    mean_reward        | 18       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1186000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00393 |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59299    |\n",
      "|    policy_loss        | 0.000281 |\n",
      "|    value_loss         | 0.0975   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 59300    |\n",
      "|    time_elapsed    | 15799    |\n",
      "|    total_timesteps | 1186000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1188000, episode_reward=20.20 +/- 4.79\n",
      "Episode length: 788.80 +/- 106.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1188000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00568 |\n",
      "|    explained_variance | 0.337    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59399    |\n",
      "|    policy_loss        | 0.000683 |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 59400    |\n",
      "|    time_elapsed    | 15811    |\n",
      "|    total_timesteps | 1188000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=17.80 +/- 9.89\n",
      "Episode length: 6042.80 +/- 10478.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0792  |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59499    |\n",
      "|    policy_loss        | 0.000693 |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59500    |\n",
      "|    time_elapsed    | 15893    |\n",
      "|    total_timesteps | 1190000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1192000, episode_reward=23.40 +/- 6.53\n",
      "Episode length: 835.60 +/- 74.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1192000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0723  |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59599    |\n",
      "|    policy_loss        | 0.00642  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59600    |\n",
      "|    time_elapsed    | 15904    |\n",
      "|    total_timesteps | 1192000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1194000, episode_reward=26.40 +/- 7.96\n",
      "Episode length: 993.00 +/- 217.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 993      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1194000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.046   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59699    |\n",
      "|    policy_loss        | 0.0606   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 59700    |\n",
      "|    time_elapsed    | 15918    |\n",
      "|    total_timesteps | 1194000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1196000, episode_reward=23.00 +/- 7.13\n",
      "Episode length: 863.00 +/- 150.53\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 863       |\n",
      "|    mean_reward        | 23        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1196000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0113   |\n",
      "|    explained_variance | 0.638     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59799     |\n",
      "|    policy_loss        | -0.000133 |\n",
      "|    value_loss         | 0.103     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 59800    |\n",
      "|    time_elapsed    | 15931    |\n",
      "|    total_timesteps | 1196000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1198000, episode_reward=21.40 +/- 2.73\n",
      "Episode length: 6047.60 +/- 10476.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1198000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0743  |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59899    |\n",
      "|    policy_loss        | 0.0567   |\n",
      "|    value_loss         | 0.248    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 59900    |\n",
      "|    time_elapsed    | 16011    |\n",
      "|    total_timesteps | 1198000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=22.60 +/- 5.68\n",
      "Episode length: 737.80 +/- 107.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 738      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0428  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 59999    |\n",
      "|    policy_loss        | 0.00114  |\n",
      "|    value_loss         | 0.0436   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 60000    |\n",
      "|    time_elapsed    | 16021    |\n",
      "|    total_timesteps | 1200000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1202000, episode_reward=21.00 +/- 3.03\n",
      "Episode length: 787.80 +/- 78.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1202000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00255 |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60099    |\n",
      "|    policy_loss        | 5.3e-05  |\n",
      "|    value_loss         | 0.0799   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 23.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 60100    |\n",
      "|    time_elapsed    | 16034    |\n",
      "|    total_timesteps | 1202000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1204000, episode_reward=18.20 +/- 3.66\n",
      "Episode length: 782.80 +/- 67.43\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 783       |\n",
      "|    mean_reward        | 18.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1204000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000325 |\n",
      "|    explained_variance | 0.986     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60199     |\n",
      "|    policy_loss        | 4.78e-06  |\n",
      "|    value_loss         | 0.0645    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60200    |\n",
      "|    time_elapsed    | 16046    |\n",
      "|    total_timesteps | 1204000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1206000, episode_reward=27.20 +/- 6.11\n",
      "Episode length: 793.60 +/- 90.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 794       |\n",
      "|    mean_reward        | 27.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1206000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.001    |\n",
      "|    explained_variance | 0.863     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60299     |\n",
      "|    policy_loss        | -2.27e-05 |\n",
      "|    value_loss         | 0.0868    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60300    |\n",
      "|    time_elapsed    | 16058    |\n",
      "|    total_timesteps | 1206000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1208000, episode_reward=26.20 +/- 7.70\n",
      "Episode length: 894.00 +/- 174.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 894      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1208000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0101  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60399    |\n",
      "|    policy_loss        | 0.000248 |\n",
      "|    value_loss         | 0.076    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60400    |\n",
      "|    time_elapsed    | 16071    |\n",
      "|    total_timesteps | 1208000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=22.80 +/- 5.56\n",
      "Episode length: 835.00 +/- 147.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 835      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.087   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60499    |\n",
      "|    policy_loss        | 0.0282   |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60500    |\n",
      "|    time_elapsed    | 16083    |\n",
      "|    total_timesteps | 1210000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1212000, episode_reward=12.80 +/- 6.79\n",
      "Episode length: 5969.40 +/- 10515.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 12.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1212000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0568  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60599    |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 60600    |\n",
      "|    time_elapsed    | 16164    |\n",
      "|    total_timesteps | 1212000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1214000, episode_reward=22.80 +/- 6.85\n",
      "Episode length: 835.80 +/- 118.73\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 836       |\n",
      "|    mean_reward        | 22.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1214000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00106  |\n",
      "|    explained_variance | 0.72      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60699     |\n",
      "|    policy_loss        | -4.15e-05 |\n",
      "|    value_loss         | 0.249     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60700    |\n",
      "|    time_elapsed    | 16176    |\n",
      "|    total_timesteps | 1214000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1216000, episode_reward=25.20 +/- 8.08\n",
      "Episode length: 834.20 +/- 172.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 834      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1216000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0384  |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60799    |\n",
      "|    policy_loss        | -0.00514 |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60800    |\n",
      "|    time_elapsed    | 16189    |\n",
      "|    total_timesteps | 1216000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1218000, episode_reward=25.20 +/- 4.66\n",
      "Episode length: 799.20 +/- 56.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 799      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1218000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0438  |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | 0.00278  |\n",
      "|    value_loss         | 0.0687   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 60900    |\n",
      "|    time_elapsed    | 16201    |\n",
      "|    total_timesteps | 1218000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=26.00 +/- 10.71\n",
      "Episode length: 941.00 +/- 237.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 941      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0454  |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 60999    |\n",
      "|    policy_loss        | -0.00607 |\n",
      "|    value_loss         | 0.224    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61000    |\n",
      "|    time_elapsed    | 16214    |\n",
      "|    total_timesteps | 1220000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1222000, episode_reward=21.00 +/- 6.32\n",
      "Episode length: 765.20 +/- 107.90\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 765       |\n",
      "|    mean_reward        | 21        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1222000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0284   |\n",
      "|    explained_variance | 0.972     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61099     |\n",
      "|    policy_loss        | -0.000321 |\n",
      "|    value_loss         | 0.017     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61100    |\n",
      "|    time_elapsed    | 16226    |\n",
      "|    total_timesteps | 1222000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1224000, episode_reward=23.40 +/- 6.37\n",
      "Episode length: 742.20 +/- 175.03\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 742      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1224000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0257  |\n",
      "|    explained_variance | 0.346    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61199    |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.616    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61200    |\n",
      "|    time_elapsed    | 16239    |\n",
      "|    total_timesteps | 1224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1226000, episode_reward=24.40 +/- 2.94\n",
      "Episode length: 871.20 +/- 81.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 871      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1226000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0861  |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61299    |\n",
      "|    policy_loss        | 0.000401 |\n",
      "|    value_loss         | 0.152    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 23.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61300    |\n",
      "|    time_elapsed    | 16252    |\n",
      "|    total_timesteps | 1226000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1228000, episode_reward=17.60 +/- 6.34\n",
      "Episode length: 5984.20 +/- 10509.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.98e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1228000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0211  |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61399    |\n",
      "|    policy_loss        | 0.000321 |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61400    |\n",
      "|    time_elapsed    | 16332    |\n",
      "|    total_timesteps | 1228000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=32.20 +/- 10.05\n",
      "Episode length: 945.20 +/- 157.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 945      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0441  |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61499    |\n",
      "|    policy_loss        | -0.00437 |\n",
      "|    value_loss         | 0.321    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61500    |\n",
      "|    time_elapsed    | 16344    |\n",
      "|    total_timesteps | 1230000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1232000, episode_reward=25.20 +/- 4.35\n",
      "Episode length: 848.80 +/- 159.12\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 849       |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1232000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000455 |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61599     |\n",
      "|    policy_loss        | -7.06e-05 |\n",
      "|    value_loss         | 0.139     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61600    |\n",
      "|    time_elapsed    | 16355    |\n",
      "|    total_timesteps | 1232000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1234000, episode_reward=22.60 +/- 7.09\n",
      "Episode length: 815.60 +/- 138.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 816      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1234000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0988  |\n",
      "|    explained_variance | 0.465    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61699    |\n",
      "|    policy_loss        | -0.045   |\n",
      "|    value_loss         | 0.546    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61700    |\n",
      "|    time_elapsed    | 16367    |\n",
      "|    total_timesteps | 1234000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1236000, episode_reward=21.80 +/- 5.46\n",
      "Episode length: 840.60 +/- 113.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1236000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0558  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61799    |\n",
      "|    policy_loss        | 0.0339   |\n",
      "|    value_loss         | 0.0975   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61800    |\n",
      "|    time_elapsed    | 16380    |\n",
      "|    total_timesteps | 1236000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1238000, episode_reward=28.00 +/- 7.54\n",
      "Episode length: 904.60 +/- 149.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 905      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1238000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0129  |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61899    |\n",
      "|    policy_loss        | -0.002   |\n",
      "|    value_loss         | 0.564    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 23.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 61900    |\n",
      "|    time_elapsed    | 16392    |\n",
      "|    total_timesteps | 1238000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=22.00 +/- 4.77\n",
      "Episode length: 768.40 +/- 121.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 768      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 61999    |\n",
      "|    policy_loss        | 0.00771  |\n",
      "|    value_loss         | 0.293    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62000    |\n",
      "|    time_elapsed    | 16405    |\n",
      "|    total_timesteps | 1240000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1242000, episode_reward=26.80 +/- 7.22\n",
      "Episode length: 906.60 +/- 184.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 907      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1242000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0463  |\n",
      "|    explained_variance | 0.101    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62099    |\n",
      "|    policy_loss        | -0.00926 |\n",
      "|    value_loss         | 0.617    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62100    |\n",
      "|    time_elapsed    | 16417    |\n",
      "|    total_timesteps | 1242000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1244000, episode_reward=20.00 +/- 6.78\n",
      "Episode length: 787.40 +/- 177.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1244000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0382  |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | 0.0079   |\n",
      "|    value_loss         | 0.0484   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62200    |\n",
      "|    time_elapsed    | 16428    |\n",
      "|    total_timesteps | 1244000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1246000, episode_reward=25.40 +/- 9.22\n",
      "Episode length: 821.80 +/- 172.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 822      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1246000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62299    |\n",
      "|    policy_loss        | 0.0603   |\n",
      "|    value_loss         | 0.0593   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62300    |\n",
      "|    time_elapsed    | 16440    |\n",
      "|    total_timesteps | 1246000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1248000, episode_reward=21.00 +/- 2.76\n",
      "Episode length: 776.20 +/- 82.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 776      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1248000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62399    |\n",
      "|    policy_loss        | -0.0923  |\n",
      "|    value_loss         | 0.0821   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62400    |\n",
      "|    time_elapsed    | 16452    |\n",
      "|    total_timesteps | 1248000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=26.40 +/- 8.19\n",
      "Episode length: 890.80 +/- 125.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 891      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62499    |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62500    |\n",
      "|    time_elapsed    | 16465    |\n",
      "|    total_timesteps | 1250000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1252000, episode_reward=23.60 +/- 7.99\n",
      "Episode length: 859.80 +/- 88.29\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 860       |\n",
      "|    mean_reward        | 23.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1252000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.9e-05  |\n",
      "|    explained_variance | 0.874     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62599     |\n",
      "|    policy_loss        | -1.81e-06 |\n",
      "|    value_loss         | 0.499     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 62600    |\n",
      "|    time_elapsed    | 16478    |\n",
      "|    total_timesteps | 1252000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1254000, episode_reward=24.00 +/- 6.16\n",
      "Episode length: 896.40 +/- 142.94\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 896       |\n",
      "|    mean_reward        | 24        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1254000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000307 |\n",
      "|    explained_variance | 0.993     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62699     |\n",
      "|    policy_loss        | 5.39e-06  |\n",
      "|    value_loss         | 0.0156    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 62700    |\n",
      "|    time_elapsed    | 16490    |\n",
      "|    total_timesteps | 1254000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1256000, episode_reward=20.20 +/- 3.92\n",
      "Episode length: 784.60 +/- 134.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 785      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1256000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0519  |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62799    |\n",
      "|    policy_loss        | -0.00686 |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 62800    |\n",
      "|    time_elapsed    | 16502    |\n",
      "|    total_timesteps | 1256000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1258000, episode_reward=26.40 +/- 5.85\n",
      "Episode length: 830.20 +/- 218.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 830      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1258000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | 0.244    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 62899    |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 1.68     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 62900    |\n",
      "|    time_elapsed    | 16515    |\n",
      "|    total_timesteps | 1258000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=21.40 +/- 5.54\n",
      "Episode length: 776.60 +/- 134.69\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 777       |\n",
      "|    mean_reward        | 21.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1260000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00143  |\n",
      "|    explained_variance | 0.927     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62999     |\n",
      "|    policy_loss        | -6.42e-06 |\n",
      "|    value_loss         | 0.117     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63000    |\n",
      "|    time_elapsed    | 16527    |\n",
      "|    total_timesteps | 1260000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1262000, episode_reward=21.20 +/- 5.04\n",
      "Episode length: 804.80 +/- 124.54\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 805       |\n",
      "|    mean_reward        | 21.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1262000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0619   |\n",
      "|    explained_variance | 0.941     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63099     |\n",
      "|    policy_loss        | -0.000703 |\n",
      "|    value_loss         | 0.173     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63100    |\n",
      "|    time_elapsed    | 16539    |\n",
      "|    total_timesteps | 1262000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1264000, episode_reward=29.20 +/- 4.96\n",
      "Episode length: 946.80 +/- 75.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 947      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1264000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00729 |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63199    |\n",
      "|    policy_loss        | -0.0458  |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63200    |\n",
      "|    time_elapsed    | 16552    |\n",
      "|    total_timesteps | 1264000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1266000, episode_reward=24.00 +/- 3.52\n",
      "Episode length: 844.80 +/- 102.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1266000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.188   |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63299    |\n",
      "|    policy_loss        | -0.00366 |\n",
      "|    value_loss         | 0.212    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63300    |\n",
      "|    time_elapsed    | 16565    |\n",
      "|    total_timesteps | 1266000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1268000, episode_reward=27.80 +/- 4.92\n",
      "Episode length: 996.20 +/- 106.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 996      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1268000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0991  |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63399    |\n",
      "|    policy_loss        | 0.0342   |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63400    |\n",
      "|    time_elapsed    | 16578    |\n",
      "|    total_timesteps | 1268000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=25.20 +/- 5.49\n",
      "Episode length: 884.20 +/- 102.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 884      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0176  |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | 0.00081  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63500    |\n",
      "|    time_elapsed    | 16591    |\n",
      "|    total_timesteps | 1270000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1272000, episode_reward=18.80 +/- 8.28\n",
      "Episode length: 6050.60 +/- 10475.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1272000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0637  |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63599    |\n",
      "|    policy_loss        | -0.0462  |\n",
      "|    value_loss         | 0.063    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63600    |\n",
      "|    time_elapsed    | 16671    |\n",
      "|    total_timesteps | 1272000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1274000, episode_reward=25.40 +/- 4.63\n",
      "Episode length: 920.60 +/- 61.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 921      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1274000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0285  |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63699    |\n",
      "|    policy_loss        | -0.00183 |\n",
      "|    value_loss         | 0.208    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63700    |\n",
      "|    time_elapsed    | 16684    |\n",
      "|    total_timesteps | 1274000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1276000, episode_reward=19.40 +/- 11.46\n",
      "Episode length: 6102.60 +/- 10449.50\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.1e+03   |\n",
      "|    mean_reward        | 19.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1276000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0708   |\n",
      "|    explained_variance | 0.996     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63799     |\n",
      "|    policy_loss        | -0.000974 |\n",
      "|    value_loss         | 0.0207    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 63800    |\n",
      "|    time_elapsed    | 16765    |\n",
      "|    total_timesteps | 1276000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1278000, episode_reward=23.40 +/- 8.26\n",
      "Episode length: 6146.20 +/- 10427.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.15e+03  |\n",
      "|    mean_reward        | 23.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1278000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0259   |\n",
      "|    explained_variance | 0.901     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63899     |\n",
      "|    policy_loss        | -0.000764 |\n",
      "|    value_loss         | 0.65      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 63900    |\n",
      "|    time_elapsed    | 16848    |\n",
      "|    total_timesteps | 1278000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=23.20 +/- 5.04\n",
      "Episode length: 860.00 +/- 91.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 860      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0189  |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 63999    |\n",
      "|    policy_loss        | 0.0012   |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64000    |\n",
      "|    time_elapsed    | 16861    |\n",
      "|    total_timesteps | 1280000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1282000, episode_reward=21.00 +/- 12.10\n",
      "Episode length: 6037.40 +/- 10482.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1282000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0318  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64099    |\n",
      "|    policy_loss        | -0.00145 |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64100    |\n",
      "|    time_elapsed    | 16942    |\n",
      "|    total_timesteps | 1282000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1284000, episode_reward=18.80 +/- 9.68\n",
      "Episode length: 6081.20 +/- 10459.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1284000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0422  |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64199    |\n",
      "|    policy_loss        | 0.00196  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64200    |\n",
      "|    time_elapsed    | 17023    |\n",
      "|    total_timesteps | 1284000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1286000, episode_reward=27.60 +/- 4.32\n",
      "Episode length: 922.00 +/- 153.36\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 922       |\n",
      "|    mean_reward        | 27.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1286000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00026  |\n",
      "|    explained_variance | 0.612     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64299     |\n",
      "|    policy_loss        | -4.68e-06 |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64300    |\n",
      "|    time_elapsed    | 17036    |\n",
      "|    total_timesteps | 1286000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1288000, episode_reward=19.80 +/- 13.33\n",
      "Episode length: 6140.60 +/- 10431.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.14e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1288000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64399    |\n",
      "|    policy_loss        | 0.000298 |\n",
      "|    value_loss         | 0.265    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64400    |\n",
      "|    time_elapsed    | 17118    |\n",
      "|    total_timesteps | 1288000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=22.80 +/- 4.07\n",
      "Episode length: 818.80 +/- 76.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0708  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64499    |\n",
      "|    policy_loss        | 3.83e-06 |\n",
      "|    value_loss         | 0.024    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64500    |\n",
      "|    time_elapsed    | 17128    |\n",
      "|    total_timesteps | 1290000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1292000, episode_reward=23.60 +/- 7.71\n",
      "Episode length: 831.40 +/- 195.87\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 831       |\n",
      "|    mean_reward        | 23.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1292000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0123   |\n",
      "|    explained_variance | 0.918     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64599     |\n",
      "|    policy_loss        | -0.000534 |\n",
      "|    value_loss         | 0.068     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64600    |\n",
      "|    time_elapsed    | 17141    |\n",
      "|    total_timesteps | 1292000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1294000, episode_reward=27.60 +/- 14.73\n",
      "Episode length: 6205.00 +/- 10398.80\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.2e+03  |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1294000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0221  |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64699    |\n",
      "|    policy_loss        | -0.00014 |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64700    |\n",
      "|    time_elapsed    | 17221    |\n",
      "|    total_timesteps | 1294000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1296000, episode_reward=26.40 +/- 7.61\n",
      "Episode length: 924.80 +/- 154.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 925      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1296000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0937  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64799    |\n",
      "|    policy_loss        | 0.00118  |\n",
      "|    value_loss         | 0.0764   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64800    |\n",
      "|    time_elapsed    | 17233    |\n",
      "|    total_timesteps | 1296000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1298000, episode_reward=28.00 +/- 5.55\n",
      "Episode length: 982.80 +/- 112.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 983      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1298000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0874  |\n",
      "|    explained_variance | 0.187    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 64899    |\n",
      "|    policy_loss        | -0.0148  |\n",
      "|    value_loss         | 0.565    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 64900    |\n",
      "|    time_elapsed    | 17246    |\n",
      "|    total_timesteps | 1298000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=24.00 +/- 7.69\n",
      "Episode length: 866.00 +/- 186.08\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 866       |\n",
      "|    mean_reward        | 24        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1300000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00804  |\n",
      "|    explained_variance | 0.974     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64999     |\n",
      "|    policy_loss        | -0.000115 |\n",
      "|    value_loss         | 0.0654    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65000    |\n",
      "|    time_elapsed    | 17258    |\n",
      "|    total_timesteps | 1300000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1302000, episode_reward=26.00 +/- 9.27\n",
      "Episode length: 901.20 +/- 223.10\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 901      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1302000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.062   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65099    |\n",
      "|    policy_loss        | 0.000524 |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65100    |\n",
      "|    time_elapsed    | 17270    |\n",
      "|    total_timesteps | 1302000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1304000, episode_reward=22.80 +/- 3.54\n",
      "Episode length: 821.60 +/- 79.67\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 822       |\n",
      "|    mean_reward        | 22.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1304000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000974 |\n",
      "|    explained_variance | 0.958     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65199     |\n",
      "|    policy_loss        | -1.33e-05 |\n",
      "|    value_loss         | 0.0646    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65200    |\n",
      "|    time_elapsed    | 17282    |\n",
      "|    total_timesteps | 1304000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1306000, episode_reward=26.00 +/- 5.22\n",
      "Episode length: 861.40 +/- 118.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 861      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1306000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65299    |\n",
      "|    policy_loss        | -0.00985 |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65300    |\n",
      "|    time_elapsed    | 17295    |\n",
      "|    total_timesteps | 1306000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1308000, episode_reward=20.80 +/- 4.58\n",
      "Episode length: 861.80 +/- 150.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1308000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0624  |\n",
      "|    explained_variance | 0.765    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65399    |\n",
      "|    policy_loss        | 0.000656 |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65400    |\n",
      "|    time_elapsed    | 17308    |\n",
      "|    total_timesteps | 1308000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=22.20 +/- 3.31\n",
      "Episode length: 781.40 +/- 120.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 781      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0828  |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65499    |\n",
      "|    policy_loss        | -0.0314  |\n",
      "|    value_loss         | 0.635    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65500    |\n",
      "|    time_elapsed    | 17320    |\n",
      "|    total_timesteps | 1310000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1312000, episode_reward=20.20 +/- 2.32\n",
      "Episode length: 6007.40 +/- 10496.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1312000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0436  |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65599    |\n",
      "|    policy_loss        | -0.00654 |\n",
      "|    value_loss         | 0.195    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65600    |\n",
      "|    time_elapsed    | 17400    |\n",
      "|    total_timesteps | 1312000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1314000, episode_reward=23.00 +/- 4.86\n",
      "Episode length: 790.20 +/- 52.07\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 790       |\n",
      "|    mean_reward        | 23        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1314000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00543  |\n",
      "|    explained_variance | 0.959     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65699     |\n",
      "|    policy_loss        | -0.000168 |\n",
      "|    value_loss         | 0.0584    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65700    |\n",
      "|    time_elapsed    | 17412    |\n",
      "|    total_timesteps | 1314000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1316000, episode_reward=29.20 +/- 7.57\n",
      "Episode length: 932.40 +/- 144.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 932      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1316000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65799    |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.079    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65800    |\n",
      "|    time_elapsed    | 17425    |\n",
      "|    total_timesteps | 1316000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1318000, episode_reward=25.80 +/- 4.87\n",
      "Episode length: 868.20 +/- 73.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1318000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0403  |\n",
      "|    explained_variance | -0.185   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65899    |\n",
      "|    policy_loss        | 0.00101  |\n",
      "|    value_loss         | 0.712    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 65900    |\n",
      "|    time_elapsed    | 17438    |\n",
      "|    total_timesteps | 1318000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=27.60 +/- 11.36\n",
      "Episode length: 856.20 +/- 95.89\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 856      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0971  |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 65999    |\n",
      "|    policy_loss        | 0.00212  |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66000    |\n",
      "|    time_elapsed    | 17451    |\n",
      "|    total_timesteps | 1320000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1322000, episode_reward=26.00 +/- 10.56\n",
      "Episode length: 944.20 +/- 216.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 944      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1322000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00108 |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | 5.01e-05 |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66100    |\n",
      "|    time_elapsed    | 17463    |\n",
      "|    total_timesteps | 1322000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1324000, episode_reward=29.80 +/- 8.47\n",
      "Episode length: 970.60 +/- 215.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 971      |\n",
      "|    mean_reward        | 29.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1324000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0181  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66199    |\n",
      "|    policy_loss        | 0.000958 |\n",
      "|    value_loss         | 0.036    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66200    |\n",
      "|    time_elapsed    | 17475    |\n",
      "|    total_timesteps | 1324000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1326000, episode_reward=16.20 +/- 10.81\n",
      "Episode length: 11287.60 +/- 12829.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1326000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0906  |\n",
      "|    explained_variance | 0.668    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66299    |\n",
      "|    policy_loss        | 0.0788   |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66300    |\n",
      "|    time_elapsed    | 17555    |\n",
      "|    total_timesteps | 1326000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1328000, episode_reward=18.20 +/- 3.54\n",
      "Episode length: 778.00 +/- 111.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 778      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1328000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0691  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66399    |\n",
      "|    policy_loss        | -0.00903 |\n",
      "|    value_loss         | 0.0562   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 23.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66400    |\n",
      "|    time_elapsed    | 17566    |\n",
      "|    total_timesteps | 1328000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=23.20 +/- 8.21\n",
      "Episode length: 725.80 +/- 102.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 726      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00755 |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | 0.00112  |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 24       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66500    |\n",
      "|    time_elapsed    | 17578    |\n",
      "|    total_timesteps | 1330000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1332000, episode_reward=29.40 +/- 8.45\n",
      "Episode length: 872.20 +/- 156.73\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 872       |\n",
      "|    mean_reward        | 29.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1332000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0117   |\n",
      "|    explained_variance | 0.97      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66599     |\n",
      "|    policy_loss        | -0.000165 |\n",
      "|    value_loss         | 0.0354    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66600    |\n",
      "|    time_elapsed    | 17590    |\n",
      "|    total_timesteps | 1332000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1334000, episode_reward=23.00 +/- 5.02\n",
      "Episode length: 818.80 +/- 100.36\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 819       |\n",
      "|    mean_reward        | 23        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1334000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000132 |\n",
      "|    explained_variance | 0.963     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66699     |\n",
      "|    policy_loss        | -3.12e-07 |\n",
      "|    value_loss         | 0.0391    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66700    |\n",
      "|    time_elapsed    | 17602    |\n",
      "|    total_timesteps | 1334000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1336000, episode_reward=20.20 +/- 5.00\n",
      "Episode length: 817.20 +/- 177.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 817      |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1336000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0563  |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    value_loss         | 0.0632   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66800    |\n",
      "|    time_elapsed    | 17614    |\n",
      "|    total_timesteps | 1336000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1338000, episode_reward=23.80 +/- 6.79\n",
      "Episode length: 825.20 +/- 185.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 825      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1338000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0479  |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 66899    |\n",
      "|    policy_loss        | 0.00395  |\n",
      "|    value_loss         | 0.0334   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 66900    |\n",
      "|    time_elapsed    | 17626    |\n",
      "|    total_timesteps | 1338000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=19.80 +/- 5.56\n",
      "Episode length: 778.40 +/- 117.03\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 778       |\n",
      "|    mean_reward        | 19.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1340000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00678  |\n",
      "|    explained_variance | 0.918     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66999     |\n",
      "|    policy_loss        | -0.000177 |\n",
      "|    value_loss         | 0.102     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67000    |\n",
      "|    time_elapsed    | 17639    |\n",
      "|    total_timesteps | 1340000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1342000, episode_reward=16.80 +/- 3.87\n",
      "Episode length: 720.00 +/- 113.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 720      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1342000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0321  |\n",
      "|    explained_variance | 0.618    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67099    |\n",
      "|    policy_loss        | 0.00537  |\n",
      "|    value_loss         | 0.433    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 67100    |\n",
      "|    time_elapsed    | 17651    |\n",
      "|    total_timesteps | 1342000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1344000, episode_reward=23.00 +/- 7.04\n",
      "Episode length: 831.20 +/- 79.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1344000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0796  |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67199    |\n",
      "|    policy_loss        | 0.00496  |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 67200    |\n",
      "|    time_elapsed    | 17664    |\n",
      "|    total_timesteps | 1344000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1346000, episode_reward=19.80 +/- 5.78\n",
      "Episode length: 807.40 +/- 132.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 807      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1346000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.045   |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67299    |\n",
      "|    policy_loss        | -0.0016  |\n",
      "|    value_loss         | 0.0544   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 67300    |\n",
      "|    time_elapsed    | 17677    |\n",
      "|    total_timesteps | 1346000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1348000, episode_reward=23.00 +/- 8.41\n",
      "Episode length: 869.20 +/- 204.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 869      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1348000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.153   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67399    |\n",
      "|    policy_loss        | -0.00402 |\n",
      "|    value_loss         | 0.0905   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 67400    |\n",
      "|    time_elapsed    | 17688    |\n",
      "|    total_timesteps | 1348000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=21.80 +/- 8.23\n",
      "Episode length: 6031.80 +/- 10484.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0279  |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67499    |\n",
      "|    policy_loss        | 0.00463  |\n",
      "|    value_loss         | 0.0347   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67500    |\n",
      "|    time_elapsed    | 17769    |\n",
      "|    total_timesteps | 1350000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1352000, episode_reward=18.80 +/- 10.50\n",
      "Episode length: 6041.40 +/- 10480.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.04e+03 |\n",
      "|    mean_reward        | 18.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1352000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0222  |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67599    |\n",
      "|    policy_loss        | 7.96e-05 |\n",
      "|    value_loss         | 0.0212   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67600    |\n",
      "|    time_elapsed    | 17851    |\n",
      "|    total_timesteps | 1352000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1354000, episode_reward=25.20 +/- 7.03\n",
      "Episode length: 829.80 +/- 100.33\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 830       |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1354000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0202   |\n",
      "|    explained_variance | 0.86      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 67699     |\n",
      "|    policy_loss        | -0.000504 |\n",
      "|    value_loss         | 0.122     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67700    |\n",
      "|    time_elapsed    | 17862    |\n",
      "|    total_timesteps | 1354000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1356000, episode_reward=27.80 +/- 5.31\n",
      "Episode length: 899.00 +/- 108.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 899      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1356000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0637  |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67800    |\n",
      "|    time_elapsed    | 17874    |\n",
      "|    total_timesteps | 1356000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1358000, episode_reward=17.20 +/- 7.05\n",
      "Episode length: 6010.20 +/- 10495.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1358000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0759  |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67899    |\n",
      "|    policy_loss        | -0.0406  |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 67900    |\n",
      "|    time_elapsed    | 17955    |\n",
      "|    total_timesteps | 1358000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=26.40 +/- 6.34\n",
      "Episode length: 930.20 +/- 146.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 930      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0483  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 67999    |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 0.095    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 24.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68000    |\n",
      "|    time_elapsed    | 17967    |\n",
      "|    total_timesteps | 1360000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1362000, episode_reward=22.60 +/- 4.32\n",
      "Episode length: 743.40 +/- 103.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 743      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1362000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68099    |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68100    |\n",
      "|    time_elapsed    | 17978    |\n",
      "|    total_timesteps | 1362000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1364000, episode_reward=28.40 +/- 7.00\n",
      "Episode length: 915.60 +/- 141.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 916      |\n",
      "|    mean_reward        | 28.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1364000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0729  |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68199    |\n",
      "|    policy_loss        | -0.0474  |\n",
      "|    value_loss         | 0.0625   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68200    |\n",
      "|    time_elapsed    | 17992    |\n",
      "|    total_timesteps | 1364000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1366000, episode_reward=27.40 +/- 5.35\n",
      "Episode length: 908.40 +/- 120.54\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1366000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0625  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68299    |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    value_loss         | 0.0667   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68300    |\n",
      "|    time_elapsed    | 18005    |\n",
      "|    total_timesteps | 1366000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1368000, episode_reward=23.80 +/- 6.85\n",
      "Episode length: 848.00 +/- 138.32\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 848      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1368000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0493  |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68399    |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68400    |\n",
      "|    time_elapsed    | 18017    |\n",
      "|    total_timesteps | 1368000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=24.20 +/- 6.73\n",
      "Episode length: 803.60 +/- 134.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 804      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68499    |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.049    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68500    |\n",
      "|    time_elapsed    | 18030    |\n",
      "|    total_timesteps | 1370000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1372000, episode_reward=25.60 +/- 4.22\n",
      "Episode length: 880.60 +/- 84.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1372000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0256  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68599    |\n",
      "|    policy_loss        | 0.000963 |\n",
      "|    value_loss         | 0.0851   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 68600    |\n",
      "|    time_elapsed    | 18043    |\n",
      "|    total_timesteps | 1372000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1374000, episode_reward=23.00 +/- 4.38\n",
      "Episode length: 6062.20 +/- 10468.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1374000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0389  |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68699    |\n",
      "|    policy_loss        | -0.00408 |\n",
      "|    value_loss         | 0.0722   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68700    |\n",
      "|    time_elapsed    | 18122    |\n",
      "|    total_timesteps | 1374000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1376000, episode_reward=24.80 +/- 7.36\n",
      "Episode length: 924.00 +/- 145.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 924      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1376000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0512  |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68799    |\n",
      "|    policy_loss        | -0.00924 |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68800    |\n",
      "|    time_elapsed    | 18134    |\n",
      "|    total_timesteps | 1376000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1378000, episode_reward=21.00 +/- 7.85\n",
      "Episode length: 828.40 +/- 124.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 828      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1378000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0928  |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68899    |\n",
      "|    policy_loss        | 0.000573 |\n",
      "|    value_loss         | 0.278    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 24.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 68900    |\n",
      "|    time_elapsed    | 18146    |\n",
      "|    total_timesteps | 1378000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=17.60 +/- 6.15\n",
      "Episode length: 715.80 +/- 134.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 716      |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0677  |\n",
      "|    explained_variance | 0.679    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 68999    |\n",
      "|    policy_loss        | 0.00885  |\n",
      "|    value_loss         | 0.288    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69000    |\n",
      "|    time_elapsed    | 18158    |\n",
      "|    total_timesteps | 1380000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1382000, episode_reward=24.60 +/- 7.23\n",
      "Episode length: 6169.40 +/- 10415.95\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.17e+03 |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1382000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0369  |\n",
      "|    explained_variance | 0.6      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69099    |\n",
      "|    policy_loss        | -0.00156 |\n",
      "|    value_loss         | 0.734    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69100    |\n",
      "|    time_elapsed    | 18237    |\n",
      "|    total_timesteps | 1382000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1384000, episode_reward=20.20 +/- 4.53\n",
      "Episode length: 804.00 +/- 129.24\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 804       |\n",
      "|    mean_reward        | 20.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1384000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000841 |\n",
      "|    explained_variance | 0.373     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69199     |\n",
      "|    policy_loss        | 5.99e-05  |\n",
      "|    value_loss         | 0.122     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69200    |\n",
      "|    time_elapsed    | 18248    |\n",
      "|    total_timesteps | 1384000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1386000, episode_reward=25.20 +/- 5.42\n",
      "Episode length: 873.00 +/- 82.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 873      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1386000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0382  |\n",
      "|    explained_variance | 0.674    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69299    |\n",
      "|    policy_loss        | -0.43    |\n",
      "|    value_loss         | 0.815    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69300    |\n",
      "|    time_elapsed    | 18261    |\n",
      "|    total_timesteps | 1386000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1388000, episode_reward=19.00 +/- 3.74\n",
      "Episode length: 6085.20 +/- 10457.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1388000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0416  |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69399    |\n",
      "|    policy_loss        | -0.00435 |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69400    |\n",
      "|    time_elapsed    | 18340    |\n",
      "|    total_timesteps | 1388000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=26.00 +/- 7.01\n",
      "Episode length: 922.80 +/- 155.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 923      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0195  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 0.000842 |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69500    |\n",
      "|    time_elapsed    | 18352    |\n",
      "|    total_timesteps | 1390000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1392000, episode_reward=27.60 +/- 8.82\n",
      "Episode length: 923.40 +/- 106.10\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 923       |\n",
      "|    mean_reward        | 27.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1392000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0289   |\n",
      "|    explained_variance | 0.967     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69599     |\n",
      "|    policy_loss        | -0.000367 |\n",
      "|    value_loss         | 0.0162    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69600    |\n",
      "|    time_elapsed    | 18366    |\n",
      "|    total_timesteps | 1392000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1394000, episode_reward=23.80 +/- 5.60\n",
      "Episode length: 834.40 +/- 138.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 834      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1394000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0749  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69699    |\n",
      "|    policy_loss        | 0.026    |\n",
      "|    value_loss         | 0.141    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69700    |\n",
      "|    time_elapsed    | 18377    |\n",
      "|    total_timesteps | 1394000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1396000, episode_reward=26.60 +/- 9.39\n",
      "Episode length: 891.20 +/- 182.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 891      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1396000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0184  |\n",
      "|    explained_variance | 0.684    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69799    |\n",
      "|    policy_loss        | 0.0014   |\n",
      "|    value_loss         | 0.211    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69800    |\n",
      "|    time_elapsed    | 18390    |\n",
      "|    total_timesteps | 1396000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1398000, episode_reward=20.20 +/- 8.21\n",
      "Episode length: 6081.60 +/- 10460.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1398000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0339  |\n",
      "|    explained_variance | 0.685    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69899    |\n",
      "|    policy_loss        | 0.00567  |\n",
      "|    value_loss         | 2.24     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 69900    |\n",
      "|    time_elapsed    | 18470    |\n",
      "|    total_timesteps | 1398000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=20.00 +/- 11.42\n",
      "Episode length: 6127.40 +/- 10437.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.13e+03 |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0432  |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 69999    |\n",
      "|    policy_loss        | -0.00403 |\n",
      "|    value_loss         | 0.209    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70000    |\n",
      "|    time_elapsed    | 18553    |\n",
      "|    total_timesteps | 1400000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1402000, episode_reward=19.60 +/- 10.89\n",
      "Episode length: 6090.00 +/- 10455.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1402000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0405  |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | -0.00625 |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70100    |\n",
      "|    time_elapsed    | 18634    |\n",
      "|    total_timesteps | 1402000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1404000, episode_reward=22.60 +/- 6.34\n",
      "Episode length: 784.00 +/- 123.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 784      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1404000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0755  |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70199    |\n",
      "|    policy_loss        | -0.00218 |\n",
      "|    value_loss         | 0.0697   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70200    |\n",
      "|    time_elapsed    | 18646    |\n",
      "|    total_timesteps | 1404000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1406000, episode_reward=26.00 +/- 9.10\n",
      "Episode length: 921.60 +/- 156.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 922      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1406000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0137  |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70299    |\n",
      "|    policy_loss        | 0.000339 |\n",
      "|    value_loss         | 0.507    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70300    |\n",
      "|    time_elapsed    | 18659    |\n",
      "|    total_timesteps | 1406000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1408000, episode_reward=23.60 +/- 4.84\n",
      "Episode length: 868.80 +/- 139.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 869      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1408000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0263  |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70399    |\n",
      "|    policy_loss        | -0.00409 |\n",
      "|    value_loss         | 0.0629   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70400    |\n",
      "|    time_elapsed    | 18671    |\n",
      "|    total_timesteps | 1408000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=23.80 +/- 12.37\n",
      "Episode length: 6187.20 +/- 10408.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.19e+03 |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.048   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70499    |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70500    |\n",
      "|    time_elapsed    | 18751    |\n",
      "|    total_timesteps | 1410000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1412000, episode_reward=25.80 +/- 4.49\n",
      "Episode length: 844.20 +/- 51.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 844      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1412000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0631  |\n",
      "|    explained_variance | 0.601    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70599    |\n",
      "|    policy_loss        | -0.00324 |\n",
      "|    value_loss         | 0.19     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70600    |\n",
      "|    time_elapsed    | 18764    |\n",
      "|    total_timesteps | 1412000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1414000, episode_reward=28.60 +/- 4.45\n",
      "Episode length: 942.80 +/- 80.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 943      |\n",
      "|    mean_reward        | 28.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1414000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.096   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70699    |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    value_loss         | 0.0664   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70700    |\n",
      "|    time_elapsed    | 18777    |\n",
      "|    total_timesteps | 1414000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1416000, episode_reward=25.20 +/- 7.47\n",
      "Episode length: 912.40 +/- 172.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 912      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1416000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.038   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70799    |\n",
      "|    policy_loss        | 0.00134  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70800    |\n",
      "|    time_elapsed    | 18789    |\n",
      "|    total_timesteps | 1416000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1418000, episode_reward=21.00 +/- 6.10\n",
      "Episode length: 795.20 +/- 134.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 795      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1418000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0309  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70899    |\n",
      "|    policy_loss        | 0.000167 |\n",
      "|    value_loss         | 0.0498   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 70900    |\n",
      "|    time_elapsed    | 18802    |\n",
      "|    total_timesteps | 1418000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=24.80 +/- 4.58\n",
      "Episode length: 853.80 +/- 97.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 854      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00335 |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 70999    |\n",
      "|    policy_loss        | 0.000156 |\n",
      "|    value_loss         | 0.356    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71000    |\n",
      "|    time_elapsed    | 18814    |\n",
      "|    total_timesteps | 1420000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1422000, episode_reward=24.40 +/- 8.69\n",
      "Episode length: 883.80 +/- 172.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 884      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1422000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0392  |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71099    |\n",
      "|    policy_loss        | 0.0369   |\n",
      "|    value_loss         | 0.338    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71100    |\n",
      "|    time_elapsed    | 18827    |\n",
      "|    total_timesteps | 1422000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1424000, episode_reward=25.80 +/- 5.74\n",
      "Episode length: 6048.00 +/- 10476.50\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.05e+03  |\n",
      "|    mean_reward        | 25.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1424000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00379  |\n",
      "|    explained_variance | 0.932     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71199     |\n",
      "|    policy_loss        | -9.99e-05 |\n",
      "|    value_loss         | 0.061     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71200    |\n",
      "|    time_elapsed    | 18906    |\n",
      "|    total_timesteps | 1424000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1426000, episode_reward=22.20 +/- 2.64\n",
      "Episode length: 858.00 +/- 69.22\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 858       |\n",
      "|    mean_reward        | 22.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1426000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000943 |\n",
      "|    explained_variance | 0.874     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71299     |\n",
      "|    policy_loss        | -8.12e-06 |\n",
      "|    value_loss         | 0.109     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71300    |\n",
      "|    time_elapsed    | 18918    |\n",
      "|    total_timesteps | 1426000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1428000, episode_reward=29.00 +/- 10.20\n",
      "Episode length: 942.40 +/- 213.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 942      |\n",
      "|    mean_reward        | 29       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1428000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00252 |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71399    |\n",
      "|    policy_loss        | 0.000142 |\n",
      "|    value_loss         | 0.0427   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71400    |\n",
      "|    time_elapsed    | 18931    |\n",
      "|    total_timesteps | 1428000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=23.40 +/- 6.22\n",
      "Episode length: 847.00 +/- 99.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 847      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0422  |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71499    |\n",
      "|    policy_loss        | 0.00462  |\n",
      "|    value_loss         | 0.0917   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71500    |\n",
      "|    time_elapsed    | 18943    |\n",
      "|    total_timesteps | 1430000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1432000, episode_reward=27.80 +/- 17.61\n",
      "Episode length: 893.40 +/- 236.65\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 893       |\n",
      "|    mean_reward        | 27.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1432000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00733  |\n",
      "|    explained_variance | 0.992     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71599     |\n",
      "|    policy_loss        | -0.000152 |\n",
      "|    value_loss         | 0.0236    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71600    |\n",
      "|    time_elapsed    | 18955    |\n",
      "|    total_timesteps | 1432000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1434000, episode_reward=25.00 +/- 5.62\n",
      "Episode length: 855.60 +/- 98.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 856      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1434000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0692  |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71699    |\n",
      "|    policy_loss        | -0.0313  |\n",
      "|    value_loss         | 0.0882   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71700    |\n",
      "|    time_elapsed    | 18968    |\n",
      "|    total_timesteps | 1434000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1436000, episode_reward=27.00 +/- 5.93\n",
      "Episode length: 931.00 +/- 129.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 931      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1436000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0513  |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71799    |\n",
      "|    policy_loss        | -0.0334  |\n",
      "|    value_loss         | 0.275    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71800    |\n",
      "|    time_elapsed    | 18982    |\n",
      "|    total_timesteps | 1436000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1438000, episode_reward=25.00 +/- 5.48\n",
      "Episode length: 874.60 +/- 53.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1438000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0588  |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71899    |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    value_loss         | 0.299    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 71900    |\n",
      "|    time_elapsed    | 18995    |\n",
      "|    total_timesteps | 1438000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=25.60 +/- 5.95\n",
      "Episode length: 874.80 +/- 147.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0022  |\n",
      "|    explained_variance | 0.478    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 71999    |\n",
      "|    policy_loss        | 0.000141 |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72000    |\n",
      "|    time_elapsed    | 19007    |\n",
      "|    total_timesteps | 1440000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1442000, episode_reward=24.80 +/- 6.76\n",
      "Episode length: 875.40 +/- 155.21\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 875      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1442000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72099    |\n",
      "|    policy_loss        | -0.00609 |\n",
      "|    value_loss         | 0.252    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72100    |\n",
      "|    time_elapsed    | 19019    |\n",
      "|    total_timesteps | 1442000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1444000, episode_reward=29.60 +/- 6.68\n",
      "Episode length: 950.80 +/- 173.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 951      |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1444000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0766  |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72199    |\n",
      "|    policy_loss        | -0.0101  |\n",
      "|    value_loss         | 0.469    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72200    |\n",
      "|    time_elapsed    | 19031    |\n",
      "|    total_timesteps | 1444000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1446000, episode_reward=22.20 +/- 8.52\n",
      "Episode length: 6104.80 +/- 10448.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1446000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0236  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72299    |\n",
      "|    policy_loss        | -0.00168 |\n",
      "|    value_loss         | 0.182    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72300    |\n",
      "|    time_elapsed    | 19112    |\n",
      "|    total_timesteps | 1446000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1448000, episode_reward=27.20 +/- 5.78\n",
      "Episode length: 964.60 +/- 173.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 965      |\n",
      "|    mean_reward        | 27.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1448000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.254   |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72399    |\n",
      "|    policy_loss        | -0.38    |\n",
      "|    value_loss         | 0.52     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72400    |\n",
      "|    time_elapsed    | 19124    |\n",
      "|    total_timesteps | 1448000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=23.00 +/- 5.55\n",
      "Episode length: 826.60 +/- 129.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 827      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0399  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72499    |\n",
      "|    policy_loss        | -0.00602 |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72500    |\n",
      "|    time_elapsed    | 19137    |\n",
      "|    total_timesteps | 1450000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1452000, episode_reward=19.80 +/- 2.99\n",
      "Episode length: 805.40 +/- 91.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 805      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1452000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.036   |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72599    |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.267    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72600    |\n",
      "|    time_elapsed    | 19150    |\n",
      "|    total_timesteps | 1452000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1454000, episode_reward=33.00 +/- 9.59\n",
      "Episode length: 975.80 +/- 173.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 976      |\n",
      "|    mean_reward        | 33       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1454000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0869  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72699    |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72700    |\n",
      "|    time_elapsed    | 19164    |\n",
      "|    total_timesteps | 1454000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1456000, episode_reward=26.60 +/- 8.89\n",
      "Episode length: 905.60 +/- 166.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1456000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72799    |\n",
      "|    policy_loss        | 0.00616  |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72800    |\n",
      "|    time_elapsed    | 19177    |\n",
      "|    total_timesteps | 1456000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1458000, episode_reward=20.40 +/- 5.78\n",
      "Episode length: 800.00 +/- 134.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 800      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1458000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0105  |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 72899    |\n",
      "|    policy_loss        | 0.000217 |\n",
      "|    value_loss         | 0.0439   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 72900    |\n",
      "|    time_elapsed    | 19190    |\n",
      "|    total_timesteps | 1458000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=17.60 +/- 3.61\n",
      "Episode length: 749.20 +/- 116.50\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 749       |\n",
      "|    mean_reward        | 17.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1460000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0349   |\n",
      "|    explained_variance | 0.979     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72999     |\n",
      "|    policy_loss        | -0.000194 |\n",
      "|    value_loss         | 0.0394    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 76       |\n",
      "|    iterations      | 73000    |\n",
      "|    time_elapsed    | 19202    |\n",
      "|    total_timesteps | 1460000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1462000, episode_reward=21.40 +/- 9.44\n",
      "Episode length: 6092.00 +/- 10455.25\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.09e+03  |\n",
      "|    mean_reward        | 21.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1462000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.61e-05 |\n",
      "|    explained_variance | 0.978     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73099     |\n",
      "|    policy_loss        | -1.53e-07 |\n",
      "|    value_loss         | 0.032     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73100    |\n",
      "|    time_elapsed    | 19284    |\n",
      "|    total_timesteps | 1462000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1464000, episode_reward=26.20 +/- 12.62\n",
      "Episode length: 6184.80 +/- 10408.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.18e+03 |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1464000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73199    |\n",
      "|    policy_loss        | -0.185   |\n",
      "|    value_loss         | 0.188    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73200    |\n",
      "|    time_elapsed    | 19363    |\n",
      "|    total_timesteps | 1464000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1466000, episode_reward=20.40 +/- 3.44\n",
      "Episode length: 824.40 +/- 44.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 824      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1466000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0752  |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73299    |\n",
      "|    policy_loss        | 0.00951  |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73300    |\n",
      "|    time_elapsed    | 19375    |\n",
      "|    total_timesteps | 1466000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1468000, episode_reward=27.60 +/- 8.40\n",
      "Episode length: 907.80 +/- 205.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1468000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0958  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73399    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0922   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73400    |\n",
      "|    time_elapsed    | 19388    |\n",
      "|    total_timesteps | 1468000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=23.20 +/- 7.14\n",
      "Episode length: 837.00 +/- 133.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 837      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0124  |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73499    |\n",
      "|    policy_loss        | 0.00144  |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73500    |\n",
      "|    time_elapsed    | 19400    |\n",
      "|    total_timesteps | 1470000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1472000, episode_reward=21.20 +/- 2.48\n",
      "Episode length: 812.40 +/- 36.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 812      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1472000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0703  |\n",
      "|    explained_variance | 0.744    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73599    |\n",
      "|    policy_loss        | -0.0172  |\n",
      "|    value_loss         | 0.0986   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73600    |\n",
      "|    time_elapsed    | 19412    |\n",
      "|    total_timesteps | 1472000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1474000, episode_reward=21.20 +/- 9.60\n",
      "Episode length: 6116.20 +/- 10442.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1474000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0663  |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73699    |\n",
      "|    policy_loss        | 0.018    |\n",
      "|    value_loss         | 0.0981   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73700    |\n",
      "|    time_elapsed    | 19493    |\n",
      "|    total_timesteps | 1474000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1476000, episode_reward=24.20 +/- 4.92\n",
      "Episode length: 825.80 +/- 125.20\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 826       |\n",
      "|    mean_reward        | 24.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1476000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000215 |\n",
      "|    explained_variance | 0.57      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73799     |\n",
      "|    policy_loss        | -3.59e-06 |\n",
      "|    value_loss         | 0.0896    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73800    |\n",
      "|    time_elapsed    | 19504    |\n",
      "|    total_timesteps | 1476000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1478000, episode_reward=15.00 +/- 8.58\n",
      "Episode length: 6027.20 +/- 10486.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1478000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0345  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73899    |\n",
      "|    policy_loss        | -0.00232 |\n",
      "|    value_loss         | 0.0833   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 73900    |\n",
      "|    time_elapsed    | 19587    |\n",
      "|    total_timesteps | 1478000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=21.80 +/- 5.49\n",
      "Episode length: 788.80 +/- 149.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 789      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0441  |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 73999    |\n",
      "|    policy_loss        | -0.00108 |\n",
      "|    value_loss         | 0.294    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74000    |\n",
      "|    time_elapsed    | 19599    |\n",
      "|    total_timesteps | 1480000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1482000, episode_reward=16.40 +/- 4.32\n",
      "Episode length: 704.20 +/- 100.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 704       |\n",
      "|    mean_reward        | 16.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1482000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0412   |\n",
      "|    explained_variance | 0.953     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74099     |\n",
      "|    policy_loss        | -0.000986 |\n",
      "|    value_loss         | 0.0706    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74100    |\n",
      "|    time_elapsed    | 19611    |\n",
      "|    total_timesteps | 1482000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1484000, episode_reward=23.80 +/- 9.99\n",
      "Episode length: 887.40 +/- 204.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 887      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1484000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74199    |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.24     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74200    |\n",
      "|    time_elapsed    | 19623    |\n",
      "|    total_timesteps | 1484000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1486000, episode_reward=28.80 +/- 8.01\n",
      "Episode length: 925.80 +/- 198.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 926      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1486000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0248  |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74299    |\n",
      "|    policy_loss        | -0.00448 |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74300    |\n",
      "|    time_elapsed    | 19636    |\n",
      "|    total_timesteps | 1486000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1488000, episode_reward=25.80 +/- 8.33\n",
      "Episode length: 6154.00 +/- 10423.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.15e+03 |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1488000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0556  |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | -0.00113 |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74400    |\n",
      "|    time_elapsed    | 19715    |\n",
      "|    total_timesteps | 1488000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=25.00 +/- 7.87\n",
      "Episode length: 793.40 +/- 234.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 793      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0468  |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74499    |\n",
      "|    policy_loss        | -0.0721  |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74500    |\n",
      "|    time_elapsed    | 19729    |\n",
      "|    total_timesteps | 1490000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1492000, episode_reward=24.40 +/- 7.71\n",
      "Episode length: 841.20 +/- 178.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 841      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1492000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0833  |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74599    |\n",
      "|    policy_loss        | 0.00286  |\n",
      "|    value_loss         | 0.238    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74600    |\n",
      "|    time_elapsed    | 19740    |\n",
      "|    total_timesteps | 1492000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1494000, episode_reward=25.60 +/- 8.31\n",
      "Episode length: 905.00 +/- 173.84\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 905       |\n",
      "|    mean_reward        | 25.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1494000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00472  |\n",
      "|    explained_variance | 0.988     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74699     |\n",
      "|    policy_loss        | -2.41e-05 |\n",
      "|    value_loss         | 0.0231    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74700    |\n",
      "|    time_elapsed    | 19754    |\n",
      "|    total_timesteps | 1494000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1496000, episode_reward=25.80 +/- 9.70\n",
      "Episode length: 890.20 +/- 171.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 890       |\n",
      "|    mean_reward        | 25.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1496000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0314   |\n",
      "|    explained_variance | 0.908     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74799     |\n",
      "|    policy_loss        | -0.000941 |\n",
      "|    value_loss         | 0.0775    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74800    |\n",
      "|    time_elapsed    | 19766    |\n",
      "|    total_timesteps | 1496000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1498000, episode_reward=25.20 +/- 8.57\n",
      "Episode length: 868.00 +/- 146.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1498000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0335  |\n",
      "|    explained_variance | 0.812    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74899    |\n",
      "|    policy_loss        | -0.0999  |\n",
      "|    value_loss         | 0.329    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 74900    |\n",
      "|    time_elapsed    | 19779    |\n",
      "|    total_timesteps | 1498000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=21.20 +/- 5.49\n",
      "Episode length: 850.20 +/- 172.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 850      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0456  |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 74999    |\n",
      "|    policy_loss        | 0.00465  |\n",
      "|    value_loss         | 0.0682   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75000    |\n",
      "|    time_elapsed    | 19790    |\n",
      "|    total_timesteps | 1500000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1502000, episode_reward=20.40 +/- 7.55\n",
      "Episode length: 744.00 +/- 142.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 744      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1502000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0663  |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75099    |\n",
      "|    policy_loss        | -0.00222 |\n",
      "|    value_loss         | 0.163    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75100    |\n",
      "|    time_elapsed    | 19802    |\n",
      "|    total_timesteps | 1502000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1504000, episode_reward=27.20 +/- 7.22\n",
      "Episode length: 835.20 +/- 177.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 835      |\n",
      "|    mean_reward        | 27.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1504000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0837  |\n",
      "|    explained_variance | 0.56     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75199    |\n",
      "|    policy_loss        | -0.0061  |\n",
      "|    value_loss         | 0.0837   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75200    |\n",
      "|    time_elapsed    | 19814    |\n",
      "|    total_timesteps | 1504000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1506000, episode_reward=21.60 +/- 2.73\n",
      "Episode length: 766.00 +/- 99.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1506000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0745  |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75299    |\n",
      "|    policy_loss        | -0.00714 |\n",
      "|    value_loss         | 0.245    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75300    |\n",
      "|    time_elapsed    | 19826    |\n",
      "|    total_timesteps | 1506000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1508000, episode_reward=17.00 +/- 9.63\n",
      "Episode length: 6057.60 +/- 10471.72\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 17       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1508000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0975  |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75399    |\n",
      "|    policy_loss        | -0.0106  |\n",
      "|    value_loss         | 0.202    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75400    |\n",
      "|    time_elapsed    | 19907    |\n",
      "|    total_timesteps | 1508000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=26.00 +/- 8.72\n",
      "Episode length: 829.80 +/- 157.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 830      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.038   |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75499    |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 0.263    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75500    |\n",
      "|    time_elapsed    | 19920    |\n",
      "|    total_timesteps | 1510000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1512000, episode_reward=27.60 +/- 8.73\n",
      "Episode length: 954.00 +/- 174.83\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 954       |\n",
      "|    mean_reward        | 27.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1512000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000794 |\n",
      "|    explained_variance | 0.952     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75599     |\n",
      "|    policy_loss        | 9.73e-06  |\n",
      "|    value_loss         | 0.0631    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75600    |\n",
      "|    time_elapsed    | 19933    |\n",
      "|    total_timesteps | 1512000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1514000, episode_reward=30.00 +/- 10.94\n",
      "Episode length: 899.80 +/- 220.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 900      |\n",
      "|    mean_reward        | 30       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1514000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0629  |\n",
      "|    explained_variance | 0.776    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75699    |\n",
      "|    policy_loss        | -0.0112  |\n",
      "|    value_loss         | 0.118    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75700    |\n",
      "|    time_elapsed    | 19945    |\n",
      "|    total_timesteps | 1514000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1516000, episode_reward=19.60 +/- 5.57\n",
      "Episode length: 785.60 +/- 138.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 786      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1516000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0993  |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75799    |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 24.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75800    |\n",
      "|    time_elapsed    | 19956    |\n",
      "|    total_timesteps | 1516000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1518000, episode_reward=23.20 +/- 7.73\n",
      "Episode length: 6074.20 +/- 10463.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1518000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.03    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 75899    |\n",
      "|    policy_loss        | -0.0014  |\n",
      "|    value_loss         | 0.59     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 75900    |\n",
      "|    time_elapsed    | 20036    |\n",
      "|    total_timesteps | 1518000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=23.40 +/- 11.31\n",
      "Episode length: 791.80 +/- 176.54\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 792       |\n",
      "|    mean_reward        | 23.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1520000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.044    |\n",
      "|    explained_variance | 0.959     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75999     |\n",
      "|    policy_loss        | -0.000328 |\n",
      "|    value_loss         | 0.0306    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 24.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76000    |\n",
      "|    time_elapsed    | 20047    |\n",
      "|    total_timesteps | 1520000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1522000, episode_reward=16.00 +/- 6.48\n",
      "Episode length: 5993.80 +/- 10503.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 16       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1522000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0952  |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76099    |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.296    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76100    |\n",
      "|    time_elapsed    | 20128    |\n",
      "|    total_timesteps | 1522000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1524000, episode_reward=29.40 +/- 8.48\n",
      "Episode length: 922.60 +/- 190.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 923      |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1524000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00575 |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76199    |\n",
      "|    policy_loss        | 8.44e-05 |\n",
      "|    value_loss         | 0.0865   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76200    |\n",
      "|    time_elapsed    | 20139    |\n",
      "|    total_timesteps | 1524000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1526000, episode_reward=19.80 +/- 5.19\n",
      "Episode length: 6063.60 +/- 10468.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1526000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0176  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76299    |\n",
      "|    policy_loss        | 0.000186 |\n",
      "|    value_loss         | 0.0676   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76300    |\n",
      "|    time_elapsed    | 20220    |\n",
      "|    total_timesteps | 1526000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1528000, episode_reward=23.20 +/- 5.71\n",
      "Episode length: 839.60 +/- 104.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 840      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1528000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0388  |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76399    |\n",
      "|    policy_loss        | 0.00282  |\n",
      "|    value_loss         | 0.0836   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76400    |\n",
      "|    time_elapsed    | 20231    |\n",
      "|    total_timesteps | 1528000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=20.20 +/- 10.65\n",
      "Episode length: 6082.00 +/- 10459.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0764  |\n",
      "|    explained_variance | 0.649    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76499    |\n",
      "|    policy_loss        | 0.0764   |\n",
      "|    value_loss         | 0.248    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76500    |\n",
      "|    time_elapsed    | 20313    |\n",
      "|    total_timesteps | 1530000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1532000, episode_reward=21.20 +/- 12.17\n",
      "Episode length: 6123.00 +/- 10439.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1532000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.11    |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76599    |\n",
      "|    policy_loss        | 0.0585   |\n",
      "|    value_loss         | 0.0536   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76600    |\n",
      "|    time_elapsed    | 20397    |\n",
      "|    total_timesteps | 1532000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1534000, episode_reward=25.20 +/- 8.84\n",
      "Episode length: 904.40 +/- 238.53\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 904      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1534000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0271  |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76699    |\n",
      "|    policy_loss        | 0.000569 |\n",
      "|    value_loss         | 0.196    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76700    |\n",
      "|    time_elapsed    | 20409    |\n",
      "|    total_timesteps | 1534000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1536000, episode_reward=26.40 +/- 3.93\n",
      "Episode length: 843.00 +/- 48.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 843      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1536000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0876  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76799    |\n",
      "|    policy_loss        | 0.0079   |\n",
      "|    value_loss         | 0.065    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76800    |\n",
      "|    time_elapsed    | 20422    |\n",
      "|    total_timesteps | 1536000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1538000, episode_reward=22.00 +/- 6.84\n",
      "Episode length: 783.60 +/- 182.85\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 784      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1538000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 0.193    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76899    |\n",
      "|    policy_loss        | 0.0494   |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 76900    |\n",
      "|    time_elapsed    | 20433    |\n",
      "|    total_timesteps | 1538000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=28.80 +/- 6.01\n",
      "Episode length: 960.40 +/- 156.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 960      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0798  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 76999    |\n",
      "|    policy_loss        | 0.0304   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 77000    |\n",
      "|    time_elapsed    | 20447    |\n",
      "|    total_timesteps | 1540000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1542000, episode_reward=24.60 +/- 5.20\n",
      "Episode length: 860.00 +/- 184.44\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 860       |\n",
      "|    mean_reward        | 24.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1542000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00475  |\n",
      "|    explained_variance | 0.982     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77099     |\n",
      "|    policy_loss        | -0.000212 |\n",
      "|    value_loss         | 0.103     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 77100    |\n",
      "|    time_elapsed    | 20459    |\n",
      "|    total_timesteps | 1542000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1544000, episode_reward=22.40 +/- 13.09\n",
      "Episode length: 6146.20 +/- 10427.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.15e+03 |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1544000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0735  |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77199    |\n",
      "|    policy_loss        | -0.0553  |\n",
      "|    value_loss         | 0.369    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 77200    |\n",
      "|    time_elapsed    | 20540    |\n",
      "|    total_timesteps | 1544000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1546000, episode_reward=22.40 +/- 9.91\n",
      "Episode length: 6114.40 +/- 10442.87\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.11e+03  |\n",
      "|    mean_reward        | 22.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1546000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000499 |\n",
      "|    explained_variance | 0.937     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77299     |\n",
      "|    policy_loss        | -8.91e-07 |\n",
      "|    value_loss         | 0.0271    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77300    |\n",
      "|    time_elapsed    | 20621    |\n",
      "|    total_timesteps | 1546000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1548000, episode_reward=18.00 +/- 3.16\n",
      "Episode length: 774.00 +/- 63.65\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 774       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1548000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000204 |\n",
      "|    explained_variance | 0.806     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77399     |\n",
      "|    policy_loss        | -1.44e-06 |\n",
      "|    value_loss         | 0.479     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 77400    |\n",
      "|    time_elapsed    | 20632    |\n",
      "|    total_timesteps | 1548000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=20.20 +/- 9.45\n",
      "Episode length: 6119.80 +/- 10440.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0278  |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77499    |\n",
      "|    policy_loss        | 0.000797 |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77500    |\n",
      "|    time_elapsed    | 20713    |\n",
      "|    total_timesteps | 1550000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1552000, episode_reward=18.20 +/- 9.60\n",
      "Episode length: 6095.00 +/- 10452.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1552000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0877  |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | -0.00279 |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77600    |\n",
      "|    time_elapsed    | 20793    |\n",
      "|    total_timesteps | 1552000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1554000, episode_reward=29.20 +/- 8.86\n",
      "Episode length: 905.60 +/- 161.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 906      |\n",
      "|    mean_reward        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1554000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.04    |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 77699    |\n",
      "|    policy_loss        | 0.000959 |\n",
      "|    value_loss         | 0.279    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77700    |\n",
      "|    time_elapsed    | 20807    |\n",
      "|    total_timesteps | 1554000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1556000, episode_reward=28.20 +/- 6.05\n",
      "Episode length: 915.40 +/- 108.25\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 915       |\n",
      "|    mean_reward        | 28.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1556000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.09e-07 |\n",
      "|    explained_variance | 0.906     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77799     |\n",
      "|    policy_loss        | -0        |\n",
      "|    value_loss         | 0.095     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77800    |\n",
      "|    time_elapsed    | 20820    |\n",
      "|    total_timesteps | 1556000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1558000, episode_reward=21.20 +/- 8.23\n",
      "Episode length: 6103.60 +/- 10448.69\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.1e+03   |\n",
      "|    mean_reward        | 21.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1558000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00152  |\n",
      "|    explained_variance | 0.916     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77899     |\n",
      "|    policy_loss        | -1.15e-06 |\n",
      "|    value_loss         | 0.026     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 77900    |\n",
      "|    time_elapsed    | 20901    |\n",
      "|    total_timesteps | 1558000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=30.80 +/- 6.05\n",
      "Episode length: 1011.60 +/- 144.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.01e+03  |\n",
      "|    mean_reward        | 30.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1560000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0149   |\n",
      "|    explained_variance | 0.971     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77999     |\n",
      "|    policy_loss        | -0.000162 |\n",
      "|    value_loss         | 0.0384    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78000    |\n",
      "|    time_elapsed    | 20914    |\n",
      "|    total_timesteps | 1560000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1562000, episode_reward=23.80 +/- 6.97\n",
      "Episode length: 835.60 +/- 150.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1562000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0314  |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | 0.00134  |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78100    |\n",
      "|    time_elapsed    | 20926    |\n",
      "|    total_timesteps | 1562000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1564000, episode_reward=25.20 +/- 8.45\n",
      "Episode length: 883.60 +/- 171.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 884      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1564000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78199    |\n",
      "|    policy_loss        | -0.00751 |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78200    |\n",
      "|    time_elapsed    | 20938    |\n",
      "|    total_timesteps | 1564000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1566000, episode_reward=23.20 +/- 7.39\n",
      "Episode length: 881.60 +/- 121.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 882      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1566000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0426  |\n",
      "|    explained_variance | 0.679    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | 0.00182  |\n",
      "|    value_loss         | 0.0855   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78300    |\n",
      "|    time_elapsed    | 20951    |\n",
      "|    total_timesteps | 1566000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1568000, episode_reward=20.80 +/- 3.87\n",
      "Episode length: 802.00 +/- 97.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 802      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1568000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.158   |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78399    |\n",
      "|    policy_loss        | -0.00408 |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78400    |\n",
      "|    time_elapsed    | 20963    |\n",
      "|    total_timesteps | 1568000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=26.40 +/- 5.71\n",
      "Episode length: 883.80 +/- 130.89\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 884       |\n",
      "|    mean_reward        | 26.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1570000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000152 |\n",
      "|    explained_variance | 0.759     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78499     |\n",
      "|    policy_loss        | -1.47e-06 |\n",
      "|    value_loss         | 0.734     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78500    |\n",
      "|    time_elapsed    | 20976    |\n",
      "|    total_timesteps | 1570000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1572000, episode_reward=21.80 +/- 3.71\n",
      "Episode length: 787.00 +/- 76.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1572000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0988  |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78599    |\n",
      "|    policy_loss        | -0.0406  |\n",
      "|    value_loss         | 0.593    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78600    |\n",
      "|    time_elapsed    | 20988    |\n",
      "|    total_timesteps | 1572000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1574000, episode_reward=21.80 +/- 7.83\n",
      "Episode length: 861.40 +/- 156.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 861      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1574000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0518  |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78699    |\n",
      "|    policy_loss        | 0.00351  |\n",
      "|    value_loss         | 0.0419   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78700    |\n",
      "|    time_elapsed    | 21001    |\n",
      "|    total_timesteps | 1574000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1576000, episode_reward=22.40 +/- 4.59\n",
      "Episode length: 892.00 +/- 87.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 892      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1576000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0359  |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78799    |\n",
      "|    policy_loss        | -0.00284 |\n",
      "|    value_loss         | 0.0839   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 78800    |\n",
      "|    time_elapsed    | 21014    |\n",
      "|    total_timesteps | 1576000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1578000, episode_reward=20.60 +/- 2.42\n",
      "Episode length: 778.80 +/- 53.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 779      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1578000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0309  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78899    |\n",
      "|    policy_loss        | 0.00574  |\n",
      "|    value_loss         | 0.174    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 78900    |\n",
      "|    time_elapsed    | 21026    |\n",
      "|    total_timesteps | 1578000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=24.60 +/- 5.95\n",
      "Episode length: 6120.60 +/- 10440.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.124   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 78999    |\n",
      "|    policy_loss        | -0.0336  |\n",
      "|    value_loss         | 0.0822   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 79000    |\n",
      "|    time_elapsed    | 21105    |\n",
      "|    total_timesteps | 1580000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1582000, episode_reward=17.40 +/- 4.50\n",
      "Episode length: 733.60 +/- 88.05\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 734       |\n",
      "|    mean_reward        | 17.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1582000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00816  |\n",
      "|    explained_variance | 0.947     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79099     |\n",
      "|    policy_loss        | -2.34e-05 |\n",
      "|    value_loss         | 0.0539    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 79100    |\n",
      "|    time_elapsed    | 21116    |\n",
      "|    total_timesteps | 1582000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1584000, episode_reward=33.40 +/- 15.32\n",
      "Episode length: 970.80 +/- 227.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 971      |\n",
      "|    mean_reward        | 33.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1584000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.105   |\n",
      "|    explained_variance | -0.0445  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79199    |\n",
      "|    policy_loss        | 0.00134  |\n",
      "|    value_loss         | 1.82     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 79200    |\n",
      "|    time_elapsed    | 21129    |\n",
      "|    total_timesteps | 1584000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1586000, episode_reward=21.80 +/- 4.26\n",
      "Episode length: 734.00 +/- 61.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1586000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0639  |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79300    |\n",
      "|    time_elapsed    | 21141    |\n",
      "|    total_timesteps | 1586000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1588000, episode_reward=23.00 +/- 7.80\n",
      "Episode length: 827.00 +/- 143.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 827      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1588000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0496  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79399    |\n",
      "|    policy_loss        | -0.204   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79400    |\n",
      "|    time_elapsed    | 21154    |\n",
      "|    total_timesteps | 1588000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=28.00 +/- 13.18\n",
      "Episode length: 884.00 +/- 187.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 884      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0308  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79499    |\n",
      "|    policy_loss        | -0.00164 |\n",
      "|    value_loss         | 0.0308   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79500    |\n",
      "|    time_elapsed    | 21167    |\n",
      "|    total_timesteps | 1590000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1592000, episode_reward=29.20 +/- 5.56\n",
      "Episode length: 900.20 +/- 120.88\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 900       |\n",
      "|    mean_reward        | 29.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1592000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000284 |\n",
      "|    explained_variance | 0.932     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79599     |\n",
      "|    policy_loss        | 2.13e-06  |\n",
      "|    value_loss         | 0.127     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79600    |\n",
      "|    time_elapsed    | 21180    |\n",
      "|    total_timesteps | 1592000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1594000, episode_reward=25.20 +/- 6.49\n",
      "Episode length: 833.20 +/- 181.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 833      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1594000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00668 |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79699    |\n",
      "|    policy_loss        | 2.82e-05 |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79700    |\n",
      "|    time_elapsed    | 21192    |\n",
      "|    total_timesteps | 1594000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1596000, episode_reward=30.60 +/- 12.39\n",
      "Episode length: 983.80 +/- 208.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 984      |\n",
      "|    mean_reward        | 30.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1596000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0117  |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 79799    |\n",
      "|    policy_loss        | 0.000134 |\n",
      "|    value_loss         | 0.0807   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79800    |\n",
      "|    time_elapsed    | 21204    |\n",
      "|    total_timesteps | 1596000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1598000, episode_reward=26.20 +/- 6.97\n",
      "Episode length: 875.40 +/- 103.33\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 875       |\n",
      "|    mean_reward        | 26.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1598000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0161   |\n",
      "|    explained_variance | 0.976     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79899     |\n",
      "|    policy_loss        | -0.000161 |\n",
      "|    value_loss         | 0.0254    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 79900    |\n",
      "|    time_elapsed    | 21218    |\n",
      "|    total_timesteps | 1598000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=25.20 +/- 12.51\n",
      "Episode length: 924.40 +/- 280.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 924       |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1600000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00276  |\n",
      "|    explained_variance | 0.951     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79999     |\n",
      "|    policy_loss        | -4.04e-05 |\n",
      "|    value_loss         | 0.105     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80000    |\n",
      "|    time_elapsed    | 21231    |\n",
      "|    total_timesteps | 1600000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1602000, episode_reward=25.20 +/- 7.05\n",
      "Episode length: 865.40 +/- 90.96\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 865       |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1602000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.88e-05 |\n",
      "|    explained_variance | 0.988     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80099     |\n",
      "|    policy_loss        | -1.44e-07 |\n",
      "|    value_loss         | 0.0612    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 28.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80100    |\n",
      "|    time_elapsed    | 21244    |\n",
      "|    total_timesteps | 1602000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1604000, episode_reward=24.20 +/- 14.32\n",
      "Episode length: 6195.60 +/- 10403.18\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.2e+03  |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1604000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00447 |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80199    |\n",
      "|    policy_loss        | 0.000753 |\n",
      "|    value_loss         | 0.407    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 28.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80200    |\n",
      "|    time_elapsed    | 21325    |\n",
      "|    total_timesteps | 1604000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1606000, episode_reward=28.40 +/- 15.83\n",
      "Episode length: 887.60 +/- 213.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 888      |\n",
      "|    mean_reward        | 28.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1606000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0463  |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80299    |\n",
      "|    policy_loss        | 0.23     |\n",
      "|    value_loss         | 0.218    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 28.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80300    |\n",
      "|    time_elapsed    | 21337    |\n",
      "|    total_timesteps | 1606000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1608000, episode_reward=32.40 +/- 8.73\n",
      "Episode length: 1009.00 +/- 213.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 32.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1608000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0594  |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80399    |\n",
      "|    policy_loss        | 0.0044   |\n",
      "|    value_loss         | 0.0778   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 28.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80400    |\n",
      "|    time_elapsed    | 21351    |\n",
      "|    total_timesteps | 1608000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=28.80 +/- 7.52\n",
      "Episode length: 990.80 +/- 166.62\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 991      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0636  |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80499    |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 946      |\n",
      "|    ep_rew_mean     | 28.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80500    |\n",
      "|    time_elapsed    | 21363    |\n",
      "|    total_timesteps | 1610000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1612000, episode_reward=23.20 +/- 5.71\n",
      "Episode length: 839.20 +/- 91.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 839      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1612000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00215 |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80599    |\n",
      "|    policy_loss        | 0.000103 |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 946      |\n",
      "|    ep_rew_mean     | 28.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80600    |\n",
      "|    time_elapsed    | 21375    |\n",
      "|    total_timesteps | 1612000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1614000, episode_reward=28.60 +/- 8.55\n",
      "Episode length: 950.60 +/- 193.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 951      |\n",
      "|    mean_reward        | 28.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1614000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0414  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80699    |\n",
      "|    policy_loss        | -0.0005  |\n",
      "|    value_loss         | 0.0608   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 28.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80700    |\n",
      "|    time_elapsed    | 21387    |\n",
      "|    total_timesteps | 1614000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1616000, episode_reward=21.60 +/- 6.50\n",
      "Episode length: 6060.80 +/- 10469.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1616000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00163 |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80799    |\n",
      "|    policy_loss        | 6.09e-05 |\n",
      "|    value_loss         | 0.0819   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 28.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80800    |\n",
      "|    time_elapsed    | 21467    |\n",
      "|    total_timesteps | 1616000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1618000, episode_reward=23.20 +/- 4.87\n",
      "Episode length: 869.80 +/- 189.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 870      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1618000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0252  |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80899    |\n",
      "|    policy_loss        | 0.00162  |\n",
      "|    value_loss         | 0.0515   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 28.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 80900    |\n",
      "|    time_elapsed    | 21479    |\n",
      "|    total_timesteps | 1618000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=26.60 +/- 7.39\n",
      "Episode length: 6150.20 +/- 10426.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.15e+03 |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 80999    |\n",
      "|    policy_loss        | 0.108    |\n",
      "|    value_loss         | 0.273    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81000    |\n",
      "|    time_elapsed    | 21558    |\n",
      "|    total_timesteps | 1620000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1622000, episode_reward=25.00 +/- 7.51\n",
      "Episode length: 928.40 +/- 159.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 928      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1622000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0546  |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81099    |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81100    |\n",
      "|    time_elapsed    | 21569    |\n",
      "|    total_timesteps | 1622000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1624000, episode_reward=21.80 +/- 6.21\n",
      "Episode length: 878.60 +/- 87.75\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 879      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1624000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.013   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81199    |\n",
      "|    policy_loss        | 0.000331 |\n",
      "|    value_loss         | 0.424    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | 28.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81200    |\n",
      "|    time_elapsed    | 21581    |\n",
      "|    total_timesteps | 1624000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1626000, episode_reward=23.80 +/- 5.04\n",
      "Episode length: 6097.40 +/- 10452.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1626000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0753  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81299    |\n",
      "|    policy_loss        | 0.000754 |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81300    |\n",
      "|    time_elapsed    | 21659    |\n",
      "|    total_timesteps | 1626000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1628000, episode_reward=34.80 +/- 10.78\n",
      "Episode length: 1045.20 +/- 181.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.05e+03 |\n",
      "|    mean_reward        | 34.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1628000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0318  |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81399    |\n",
      "|    policy_loss        | 0.000696 |\n",
      "|    value_loss         | 0.143    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81400    |\n",
      "|    time_elapsed    | 21671    |\n",
      "|    total_timesteps | 1628000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=26.40 +/- 3.88\n",
      "Episode length: 917.20 +/- 73.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 917      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00663 |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81499    |\n",
      "|    policy_loss        | 2.48e-05 |\n",
      "|    value_loss         | 0.0492   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81500    |\n",
      "|    time_elapsed    | 21684    |\n",
      "|    total_timesteps | 1630000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1632000, episode_reward=24.20 +/- 8.26\n",
      "Episode length: 840.80 +/- 179.78\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 841       |\n",
      "|    mean_reward        | 24.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1632000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000216 |\n",
      "|    explained_variance | 0.979     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81599     |\n",
      "|    policy_loss        | -4.18e-06 |\n",
      "|    value_loss         | 0.0335    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81600    |\n",
      "|    time_elapsed    | 21696    |\n",
      "|    total_timesteps | 1632000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1634000, episode_reward=25.00 +/- 7.01\n",
      "Episode length: 856.80 +/- 152.57\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 857      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1634000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0751  |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81699    |\n",
      "|    policy_loss        | 0.00861  |\n",
      "|    value_loss         | 0.0579   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81700    |\n",
      "|    time_elapsed    | 21709    |\n",
      "|    total_timesteps | 1634000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1636000, episode_reward=25.20 +/- 6.73\n",
      "Episode length: 6076.80 +/- 10462.64\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.08e+03  |\n",
      "|    mean_reward        | 25.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1636000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000625 |\n",
      "|    explained_variance | 0.944     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81799     |\n",
      "|    policy_loss        | -1.91e-05 |\n",
      "|    value_loss         | 0.0861    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81800    |\n",
      "|    time_elapsed    | 21788    |\n",
      "|    total_timesteps | 1636000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1638000, episode_reward=23.60 +/- 4.32\n",
      "Episode length: 830.80 +/- 92.27\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1638000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0295  |\n",
      "|    explained_variance | -0.0219  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81899    |\n",
      "|    policy_loss        | 0.00115  |\n",
      "|    value_loss         | 0.765    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 81900    |\n",
      "|    time_elapsed    | 21800    |\n",
      "|    total_timesteps | 1638000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=22.60 +/- 3.98\n",
      "Episode length: 821.40 +/- 115.25\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 821      |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0744  |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 81999    |\n",
      "|    policy_loss        | 0.00255  |\n",
      "|    value_loss         | 0.426    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82000    |\n",
      "|    time_elapsed    | 21813    |\n",
      "|    total_timesteps | 1640000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1642000, episode_reward=28.80 +/- 16.14\n",
      "Episode length: 907.80 +/- 233.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1642000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.228   |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82099    |\n",
      "|    policy_loss        | -0.258   |\n",
      "|    value_loss         | 0.338    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82100    |\n",
      "|    time_elapsed    | 21825    |\n",
      "|    total_timesteps | 1642000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1644000, episode_reward=22.00 +/- 4.24\n",
      "Episode length: 820.80 +/- 80.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 821      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1644000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82199    |\n",
      "|    policy_loss        | -0.00454 |\n",
      "|    value_loss         | 0.23     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82200    |\n",
      "|    time_elapsed    | 21838    |\n",
      "|    total_timesteps | 1644000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1646000, episode_reward=21.60 +/- 6.95\n",
      "Episode length: 6090.60 +/- 10454.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1646000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.105   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82299    |\n",
      "|    policy_loss        | 0.037    |\n",
      "|    value_loss         | 0.184    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82300    |\n",
      "|    time_elapsed    | 21918    |\n",
      "|    total_timesteps | 1646000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1648000, episode_reward=27.40 +/- 7.28\n",
      "Episode length: 907.80 +/- 151.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 908      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1648000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0787  |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82399    |\n",
      "|    policy_loss        | -0.00772 |\n",
      "|    value_loss         | 0.205    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82400    |\n",
      "|    time_elapsed    | 21929    |\n",
      "|    total_timesteps | 1648000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=26.00 +/- 8.88\n",
      "Episode length: 897.20 +/- 160.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 897      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0627  |\n",
      "|    explained_variance | 0.679    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82499    |\n",
      "|    policy_loss        | -0.00274 |\n",
      "|    value_loss         | 0.578    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82500    |\n",
      "|    time_elapsed    | 21942    |\n",
      "|    total_timesteps | 1650000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1652000, episode_reward=28.80 +/- 4.87\n",
      "Episode length: 836.00 +/- 53.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 836      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1652000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00944 |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82599    |\n",
      "|    policy_loss        | 0.00039  |\n",
      "|    value_loss         | 0.0358   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82600    |\n",
      "|    time_elapsed    | 21954    |\n",
      "|    total_timesteps | 1652000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1654000, episode_reward=22.00 +/- 4.56\n",
      "Episode length: 804.40 +/- 94.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 804      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1654000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0408  |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82699    |\n",
      "|    policy_loss        | 0.00438  |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82700    |\n",
      "|    time_elapsed    | 21967    |\n",
      "|    total_timesteps | 1654000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1656000, episode_reward=27.00 +/- 4.05\n",
      "Episode length: 972.20 +/- 96.12\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 972      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1656000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0703  |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82799    |\n",
      "|    policy_loss        | -0.0692  |\n",
      "|    value_loss         | 0.243    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82800    |\n",
      "|    time_elapsed    | 21980    |\n",
      "|    total_timesteps | 1656000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1658000, episode_reward=25.20 +/- 9.54\n",
      "Episode length: 890.60 +/- 128.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 891      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1658000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0193  |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82899    |\n",
      "|    policy_loss        | 0.00238  |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 82900    |\n",
      "|    time_elapsed    | 21992    |\n",
      "|    total_timesteps | 1658000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=25.80 +/- 7.88\n",
      "Episode length: 913.20 +/- 123.78\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 913      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.143   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 82999    |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.067    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83000    |\n",
      "|    time_elapsed    | 22005    |\n",
      "|    total_timesteps | 1660000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1662000, episode_reward=24.80 +/- 7.44\n",
      "Episode length: 865.00 +/- 186.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 865      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1662000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83099    |\n",
      "|    policy_loss        | -0.0955  |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83100    |\n",
      "|    time_elapsed    | 22017    |\n",
      "|    total_timesteps | 1662000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1664000, episode_reward=24.80 +/- 9.68\n",
      "Episode length: 806.00 +/- 135.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 806      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1664000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0118  |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83199    |\n",
      "|    policy_loss        | 1.66e-05 |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83200    |\n",
      "|    time_elapsed    | 22029    |\n",
      "|    total_timesteps | 1664000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1666000, episode_reward=25.00 +/- 7.40\n",
      "Episode length: 895.60 +/- 105.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 896      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1666000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0994  |\n",
      "|    explained_variance | 0.112    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83299    |\n",
      "|    policy_loss        | 0.0461   |\n",
      "|    value_loss         | 0.475    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83300    |\n",
      "|    time_elapsed    | 22041    |\n",
      "|    total_timesteps | 1666000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1668000, episode_reward=24.00 +/- 5.18\n",
      "Episode length: 876.60 +/- 129.83\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 877      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1668000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0903  |\n",
      "|    explained_variance | 0.434    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83399    |\n",
      "|    policy_loss        | 0.144    |\n",
      "|    value_loss         | 0.346    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83400    |\n",
      "|    time_elapsed    | 22054    |\n",
      "|    total_timesteps | 1668000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=24.80 +/- 7.44\n",
      "Episode length: 839.80 +/- 175.99\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 840       |\n",
      "|    mean_reward        | 24.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1670000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.123    |\n",
      "|    explained_variance | 0.954     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83499     |\n",
      "|    policy_loss        | -0.000735 |\n",
      "|    value_loss         | 0.166     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83500    |\n",
      "|    time_elapsed    | 22067    |\n",
      "|    total_timesteps | 1670000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1672000, episode_reward=29.40 +/- 8.73\n",
      "Episode length: 929.00 +/- 119.62\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 929       |\n",
      "|    mean_reward        | 29.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1672000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00109  |\n",
      "|    explained_variance | 0.802     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83599     |\n",
      "|    policy_loss        | -2.28e-05 |\n",
      "|    value_loss         | 0.105     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83600    |\n",
      "|    time_elapsed    | 22079    |\n",
      "|    total_timesteps | 1672000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1674000, episode_reward=24.80 +/- 6.05\n",
      "Episode length: 865.20 +/- 112.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 865      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1674000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0565  |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83699    |\n",
      "|    policy_loss        | 0.00287  |\n",
      "|    value_loss         | 0.0396   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83700    |\n",
      "|    time_elapsed    | 22092    |\n",
      "|    total_timesteps | 1674000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1676000, episode_reward=19.40 +/- 6.62\n",
      "Episode length: 6105.00 +/- 10448.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1676000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0867  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83799    |\n",
      "|    policy_loss        | 0.00295  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83800    |\n",
      "|    time_elapsed    | 22172    |\n",
      "|    total_timesteps | 1676000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1678000, episode_reward=37.20 +/- 9.02\n",
      "Episode length: 1109.80 +/- 169.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.11e+03 |\n",
      "|    mean_reward        | 37.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1678000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00434 |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83899    |\n",
      "|    policy_loss        | 9.06e-05 |\n",
      "|    value_loss         | 0.0923   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 83900    |\n",
      "|    time_elapsed    | 22185    |\n",
      "|    total_timesteps | 1678000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=20.60 +/- 3.32\n",
      "Episode length: 798.20 +/- 73.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 20.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0402  |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 83999    |\n",
      "|    policy_loss        | -0.00144 |\n",
      "|    value_loss         | 0.384    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84000    |\n",
      "|    time_elapsed    | 22197    |\n",
      "|    total_timesteps | 1680000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1682000, episode_reward=30.60 +/- 6.68\n",
      "Episode length: 988.00 +/- 186.76\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 988      |\n",
      "|    mean_reward        | 30.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1682000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0574  |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84099    |\n",
      "|    policy_loss        | 0.00784  |\n",
      "|    value_loss         | 0.0651   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84100    |\n",
      "|    time_elapsed    | 22209    |\n",
      "|    total_timesteps | 1682000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1684000, episode_reward=23.60 +/- 7.84\n",
      "Episode length: 862.20 +/- 133.67\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 862      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1684000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0731  |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84199    |\n",
      "|    policy_loss        | 0.00977  |\n",
      "|    value_loss         | 0.0795   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84200    |\n",
      "|    time_elapsed    | 22222    |\n",
      "|    total_timesteps | 1684000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1686000, episode_reward=27.40 +/- 7.63\n",
      "Episode length: 885.00 +/- 88.60\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 885      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1686000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0104  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84299    |\n",
      "|    policy_loss        | 0.000179 |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84300    |\n",
      "|    time_elapsed    | 22235    |\n",
      "|    total_timesteps | 1686000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1688000, episode_reward=17.60 +/- 9.35\n",
      "Episode length: 6060.00 +/- 10470.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 17.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1688000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0626  |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84399    |\n",
      "|    policy_loss        | 0.00296  |\n",
      "|    value_loss         | 0.147    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84400    |\n",
      "|    time_elapsed    | 22315    |\n",
      "|    total_timesteps | 1688000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=28.20 +/- 16.96\n",
      "Episode length: 6218.00 +/- 10393.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.22e+03 |\n",
      "|    mean_reward        | 28.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00605 |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84499    |\n",
      "|    policy_loss        | 0.00119  |\n",
      "|    value_loss         | 0.29     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84500    |\n",
      "|    time_elapsed    | 22399    |\n",
      "|    total_timesteps | 1690000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1692000, episode_reward=20.20 +/- 5.74\n",
      "Episode length: 5994.60 +/- 10502.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1692000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0451  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84599    |\n",
      "|    policy_loss        | 0.000456 |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84600    |\n",
      "|    time_elapsed    | 22479    |\n",
      "|    total_timesteps | 1692000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1694000, episode_reward=19.60 +/- 10.86\n",
      "Episode length: 6067.00 +/- 10467.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1694000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0153  |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84699    |\n",
      "|    policy_loss        | -0.00031 |\n",
      "|    value_loss         | 0.302    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84700    |\n",
      "|    time_elapsed    | 22560    |\n",
      "|    total_timesteps | 1694000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1696000, episode_reward=21.20 +/- 6.62\n",
      "Episode length: 749.20 +/- 108.68\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 749      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1696000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0475  |\n",
      "|    explained_variance | 0.334    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84799    |\n",
      "|    policy_loss        | 0.00753  |\n",
      "|    value_loss         | 0.429    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84800    |\n",
      "|    time_elapsed    | 22572    |\n",
      "|    total_timesteps | 1696000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1698000, episode_reward=25.80 +/- 9.28\n",
      "Episode length: 897.40 +/- 254.05\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 897      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1698000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.088   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84899    |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 84900    |\n",
      "|    time_elapsed    | 22584    |\n",
      "|    total_timesteps | 1698000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=21.00 +/- 11.30\n",
      "Episode length: 6096.40 +/- 10453.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0829  |\n",
      "|    explained_variance | 0.447    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 84999    |\n",
      "|    policy_loss        | -0.00061 |\n",
      "|    value_loss         | 0.474    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 85000    |\n",
      "|    time_elapsed    | 22665    |\n",
      "|    total_timesteps | 1700000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1702000, episode_reward=26.20 +/- 12.09\n",
      "Episode length: 858.40 +/- 214.89\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 858       |\n",
      "|    mean_reward        | 26.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1702000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.024    |\n",
      "|    explained_variance | 0.979     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85099     |\n",
      "|    policy_loss        | -0.000611 |\n",
      "|    value_loss         | 0.0313    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 75       |\n",
      "|    iterations      | 85100    |\n",
      "|    time_elapsed    | 22676    |\n",
      "|    total_timesteps | 1702000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1704000, episode_reward=28.20 +/- 11.89\n",
      "Episode length: 6206.00 +/- 10399.61\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.21e+03  |\n",
      "|    mean_reward        | 28.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1704000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0429   |\n",
      "|    explained_variance | 0.991     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85199     |\n",
      "|    policy_loss        | -0.000409 |\n",
      "|    value_loss         | 0.0247    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85200    |\n",
      "|    time_elapsed    | 22756    |\n",
      "|    total_timesteps | 1704000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1706000, episode_reward=17.80 +/- 3.97\n",
      "Episode length: 771.80 +/- 115.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 772       |\n",
      "|    mean_reward        | 17.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1706000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000612 |\n",
      "|    explained_variance | 0.964     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85299     |\n",
      "|    policy_loss        | -3.15e-06 |\n",
      "|    value_loss         | 0.0539    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85300    |\n",
      "|    time_elapsed    | 22768    |\n",
      "|    total_timesteps | 1706000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1708000, episode_reward=26.00 +/- 9.01\n",
      "Episode length: 6144.40 +/- 10428.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.14e+03 |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1708000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0821  |\n",
      "|    explained_variance | 0.848    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85399    |\n",
      "|    policy_loss        | 0.00567  |\n",
      "|    value_loss         | 0.0623   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 28.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85400    |\n",
      "|    time_elapsed    | 22848    |\n",
      "|    total_timesteps | 1708000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=16.80 +/- 12.42\n",
      "Episode length: 5989.60 +/- 10505.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.99e+03 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0324  |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85499    |\n",
      "|    policy_loss        | 0.00782  |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85500    |\n",
      "|    time_elapsed    | 22932    |\n",
      "|    total_timesteps | 1710000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1712000, episode_reward=21.40 +/- 5.43\n",
      "Episode length: 845.00 +/- 99.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1712000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0815  |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85599    |\n",
      "|    policy_loss        | -0.176   |\n",
      "|    value_loss         | 0.346    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85600    |\n",
      "|    time_elapsed    | 22944    |\n",
      "|    total_timesteps | 1712000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1714000, episode_reward=15.00 +/- 12.44\n",
      "Episode length: 11253.80 +/- 12856.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 15       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1714000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0301  |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85699    |\n",
      "|    policy_loss        | 3.79e-06 |\n",
      "|    value_loss         | 0.314    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85700    |\n",
      "|    time_elapsed    | 23024    |\n",
      "|    total_timesteps | 1714000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1716000, episode_reward=28.00 +/- 7.82\n",
      "Episode length: 816.00 +/- 97.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 816      |\n",
      "|    mean_reward        | 28       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1716000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0215  |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85799    |\n",
      "|    policy_loss        | -0.00317 |\n",
      "|    value_loss         | 0.531    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85800    |\n",
      "|    time_elapsed    | 23035    |\n",
      "|    total_timesteps | 1716000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1718000, episode_reward=19.80 +/- 5.46\n",
      "Episode length: 754.40 +/- 131.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 754      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1718000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.511    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85899    |\n",
      "|    policy_loss        | 0.0307   |\n",
      "|    value_loss         | 0.355    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 85900    |\n",
      "|    time_elapsed    | 23048    |\n",
      "|    total_timesteps | 1718000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=21.80 +/- 7.41\n",
      "Episode length: 852.80 +/- 182.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 853      |\n",
      "|    mean_reward        | 21.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0932  |\n",
      "|    explained_variance | 0.66     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 85999    |\n",
      "|    policy_loss        | 0.035    |\n",
      "|    value_loss         | 0.145    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86000    |\n",
      "|    time_elapsed    | 23060    |\n",
      "|    total_timesteps | 1720000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1722000, episode_reward=21.00 +/- 4.98\n",
      "Episode length: 833.00 +/- 130.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 833      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1722000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86099    |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86100    |\n",
      "|    time_elapsed    | 23073    |\n",
      "|    total_timesteps | 1722000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1724000, episode_reward=23.60 +/- 3.93\n",
      "Episode length: 844.40 +/- 144.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 844      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1724000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.122   |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86199    |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.499    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86200    |\n",
      "|    time_elapsed    | 23085    |\n",
      "|    total_timesteps | 1724000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1726000, episode_reward=27.00 +/- 9.94\n",
      "Episode length: 899.20 +/- 187.71\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 899      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1726000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86299    |\n",
      "|    policy_loss        | -0.0789  |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86300    |\n",
      "|    time_elapsed    | 23099    |\n",
      "|    total_timesteps | 1726000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1728000, episode_reward=26.60 +/- 3.67\n",
      "Episode length: 845.40 +/- 101.90\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1728000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86399    |\n",
      "|    policy_loss        | 0.0722   |\n",
      "|    value_loss         | 0.0815   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86400    |\n",
      "|    time_elapsed    | 23111    |\n",
      "|    total_timesteps | 1728000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=22.40 +/- 7.06\n",
      "Episode length: 870.40 +/- 188.92\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 870       |\n",
      "|    mean_reward        | 22.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1730000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.019    |\n",
      "|    explained_variance | 0.956     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86499     |\n",
      "|    policy_loss        | -0.000379 |\n",
      "|    value_loss         | 0.052     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86500    |\n",
      "|    time_elapsed    | 23123    |\n",
      "|    total_timesteps | 1730000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1732000, episode_reward=23.60 +/- 4.80\n",
      "Episode length: 897.60 +/- 127.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 898      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1732000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0327  |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86599    |\n",
      "|    policy_loss        | 0.00609  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86600    |\n",
      "|    time_elapsed    | 23137    |\n",
      "|    total_timesteps | 1732000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1734000, episode_reward=27.60 +/- 5.12\n",
      "Episode length: 881.20 +/- 66.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 881      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1734000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86699    |\n",
      "|    policy_loss        | -0.0591  |\n",
      "|    value_loss         | 0.0491   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86700    |\n",
      "|    time_elapsed    | 23150    |\n",
      "|    total_timesteps | 1734000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1736000, episode_reward=21.40 +/- 6.09\n",
      "Episode length: 6031.20 +/- 10484.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.03e+03 |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1736000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0455  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86799    |\n",
      "|    policy_loss        | 0.00137  |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86800    |\n",
      "|    time_elapsed    | 23229    |\n",
      "|    total_timesteps | 1736000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1738000, episode_reward=20.20 +/- 11.50\n",
      "Episode length: 6017.40 +/- 10491.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1738000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0985  |\n",
      "|    explained_variance | 0.73     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86899    |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 86900    |\n",
      "|    time_elapsed    | 23311    |\n",
      "|    total_timesteps | 1738000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=20.80 +/- 7.57\n",
      "Episode length: 779.60 +/- 192.11\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 780      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 86999    |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87000    |\n",
      "|    time_elapsed    | 23323    |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1742000, episode_reward=26.20 +/- 8.73\n",
      "Episode length: 6212.80 +/- 10393.93\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.21e+03 |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1742000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.068   |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87099    |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    value_loss         | 0.18     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 24.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87100    |\n",
      "|    time_elapsed    | 23404    |\n",
      "|    total_timesteps | 1742000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1744000, episode_reward=21.60 +/- 7.74\n",
      "Episode length: 818.60 +/- 136.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 819      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1744000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0822  |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87199    |\n",
      "|    policy_loss        | -0.219   |\n",
      "|    value_loss         | 1.31     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 24.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87200    |\n",
      "|    time_elapsed    | 23415    |\n",
      "|    total_timesteps | 1744000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1746000, episode_reward=23.40 +/- 3.14\n",
      "Episode length: 888.20 +/- 54.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 888      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1746000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87299    |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 0.277    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87300    |\n",
      "|    time_elapsed    | 23428    |\n",
      "|    total_timesteps | 1746000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1748000, episode_reward=19.60 +/- 6.09\n",
      "Episode length: 787.80 +/- 141.29\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1748000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87399    |\n",
      "|    policy_loss        | -0.0749  |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87400    |\n",
      "|    time_elapsed    | 23441    |\n",
      "|    total_timesteps | 1748000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=26.60 +/- 9.95\n",
      "Episode length: 858.80 +/- 168.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 859      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.107   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87499    |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.315    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87500    |\n",
      "|    time_elapsed    | 23453    |\n",
      "|    total_timesteps | 1750000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1752000, episode_reward=22.80 +/- 5.71\n",
      "Episode length: 748.00 +/- 138.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 748      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1752000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0496  |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87599    |\n",
      "|    policy_loss        | -0.00466 |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 25.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87600    |\n",
      "|    time_elapsed    | 23465    |\n",
      "|    total_timesteps | 1752000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1754000, episode_reward=26.80 +/- 9.70\n",
      "Episode length: 6132.60 +/- 10434.13\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.13e+03 |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1754000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00714 |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87699    |\n",
      "|    policy_loss        | -0.0759  |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87700    |\n",
      "|    time_elapsed    | 23544    |\n",
      "|    total_timesteps | 1754000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1756000, episode_reward=24.80 +/- 5.42\n",
      "Episode length: 6075.20 +/- 10463.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1756000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00143 |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87799    |\n",
      "|    policy_loss        | -1.8e-05 |\n",
      "|    value_loss         | 0.483    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87800    |\n",
      "|    time_elapsed    | 23623    |\n",
      "|    total_timesteps | 1756000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1758000, episode_reward=27.60 +/- 5.89\n",
      "Episode length: 6106.20 +/- 10447.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.11e+03 |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1758000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0315  |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87899    |\n",
      "|    policy_loss        | -0.00139 |\n",
      "|    value_loss         | 0.0969   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 87900    |\n",
      "|    time_elapsed    | 23702    |\n",
      "|    total_timesteps | 1758000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=21.00 +/- 6.36\n",
      "Episode length: 6082.20 +/- 10459.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 87999    |\n",
      "|    policy_loss        | -0.0459  |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 88000    |\n",
      "|    time_elapsed    | 23784    |\n",
      "|    total_timesteps | 1760000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1762000, episode_reward=25.00 +/- 6.87\n",
      "Episode length: 845.40 +/- 139.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 845      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1762000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | 0.684    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88099    |\n",
      "|    policy_loss        | 0.0169   |\n",
      "|    value_loss         | 0.0809   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88100    |\n",
      "|    time_elapsed    | 23797    |\n",
      "|    total_timesteps | 1762000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1764000, episode_reward=23.00 +/- 9.12\n",
      "Episode length: 880.40 +/- 204.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 880      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1764000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88199    |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    value_loss         | 0.0338   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88200    |\n",
      "|    time_elapsed    | 23810    |\n",
      "|    total_timesteps | 1764000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1766000, episode_reward=23.40 +/- 4.45\n",
      "Episode length: 800.20 +/- 116.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 800      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1766000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0377  |\n",
      "|    explained_variance | 0.759    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88299    |\n",
      "|    policy_loss        | 0.124    |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88300    |\n",
      "|    time_elapsed    | 23821    |\n",
      "|    total_timesteps | 1766000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1768000, episode_reward=26.40 +/- 5.50\n",
      "Episode length: 882.60 +/- 113.52\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 883      |\n",
      "|    mean_reward        | 26.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1768000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0886  |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88399    |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.278    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88400    |\n",
      "|    time_elapsed    | 23835    |\n",
      "|    total_timesteps | 1768000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=25.00 +/- 7.77\n",
      "Episode length: 788.40 +/- 100.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 788      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00247 |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88499    |\n",
      "|    policy_loss        | 2.57e-05 |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88500    |\n",
      "|    time_elapsed    | 23847    |\n",
      "|    total_timesteps | 1770000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1772000, episode_reward=22.80 +/- 8.89\n",
      "Episode length: 814.20 +/- 93.73\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 814       |\n",
      "|    mean_reward        | 22.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1772000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00707  |\n",
      "|    explained_variance | 0.0306    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88599     |\n",
      "|    policy_loss        | -5.36e-05 |\n",
      "|    value_loss         | 0.863     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88600    |\n",
      "|    time_elapsed    | 23859    |\n",
      "|    total_timesteps | 1772000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1774000, episode_reward=24.20 +/- 5.19\n",
      "Episode length: 891.40 +/- 107.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 891      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1774000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88699    |\n",
      "|    policy_loss        | -0.00859 |\n",
      "|    value_loss         | 0.185    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88700    |\n",
      "|    time_elapsed    | 23872    |\n",
      "|    total_timesteps | 1774000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1776000, episode_reward=19.80 +/- 5.27\n",
      "Episode length: 772.60 +/- 80.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 773      |\n",
      "|    mean_reward        | 19.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1776000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0456  |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88799    |\n",
      "|    policy_loss        | 0.000887 |\n",
      "|    value_loss         | 0.0725   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88800    |\n",
      "|    time_elapsed    | 23884    |\n",
      "|    total_timesteps | 1776000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1778000, episode_reward=26.20 +/- 5.31\n",
      "Episode length: 907.40 +/- 104.91\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 907      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1778000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88899    |\n",
      "|    policy_loss        | 0.0376   |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 88900    |\n",
      "|    time_elapsed    | 23897    |\n",
      "|    total_timesteps | 1778000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=15.40 +/- 13.41\n",
      "Episode length: 11304.40 +/- 12815.99\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0863  |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 88999    |\n",
      "|    policy_loss        | -0.0451  |\n",
      "|    value_loss         | 0.252    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 89000    |\n",
      "|    time_elapsed    | 23977    |\n",
      "|    total_timesteps | 1780000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1782000, episode_reward=23.80 +/- 9.20\n",
      "Episode length: 6050.20 +/- 10475.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1782000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0215  |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89099    |\n",
      "|    policy_loss        | -0.00109 |\n",
      "|    value_loss         | 0.32     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 74       |\n",
      "|    iterations      | 89100    |\n",
      "|    time_elapsed    | 24057    |\n",
      "|    total_timesteps | 1782000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1784000, episode_reward=29.00 +/- 11.10\n",
      "Episode length: 6189.40 +/- 10406.42\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.19e+03 |\n",
      "|    mean_reward        | 29       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1784000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0933  |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89199    |\n",
      "|    policy_loss        | -0.00359 |\n",
      "|    value_loss         | 0.0838   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89200    |\n",
      "|    time_elapsed    | 24137    |\n",
      "|    total_timesteps | 1784000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1786000, episode_reward=31.00 +/- 16.19\n",
      "Episode length: 6266.60 +/- 10368.21\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.27e+03  |\n",
      "|    mean_reward        | 31        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1786000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.118    |\n",
      "|    explained_variance | 0.957     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89299     |\n",
      "|    policy_loss        | -7.39e-05 |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89300    |\n",
      "|    time_elapsed    | 24217    |\n",
      "|    total_timesteps | 1786000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1788000, episode_reward=22.40 +/- 11.53\n",
      "Episode length: 6161.60 +/- 10421.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.16e+03 |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1788000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0433  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89399    |\n",
      "|    policy_loss        | 0.000747 |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 26       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89400    |\n",
      "|    time_elapsed    | 24298    |\n",
      "|    total_timesteps | 1788000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=20.40 +/- 8.21\n",
      "Episode length: 765.60 +/- 178.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 766      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0982  |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89499    |\n",
      "|    policy_loss        | -0.762   |\n",
      "|    value_loss         | 0.683    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89500    |\n",
      "|    time_elapsed    | 24310    |\n",
      "|    total_timesteps | 1790000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1792000, episode_reward=26.80 +/- 5.88\n",
      "Episode length: 867.80 +/- 153.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1792000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0284  |\n",
      "|    explained_variance | 0.853    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89599    |\n",
      "|    policy_loss        | 0.00544  |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89600    |\n",
      "|    time_elapsed    | 24323    |\n",
      "|    total_timesteps | 1792000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1794000, episode_reward=26.60 +/- 8.55\n",
      "Episode length: 6124.60 +/- 10438.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.12e+03 |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1794000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0525  |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89699    |\n",
      "|    policy_loss        | 0.000104 |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89700    |\n",
      "|    time_elapsed    | 24403    |\n",
      "|    total_timesteps | 1794000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1796000, episode_reward=22.60 +/- 8.96\n",
      "Episode length: 6050.40 +/- 10475.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 22.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1796000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0766  |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89799    |\n",
      "|    policy_loss        | 0.00907  |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89800    |\n",
      "|    time_elapsed    | 24484    |\n",
      "|    total_timesteps | 1796000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1798000, episode_reward=25.40 +/- 3.83\n",
      "Episode length: 916.40 +/- 91.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 916      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1798000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00177 |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 89899    |\n",
      "|    policy_loss        | 8.28e-05 |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 89900    |\n",
      "|    time_elapsed    | 24495    |\n",
      "|    total_timesteps | 1798000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=32.40 +/- 7.96\n",
      "Episode length: 1010.80 +/- 156.40\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 1.01e+03  |\n",
      "|    mean_reward        | 32.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1800000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0015   |\n",
      "|    explained_variance | 0.848     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89999     |\n",
      "|    policy_loss        | -0.000261 |\n",
      "|    value_loss         | 0.401     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90000    |\n",
      "|    time_elapsed    | 24508    |\n",
      "|    total_timesteps | 1800000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1802000, episode_reward=21.00 +/- 6.42\n",
      "Episode length: 5966.00 +/- 10517.64\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 5.97e+03 |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1802000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0478  |\n",
      "|    explained_variance | 0.554    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90099    |\n",
      "|    policy_loss        | -0.00506 |\n",
      "|    value_loss         | 0.0971   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90100    |\n",
      "|    time_elapsed    | 24587    |\n",
      "|    total_timesteps | 1802000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1804000, episode_reward=22.40 +/- 11.98\n",
      "Episode length: 6112.80 +/- 10443.74\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.11e+03  |\n",
      "|    mean_reward        | 22.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1804000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00691  |\n",
      "|    explained_variance | 0.601     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90199     |\n",
      "|    policy_loss        | -0.000371 |\n",
      "|    value_loss         | 0.305     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90200    |\n",
      "|    time_elapsed    | 24667    |\n",
      "|    total_timesteps | 1804000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1806000, episode_reward=29.40 +/- 8.96\n",
      "Episode length: 939.60 +/- 180.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 940      |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1806000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0922  |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90299    |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    value_loss         | 0.279    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90300    |\n",
      "|    time_elapsed    | 24678    |\n",
      "|    total_timesteps | 1806000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1808000, episode_reward=21.00 +/- 5.29\n",
      "Episode length: 789.00 +/- 97.83\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 789       |\n",
      "|    mean_reward        | 21        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1808000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.32e-05 |\n",
      "|    explained_variance | 0.981     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90399     |\n",
      "|    policy_loss        | 2.33e-08  |\n",
      "|    value_loss         | 0.0372    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90400    |\n",
      "|    time_elapsed    | 24691    |\n",
      "|    total_timesteps | 1808000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=26.80 +/- 9.17\n",
      "Episode length: 933.20 +/- 228.06\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 933      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.201   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90499    |\n",
      "|    policy_loss        | -0.0425  |\n",
      "|    value_loss         | 0.0412   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90500    |\n",
      "|    time_elapsed    | 24704    |\n",
      "|    total_timesteps | 1810000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1812000, episode_reward=26.20 +/- 8.63\n",
      "Episode length: 879.60 +/- 148.31\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 880      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1812000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0578  |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90599    |\n",
      "|    policy_loss        | -0.0477  |\n",
      "|    value_loss         | 0.0114   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90600    |\n",
      "|    time_elapsed    | 24717    |\n",
      "|    total_timesteps | 1812000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1814000, episode_reward=23.40 +/- 7.03\n",
      "Episode length: 814.40 +/- 211.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 814      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1814000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0589  |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90699    |\n",
      "|    policy_loss        | -0.321   |\n",
      "|    value_loss         | 0.461    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90700    |\n",
      "|    time_elapsed    | 24730    |\n",
      "|    total_timesteps | 1814000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1816000, episode_reward=19.60 +/- 10.05\n",
      "Episode length: 6070.00 +/- 10465.38\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1816000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0534  |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90799    |\n",
      "|    policy_loss        | 0.134    |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90800    |\n",
      "|    time_elapsed    | 24811    |\n",
      "|    total_timesteps | 1816000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1818000, episode_reward=16.20 +/- 8.06\n",
      "Episode length: 6050.60 +/- 10474.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1818000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0964  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90899    |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 90900    |\n",
      "|    time_elapsed    | 24894    |\n",
      "|    total_timesteps | 1818000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=24.80 +/- 7.36\n",
      "Episode length: 830.80 +/- 188.51\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 831      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.578    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 90999    |\n",
      "|    policy_loss        | -0.145   |\n",
      "|    value_loss         | 0.274    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 91000    |\n",
      "|    time_elapsed    | 24906    |\n",
      "|    total_timesteps | 1820000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1822000, episode_reward=25.00 +/- 11.92\n",
      "Episode length: 870.80 +/- 242.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 871      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1822000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0852  |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91099    |\n",
      "|    policy_loss        | -0.00172 |\n",
      "|    value_loss         | 0.0305   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 27.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 91100    |\n",
      "|    time_elapsed    | 24919    |\n",
      "|    total_timesteps | 1822000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1824000, episode_reward=21.00 +/- 10.55\n",
      "Episode length: 6099.40 +/- 10450.77\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1824000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0538  |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91199    |\n",
      "|    policy_loss        | 0.0183   |\n",
      "|    value_loss         | 0.257    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 91200    |\n",
      "|    time_elapsed    | 25000    |\n",
      "|    total_timesteps | 1824000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1826000, episode_reward=24.40 +/- 5.82\n",
      "Episode length: 867.00 +/- 97.94\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 867      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1826000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00195 |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91299    |\n",
      "|    policy_loss        | 1.67e-05 |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 91300    |\n",
      "|    time_elapsed    | 25013    |\n",
      "|    total_timesteps | 1826000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1828000, episode_reward=26.00 +/- 6.69\n",
      "Episode length: 927.20 +/- 169.81\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 927      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1828000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0524  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91399    |\n",
      "|    policy_loss        | 0.00108  |\n",
      "|    value_loss         | 0.0968   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 91400    |\n",
      "|    time_elapsed    | 25027    |\n",
      "|    total_timesteps | 1828000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=18.20 +/- 4.71\n",
      "Episode length: 6010.20 +/- 10494.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.01e+03 |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00187 |\n",
      "|    explained_variance | 0.688    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91499    |\n",
      "|    policy_loss        | 0.000115 |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 91500    |\n",
      "|    time_elapsed    | 25107    |\n",
      "|    total_timesteps | 1830000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1832000, episode_reward=25.40 +/- 8.98\n",
      "Episode length: 848.80 +/- 147.66\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 849      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1832000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0935  |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91599    |\n",
      "|    policy_loss        | 0.0361   |\n",
      "|    value_loss         | 0.327    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 91600    |\n",
      "|    time_elapsed    | 25120    |\n",
      "|    total_timesteps | 1832000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1834000, episode_reward=26.60 +/- 7.96\n",
      "Episode length: 925.80 +/- 79.45\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 926      |\n",
      "|    mean_reward        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1834000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0371  |\n",
      "|    explained_variance | 0.926    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91699    |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    value_loss         | 0.0837   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 91700    |\n",
      "|    time_elapsed    | 25133    |\n",
      "|    total_timesteps | 1834000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1836000, episode_reward=25.00 +/- 7.95\n",
      "Episode length: 902.00 +/- 207.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 902      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1836000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0298  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91799    |\n",
      "|    policy_loss        | -0.00213 |\n",
      "|    value_loss         | 0.129    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 91800    |\n",
      "|    time_elapsed    | 25146    |\n",
      "|    total_timesteps | 1836000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1838000, episode_reward=30.80 +/- 7.96\n",
      "Episode length: 938.60 +/- 82.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 939      |\n",
      "|    mean_reward        | 30.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1838000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0657  |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91899    |\n",
      "|    policy_loss        | 0.00732  |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 28.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 91900    |\n",
      "|    time_elapsed    | 25159    |\n",
      "|    total_timesteps | 1838000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=25.20 +/- 6.62\n",
      "Episode length: 797.60 +/- 100.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 798      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0816  |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 91999    |\n",
      "|    policy_loss        | 0.00284  |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 28       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92000    |\n",
      "|    time_elapsed    | 25171    |\n",
      "|    total_timesteps | 1840000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1842000, episode_reward=26.80 +/- 5.49\n",
      "Episode length: 926.40 +/- 125.93\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 926       |\n",
      "|    mean_reward        | 26.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1842000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00562  |\n",
      "|    explained_variance | 0.682     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92099     |\n",
      "|    policy_loss        | -0.000506 |\n",
      "|    value_loss         | 0.425     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92100    |\n",
      "|    time_elapsed    | 25184    |\n",
      "|    total_timesteps | 1842000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1844000, episode_reward=27.60 +/- 5.75\n",
      "Episode length: 808.80 +/- 141.70\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 809      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1844000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.167   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92199    |\n",
      "|    policy_loss        | 0.169    |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92200    |\n",
      "|    time_elapsed    | 25197    |\n",
      "|    total_timesteps | 1844000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1846000, episode_reward=27.60 +/- 7.34\n",
      "Episode length: 872.80 +/- 149.61\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 873      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1846000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92299    |\n",
      "|    policy_loss        | 0.0253   |\n",
      "|    value_loss         | 0.392    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 27.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92300    |\n",
      "|    time_elapsed    | 25209    |\n",
      "|    total_timesteps | 1846000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1848000, episode_reward=23.00 +/- 12.49\n",
      "Episode length: 6142.00 +/- 10429.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.14e+03 |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1848000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0642  |\n",
      "|    explained_variance | -0.0358  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92399    |\n",
      "|    policy_loss        | 0.0616   |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92400    |\n",
      "|    time_elapsed    | 25290    |\n",
      "|    total_timesteps | 1848000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=23.80 +/- 14.51\n",
      "Episode length: 6213.60 +/- 10394.44\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.21e+03 |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0796  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92499    |\n",
      "|    policy_loss        | 0.0672   |\n",
      "|    value_loss         | 0.0973   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 92500    |\n",
      "|    time_elapsed    | 25371    |\n",
      "|    total_timesteps | 1850000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1852000, episode_reward=26.80 +/- 10.93\n",
      "Episode length: 924.20 +/- 202.14\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 924      |\n",
      "|    mean_reward        | 26.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1852000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00406 |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92599    |\n",
      "|    policy_loss        | 0.000196 |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 28.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 92600    |\n",
      "|    time_elapsed    | 25385    |\n",
      "|    total_timesteps | 1852000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1854000, episode_reward=31.40 +/- 9.37\n",
      "Episode length: 938.60 +/- 223.39\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 939      |\n",
      "|    mean_reward        | 31.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1854000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0881  |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92699    |\n",
      "|    policy_loss        | 0.0343   |\n",
      "|    value_loss         | 0.0675   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 27.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 92700    |\n",
      "|    time_elapsed    | 25399    |\n",
      "|    total_timesteps | 1854000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1856000, episode_reward=27.20 +/- 4.31\n",
      "Episode length: 884.80 +/- 87.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 885      |\n",
      "|    mean_reward        | 27.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1856000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.065   |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 92799    |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    value_loss         | 0.278    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92800    |\n",
      "|    time_elapsed    | 25412    |\n",
      "|    total_timesteps | 1856000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1858000, episode_reward=28.60 +/- 10.27\n",
      "Episode length: 875.20 +/- 196.35\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 875       |\n",
      "|    mean_reward        | 28.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1858000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0268   |\n",
      "|    explained_variance | 0.803     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92899     |\n",
      "|    policy_loss        | -0.000531 |\n",
      "|    value_loss         | 0.176     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 92900    |\n",
      "|    time_elapsed    | 25424    |\n",
      "|    total_timesteps | 1858000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=24.80 +/- 6.62\n",
      "Episode length: 6132.00 +/- 10435.11\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.13e+03  |\n",
      "|    mean_reward        | 24.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1860000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00591  |\n",
      "|    explained_variance | 0.925     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92999     |\n",
      "|    policy_loss        | -0.000106 |\n",
      "|    value_loss         | 0.16      |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 93000    |\n",
      "|    time_elapsed    | 25502    |\n",
      "|    total_timesteps | 1860000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1862000, episode_reward=31.20 +/- 5.46\n",
      "Episode length: 899.80 +/- 124.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 900      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1862000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0347  |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93099    |\n",
      "|    policy_loss        | 0.038    |\n",
      "|    value_loss         | 0.0532   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 93100    |\n",
      "|    time_elapsed    | 25515    |\n",
      "|    total_timesteps | 1862000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1864000, episode_reward=27.40 +/- 13.88\n",
      "Episode length: 948.20 +/- 241.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 948      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1864000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0639  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93199    |\n",
      "|    policy_loss        | -0.00302 |\n",
      "|    value_loss         | 0.0516   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93200    |\n",
      "|    time_elapsed    | 25527    |\n",
      "|    total_timesteps | 1864000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1866000, episode_reward=25.20 +/- 6.24\n",
      "Episode length: 888.80 +/- 134.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 889      |\n",
      "|    mean_reward        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1866000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.165   |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93299    |\n",
      "|    policy_loss        | -0.633   |\n",
      "|    value_loss         | 0.468    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93300    |\n",
      "|    time_elapsed    | 25540    |\n",
      "|    total_timesteps | 1866000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1868000, episode_reward=23.40 +/- 5.46\n",
      "Episode length: 860.40 +/- 68.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 860       |\n",
      "|    mean_reward        | 23.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1868000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0121   |\n",
      "|    explained_variance | 0.892     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93399     |\n",
      "|    policy_loss        | -0.000323 |\n",
      "|    value_loss         | 0.141     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93400    |\n",
      "|    time_elapsed    | 25553    |\n",
      "|    total_timesteps | 1868000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=30.20 +/- 3.49\n",
      "Episode length: 939.00 +/- 104.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 939      |\n",
      "|    mean_reward        | 30.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0725  |\n",
      "|    explained_variance | 0.868    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93499    |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.203    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93500    |\n",
      "|    time_elapsed    | 25566    |\n",
      "|    total_timesteps | 1870000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1872000, episode_reward=27.80 +/- 8.18\n",
      "Episode length: 964.80 +/- 178.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 965      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1872000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.854    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93599    |\n",
      "|    policy_loss        | 0.0653   |\n",
      "|    value_loss         | 0.403    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 25.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93600    |\n",
      "|    time_elapsed    | 25578    |\n",
      "|    total_timesteps | 1872000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1874000, episode_reward=25.60 +/- 3.44\n",
      "Episode length: 872.00 +/- 99.96\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 872      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1874000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0694  |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93699    |\n",
      "|    policy_loss        | 0.00383  |\n",
      "|    value_loss         | 0.0417   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93700    |\n",
      "|    time_elapsed    | 25591    |\n",
      "|    total_timesteps | 1874000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1876000, episode_reward=27.60 +/- 4.13\n",
      "Episode length: 900.20 +/- 34.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 900      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1876000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0319  |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93799    |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    value_loss         | 0.072    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93800    |\n",
      "|    time_elapsed    | 25604    |\n",
      "|    total_timesteps | 1876000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1878000, episode_reward=29.40 +/- 6.05\n",
      "Episode length: 1010.20 +/- 133.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.01e+03 |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1878000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0944  |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93899    |\n",
      "|    policy_loss        | -0.0227  |\n",
      "|    value_loss         | 0.602    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 93900    |\n",
      "|    time_elapsed    | 25617    |\n",
      "|    total_timesteps | 1878000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=25.80 +/- 7.03\n",
      "Episode length: 918.80 +/- 162.24\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 919      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0387  |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 93999    |\n",
      "|    policy_loss        | -0.00908 |\n",
      "|    value_loss         | 0.471    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94000    |\n",
      "|    time_elapsed    | 25630    |\n",
      "|    total_timesteps | 1880000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1882000, episode_reward=19.60 +/- 4.76\n",
      "Episode length: 6050.40 +/- 10475.33\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1882000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0658  |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94099    |\n",
      "|    policy_loss        | -0.0512  |\n",
      "|    value_loss         | 0.0804   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 25.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94100    |\n",
      "|    time_elapsed    | 25710    |\n",
      "|    total_timesteps | 1882000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1884000, episode_reward=20.80 +/- 7.86\n",
      "Episode length: 740.60 +/- 142.23\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 741      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1884000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0657  |\n",
      "|    explained_variance | 0.9      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94199    |\n",
      "|    policy_loss        | -0.00245 |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94200    |\n",
      "|    time_elapsed    | 25723    |\n",
      "|    total_timesteps | 1884000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1886000, episode_reward=26.40 +/- 9.00\n",
      "Episode length: 868.80 +/- 168.62\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 869       |\n",
      "|    mean_reward        | 26.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1886000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000736 |\n",
      "|    explained_variance | 0.989     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94299     |\n",
      "|    policy_loss        | 1.42e-05  |\n",
      "|    value_loss         | 0.0253    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 25.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94300    |\n",
      "|    time_elapsed    | 25736    |\n",
      "|    total_timesteps | 1886000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1888000, episode_reward=27.80 +/- 6.65\n",
      "Episode length: 944.60 +/- 142.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 945      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1888000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0295  |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94399    |\n",
      "|    policy_loss        | -0.00251 |\n",
      "|    value_loss         | 0.0807   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94400    |\n",
      "|    time_elapsed    | 25748    |\n",
      "|    total_timesteps | 1888000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=20.20 +/- 7.57\n",
      "Episode length: 6018.00 +/- 10491.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.02e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0591  |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94499    |\n",
      "|    policy_loss        | -0.0208  |\n",
      "|    value_loss         | 0.398    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 25.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94500    |\n",
      "|    time_elapsed    | 25828    |\n",
      "|    total_timesteps | 1890000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1892000, episode_reward=24.20 +/- 16.08\n",
      "Episode length: 6125.20 +/- 10439.37\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.13e+03  |\n",
      "|    mean_reward        | 24.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1892000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0146   |\n",
      "|    explained_variance | 0.984     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94599     |\n",
      "|    policy_loss        | -0.000156 |\n",
      "|    value_loss         | 0.0611    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94600    |\n",
      "|    time_elapsed    | 25913    |\n",
      "|    total_timesteps | 1892000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1894000, episode_reward=30.20 +/- 8.23\n",
      "Episode length: 891.80 +/- 253.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 892      |\n",
      "|    mean_reward        | 30.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1894000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0616  |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94699    |\n",
      "|    policy_loss        | -0.062   |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 94700    |\n",
      "|    time_elapsed    | 25926    |\n",
      "|    total_timesteps | 1894000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1896000, episode_reward=23.20 +/- 6.08\n",
      "Episode length: 6067.80 +/- 10466.79\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1896000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0309  |\n",
      "|    explained_variance | -0.0846  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94799    |\n",
      "|    policy_loss        | 0.00106  |\n",
      "|    value_loss         | 2.97     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 94800    |\n",
      "|    time_elapsed    | 26005    |\n",
      "|    total_timesteps | 1896000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1898000, episode_reward=16.80 +/- 9.41\n",
      "Episode length: 11300.00 +/- 12819.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.13e+04 |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1898000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0833  |\n",
      "|    explained_variance | 0.373    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 94899    |\n",
      "|    policy_loss        | 0.0559   |\n",
      "|    value_loss         | 0.247    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 94900    |\n",
      "|    time_elapsed    | 26085    |\n",
      "|    total_timesteps | 1898000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=25.00 +/- 6.63\n",
      "Episode length: 887.80 +/- 131.71\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 888       |\n",
      "|    mean_reward        | 25        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1900000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00826  |\n",
      "|    explained_variance | 0.963     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94999     |\n",
      "|    policy_loss        | -0.000147 |\n",
      "|    value_loss         | 0.0705    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95000    |\n",
      "|    time_elapsed    | 26096    |\n",
      "|    total_timesteps | 1900000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1902000, episode_reward=27.60 +/- 8.26\n",
      "Episode length: 925.20 +/- 132.16\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 925      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1902000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0582  |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95099    |\n",
      "|    policy_loss        | -0.0399  |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 26.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95100    |\n",
      "|    time_elapsed    | 26108    |\n",
      "|    total_timesteps | 1902000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1904000, episode_reward=24.40 +/- 5.85\n",
      "Episode length: 868.80 +/- 138.09\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 869      |\n",
      "|    mean_reward        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1904000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0345  |\n",
      "|    explained_variance | 0.681    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95199    |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 0.485    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95200    |\n",
      "|    time_elapsed    | 26121    |\n",
      "|    total_timesteps | 1904000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1906000, episode_reward=19.00 +/- 8.05\n",
      "Episode length: 6083.80 +/- 10458.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.08e+03 |\n",
      "|    mean_reward        | 19       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1906000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00989 |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95299    |\n",
      "|    policy_loss        | 0.0015   |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95300    |\n",
      "|    time_elapsed    | 26200    |\n",
      "|    total_timesteps | 1906000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1908000, episode_reward=23.80 +/- 10.57\n",
      "Episode length: 910.60 +/- 223.60\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 911       |\n",
      "|    mean_reward        | 23.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1908000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000632 |\n",
      "|    explained_variance | 0.75      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95399     |\n",
      "|    policy_loss        | 6.12e-05  |\n",
      "|    value_loss         | 0.166     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95400    |\n",
      "|    time_elapsed    | 26211    |\n",
      "|    total_timesteps | 1908000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=23.60 +/- 3.50\n",
      "Episode length: 867.80 +/- 113.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 868      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.1     |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95499    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.0778   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95500    |\n",
      "|    time_elapsed    | 26224    |\n",
      "|    total_timesteps | 1910000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1912000, episode_reward=22.20 +/- 11.23\n",
      "Episode length: 6156.00 +/- 10422.19\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.16e+03 |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1912000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0368  |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95599    |\n",
      "|    policy_loss        | 0.00536  |\n",
      "|    value_loss         | 0.0574   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95600    |\n",
      "|    time_elapsed    | 26305    |\n",
      "|    total_timesteps | 1912000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1914000, episode_reward=19.40 +/- 6.50\n",
      "Episode length: 809.00 +/- 177.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 809      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1914000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00876 |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95699    |\n",
      "|    policy_loss        | 0.000102 |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95700    |\n",
      "|    time_elapsed    | 26316    |\n",
      "|    total_timesteps | 1914000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1916000, episode_reward=21.40 +/- 3.93\n",
      "Episode length: 813.20 +/- 83.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 813      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1916000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0689  |\n",
      "|    explained_variance | 0.286    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95799    |\n",
      "|    policy_loss        | -0.0216  |\n",
      "|    value_loss         | 1.6      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95800    |\n",
      "|    time_elapsed    | 26328    |\n",
      "|    total_timesteps | 1916000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1918000, episode_reward=26.60 +/- 9.26\n",
      "Episode length: 918.20 +/- 170.57\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 918       |\n",
      "|    mean_reward        | 26.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1918000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000597 |\n",
      "|    explained_variance | 0.975     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95899     |\n",
      "|    policy_loss        | 5.05e-05  |\n",
      "|    value_loss         | 0.144     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 95900    |\n",
      "|    time_elapsed    | 26341    |\n",
      "|    total_timesteps | 1918000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=22.00 +/- 5.44\n",
      "Episode length: 786.60 +/- 101.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 787      |\n",
      "|    mean_reward        | 22       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0347  |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 95999    |\n",
      "|    policy_loss        | -0.0107  |\n",
      "|    value_loss         | 0.323    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96000    |\n",
      "|    time_elapsed    | 26352    |\n",
      "|    total_timesteps | 1920000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1922000, episode_reward=23.00 +/- 5.97\n",
      "Episode length: 812.40 +/- 148.47\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 812      |\n",
      "|    mean_reward        | 23       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1922000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00181 |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96099    |\n",
      "|    policy_loss        | 1.05e-05 |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 26.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96100    |\n",
      "|    time_elapsed    | 26364    |\n",
      "|    total_timesteps | 1922000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1924000, episode_reward=25.00 +/- 6.78\n",
      "Episode length: 833.60 +/- 64.46\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 834       |\n",
      "|    mean_reward        | 25        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1924000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0118   |\n",
      "|    explained_variance | 0.944     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96199     |\n",
      "|    policy_loss        | -0.000409 |\n",
      "|    value_loss         | 0.0396    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96200    |\n",
      "|    time_elapsed    | 26377    |\n",
      "|    total_timesteps | 1924000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1926000, episode_reward=23.20 +/- 11.86\n",
      "Episode length: 734.20 +/- 85.15\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 734      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1926000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0646  |\n",
      "|    explained_variance | 0.925    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96299    |\n",
      "|    policy_loss        | -0.0186  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 26.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96300    |\n",
      "|    time_elapsed    | 26389    |\n",
      "|    total_timesteps | 1926000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1928000, episode_reward=24.60 +/- 8.80\n",
      "Episode length: 842.80 +/- 165.48\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 843      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1928000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0389  |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96399    |\n",
      "|    policy_loss        | 0.00382  |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 96400    |\n",
      "|    time_elapsed    | 26401    |\n",
      "|    total_timesteps | 1928000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=24.80 +/- 9.81\n",
      "Episode length: 6099.60 +/- 10451.94\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 6.1e+03   |\n",
      "|    mean_reward        | 24.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1930000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0158   |\n",
      "|    explained_variance | 0.984     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96499     |\n",
      "|    policy_loss        | -0.000891 |\n",
      "|    value_loss         | 0.0891    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 26.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96500    |\n",
      "|    time_elapsed    | 26480    |\n",
      "|    total_timesteps | 1930000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1932000, episode_reward=28.00 +/- 2.45\n",
      "Episode length: 908.00 +/- 46.92\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 908       |\n",
      "|    mean_reward        | 28        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1932000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0141   |\n",
      "|    explained_variance | 0.978     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96599     |\n",
      "|    policy_loss        | -4.01e-05 |\n",
      "|    value_loss         | 0.025     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96600    |\n",
      "|    time_elapsed    | 26492    |\n",
      "|    total_timesteps | 1932000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1934000, episode_reward=25.60 +/- 9.07\n",
      "Episode length: 837.00 +/- 92.74\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 837      |\n",
      "|    mean_reward        | 25.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1934000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.119   |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96699    |\n",
      "|    policy_loss        | 0.0171   |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96700    |\n",
      "|    time_elapsed    | 26506    |\n",
      "|    total_timesteps | 1934000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1936000, episode_reward=22.80 +/- 8.47\n",
      "Episode length: 6089.20 +/- 10456.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.09e+03 |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1936000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0904  |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96799    |\n",
      "|    policy_loss        | -0.0189  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 26.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96800    |\n",
      "|    time_elapsed    | 26586    |\n",
      "|    total_timesteps | 1936000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1938000, episode_reward=25.00 +/- 6.54\n",
      "Episode length: 900.80 +/- 199.40\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 901      |\n",
      "|    mean_reward        | 25       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1938000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96899    |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0431   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 96900    |\n",
      "|    time_elapsed    | 26598    |\n",
      "|    total_timesteps | 1938000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=31.60 +/- 11.25\n",
      "Episode length: 984.00 +/- 218.04\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 984      |\n",
      "|    mean_reward        | 31.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 96999    |\n",
      "|    policy_loss        | 0.105    |\n",
      "|    value_loss         | 0.346    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97000    |\n",
      "|    time_elapsed    | 26610    |\n",
      "|    total_timesteps | 1940000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1942000, episode_reward=24.60 +/- 7.61\n",
      "Episode length: 848.40 +/- 128.43\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 848      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1942000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.227   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97099    |\n",
      "|    policy_loss        | -0.0641  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97100    |\n",
      "|    time_elapsed    | 26622    |\n",
      "|    total_timesteps | 1942000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1944000, episode_reward=27.60 +/- 7.76\n",
      "Episode length: 890.60 +/- 158.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 891      |\n",
      "|    mean_reward        | 27.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1944000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.172   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97199    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.11     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97200    |\n",
      "|    time_elapsed    | 26635    |\n",
      "|    total_timesteps | 1944000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1946000, episode_reward=22.20 +/- 12.77\n",
      "Episode length: 6073.20 +/- 10464.37\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.07e+03 |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1946000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0746  |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97299    |\n",
      "|    policy_loss        | 0.0117   |\n",
      "|    value_loss         | 0.0856   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97300    |\n",
      "|    time_elapsed    | 26716    |\n",
      "|    total_timesteps | 1946000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1948000, episode_reward=26.60 +/- 5.57\n",
      "Episode length: 893.20 +/- 161.13\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 893       |\n",
      "|    mean_reward        | 26.6      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1948000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0161   |\n",
      "|    explained_variance | 0.984     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97399     |\n",
      "|    policy_loss        | -0.000119 |\n",
      "|    value_loss         | 0.0814    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97400    |\n",
      "|    time_elapsed    | 26728    |\n",
      "|    total_timesteps | 1948000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=29.60 +/- 2.06\n",
      "Episode length: 958.20 +/- 80.46\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 958      |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0464  |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97499    |\n",
      "|    policy_loss        | -0.0242  |\n",
      "|    value_loss         | 0.0497   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97500    |\n",
      "|    time_elapsed    | 26741    |\n",
      "|    total_timesteps | 1950000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1952000, episode_reward=25.80 +/- 7.14\n",
      "Episode length: 858.40 +/- 95.84\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 858      |\n",
      "|    mean_reward        | 25.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1952000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97599    |\n",
      "|    policy_loss        | 0.0427   |\n",
      "|    value_loss         | 0.0818   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 97600    |\n",
      "|    time_elapsed    | 26754    |\n",
      "|    total_timesteps | 1952000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1954000, episode_reward=26.20 +/- 7.08\n",
      "Episode length: 952.40 +/- 211.65\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 952      |\n",
      "|    mean_reward        | 26.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1954000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.674    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97699    |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.357    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 922      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 97700    |\n",
      "|    time_elapsed    | 26766    |\n",
      "|    total_timesteps | 1954000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1956000, episode_reward=24.00 +/- 6.07\n",
      "Episode length: 821.80 +/- 132.71\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 822       |\n",
      "|    mean_reward        | 24        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1956000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000167 |\n",
      "|    explained_variance | 0.796     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97799     |\n",
      "|    policy_loss        | -3.21e-06 |\n",
      "|    value_loss         | 0.222     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 97800    |\n",
      "|    time_elapsed    | 26778    |\n",
      "|    total_timesteps | 1956000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1958000, episode_reward=20.80 +/- 3.76\n",
      "Episode length: 790.20 +/- 53.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 790      |\n",
      "|    mean_reward        | 20.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1958000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0907  |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97899    |\n",
      "|    policy_loss        | 0.00393  |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 97900    |\n",
      "|    time_elapsed    | 26790    |\n",
      "|    total_timesteps | 1958000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=20.20 +/- 6.18\n",
      "Episode length: 6052.40 +/- 10473.92\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.05e+03 |\n",
      "|    mean_reward        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0577  |\n",
      "|    explained_variance | 0.79     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 97999    |\n",
      "|    policy_loss        | -0.00683 |\n",
      "|    value_loss         | 0.0639   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 922      |\n",
      "|    ep_rew_mean     | 27.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 98000    |\n",
      "|    time_elapsed    | 26870    |\n",
      "|    total_timesteps | 1960000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1962000, episode_reward=22.40 +/- 4.59\n",
      "Episode length: 877.80 +/- 113.30\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 878      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1962000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0684  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98099    |\n",
      "|    policy_loss        | 0.0194   |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 98100    |\n",
      "|    time_elapsed    | 26881    |\n",
      "|    total_timesteps | 1962000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1964000, episode_reward=24.20 +/- 7.52\n",
      "Episode length: 857.20 +/- 183.02\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 857      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1964000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98199    |\n",
      "|    policy_loss        | 0.00736  |\n",
      "|    value_loss         | 0.0667   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98200    |\n",
      "|    time_elapsed    | 26893    |\n",
      "|    total_timesteps | 1964000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1966000, episode_reward=31.20 +/- 11.30\n",
      "Episode length: 998.20 +/- 241.17\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 998      |\n",
      "|    mean_reward        | 31.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1966000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0375  |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98299    |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 27.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98300    |\n",
      "|    time_elapsed    | 26906    |\n",
      "|    total_timesteps | 1966000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1968000, episode_reward=22.40 +/- 11.11\n",
      "Episode length: 806.20 +/- 235.50\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 806      |\n",
      "|    mean_reward        | 22.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1968000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00349 |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98399    |\n",
      "|    policy_loss        | 0.000148 |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98400    |\n",
      "|    time_elapsed    | 26918    |\n",
      "|    total_timesteps | 1968000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=19.20 +/- 3.71\n",
      "Episode length: 760.40 +/- 67.34\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 760      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.368   |\n",
      "|    explained_variance | 0.826    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98499    |\n",
      "|    policy_loss        | 0.0319   |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98500    |\n",
      "|    time_elapsed    | 26930    |\n",
      "|    total_timesteps | 1970000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1972000, episode_reward=26.00 +/- 10.37\n",
      "Episode length: 862.60 +/- 114.20\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 863      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1972000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0407  |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98599    |\n",
      "|    policy_loss        | 0.00559  |\n",
      "|    value_loss         | 0.0456   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98600    |\n",
      "|    time_elapsed    | 26942    |\n",
      "|    total_timesteps | 1972000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1974000, episode_reward=11.80 +/- 9.39\n",
      "Episode length: 11229.60 +/- 12876.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 1.12e+04 |\n",
      "|    mean_reward        | 11.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1974000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0157  |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98699    |\n",
      "|    policy_loss        | 0.000254 |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 73       |\n",
      "|    iterations      | 98700    |\n",
      "|    time_elapsed    | 27022    |\n",
      "|    total_timesteps | 1974000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1976000, episode_reward=23.60 +/- 9.41\n",
      "Episode length: 6100.00 +/- 10451.26\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1976000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0119  |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98799    |\n",
      "|    policy_loss        | -0.00112 |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 98800    |\n",
      "|    time_elapsed    | 27105    |\n",
      "|    total_timesteps | 1976000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1978000, episode_reward=23.60 +/- 13.78\n",
      "Episode length: 6104.20 +/- 10448.97\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.1e+03  |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1978000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00543 |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98899    |\n",
      "|    policy_loss        | 5.59e-05 |\n",
      "|    value_loss         | 0.0796   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 98900    |\n",
      "|    time_elapsed    | 27186    |\n",
      "|    total_timesteps | 1978000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=28.60 +/- 2.24\n",
      "Episode length: 903.60 +/- 101.28\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 904      |\n",
      "|    mean_reward        | 28.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 98999    |\n",
      "|    policy_loss        | 0.0173   |\n",
      "|    value_loss         | 0.245    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99000    |\n",
      "|    time_elapsed    | 27197    |\n",
      "|    total_timesteps | 1980000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1982000, episode_reward=25.40 +/- 3.61\n",
      "Episode length: 848.60 +/- 69.56\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 849      |\n",
      "|    mean_reward        | 25.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1982000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0857  |\n",
      "|    explained_variance | -0.547   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99099    |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    value_loss         | 0.421    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99100    |\n",
      "|    time_elapsed    | 27210    |\n",
      "|    total_timesteps | 1982000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1984000, episode_reward=25.00 +/- 6.13\n",
      "Episode length: 852.20 +/- 127.81\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 852       |\n",
      "|    mean_reward        | 25        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1984000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00932  |\n",
      "|    explained_variance | 0.982     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99199     |\n",
      "|    policy_loss        | -0.000178 |\n",
      "|    value_loss         | 0.0346    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 27.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99200    |\n",
      "|    time_elapsed    | 27223    |\n",
      "|    total_timesteps | 1984000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1986000, episode_reward=23.00 +/- 4.60\n",
      "Episode length: 823.00 +/- 81.33\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 823       |\n",
      "|    mean_reward        | 23        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1986000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000208 |\n",
      "|    explained_variance | 0.829     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99299     |\n",
      "|    policy_loss        | -6.77e-06 |\n",
      "|    value_loss         | 0.0664    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99300    |\n",
      "|    time_elapsed    | 27235    |\n",
      "|    total_timesteps | 1986000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1988000, episode_reward=28.80 +/- 7.98\n",
      "Episode length: 926.40 +/- 176.07\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 926      |\n",
      "|    mean_reward        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1988000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0378  |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99399    |\n",
      "|    policy_loss        | -0.00142 |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99400    |\n",
      "|    time_elapsed    | 27248    |\n",
      "|    total_timesteps | 1988000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=21.00 +/- 3.63\n",
      "Episode length: 6057.80 +/- 10471.35\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0287  |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99499    |\n",
      "|    policy_loss        | -0.0432  |\n",
      "|    value_loss         | 0.0855   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 26.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99500    |\n",
      "|    time_elapsed    | 27328    |\n",
      "|    total_timesteps | 1990000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1992000, episode_reward=24.80 +/- 7.41\n",
      "Episode length: 924.80 +/- 213.59\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 925      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1992000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.06    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99599    |\n",
      "|    policy_loss        | -0.0365  |\n",
      "|    value_loss         | 0.203    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 916      |\n",
      "|    ep_rew_mean     | 27.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99600    |\n",
      "|    time_elapsed    | 27341    |\n",
      "|    total_timesteps | 1992000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1994000, episode_reward=27.40 +/- 7.99\n",
      "Episode length: 889.00 +/- 147.87\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 889      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1994000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00376 |\n",
      "|    explained_variance | 0.43     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99699    |\n",
      "|    policy_loss        | 0.000339 |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99700    |\n",
      "|    time_elapsed    | 27353    |\n",
      "|    total_timesteps | 1994000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1996000, episode_reward=21.80 +/- 5.42\n",
      "Episode length: 791.80 +/- 82.24\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 792       |\n",
      "|    mean_reward        | 21.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 1996000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00189  |\n",
      "|    explained_variance | 0.614     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99799     |\n",
      "|    policy_loss        | -7.99e-05 |\n",
      "|    value_loss         | 0.155     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99800    |\n",
      "|    time_elapsed    | 27365    |\n",
      "|    total_timesteps | 1996000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1998000, episode_reward=21.60 +/- 9.13\n",
      "Episode length: 6060.20 +/- 10469.98\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 6.06e+03 |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1998000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0092  |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99899    |\n",
      "|    policy_loss        | 0.000142 |\n",
      "|    value_loss         | 0.0682   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 26.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 99900    |\n",
      "|    time_elapsed    | 27445    |\n",
      "|    total_timesteps | 1998000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=31.80 +/- 11.72\n",
      "Episode length: 952.60 +/- 232.58\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 953       |\n",
      "|    mean_reward        | 31.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 2000000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0229   |\n",
      "|    explained_variance | 0.984     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99999     |\n",
      "|    policy_loss        | -0.000166 |\n",
      "|    value_loss         | 0.0362    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 72       |\n",
      "|    iterations      | 100000   |\n",
      "|    time_elapsed    | 27457    |\n",
      "|    total_timesteps | 2000000  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.a2c.a2c.A2C at 0x7f363ee17f40>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = make_atari_env(\"ALE/Breakout-v5\", n_envs=4, seed=52)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
    "                             log_path=\"./logs/\", eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "vec_env = make_atari_env(\"ALE/Breakout-v5\", n_envs=4, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "vec_env = VecFrameStack(vec_env, n_stack=4)\n",
    "vec_env = VecTransposeImage(vec_env)\n",
    "\n",
    "model = A2C(\"CnnPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=2_000_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa44d15a54c1325",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T14:20:11.001425222Z",
     "start_time": "2024-02-26T14:19:54.444081910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 15:19:57.157325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-26 15:19:57.157376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-26 15:19:57.158889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-26 15:19:57.168864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-26 15:19:58.183896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maliti/Prog/Global-Feature-Extraction/venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 0.5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from utils import render_env_with_model, evaluate_policy\n",
    "from stable_baselines3.common.vec_env import VecTransposeImage\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "\n",
    "eval_env = make_atari_env(\"ALE/Breakout-v5\", n_envs=2, seed=10)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "# Load the best model\n",
    "model = A2C.load(\"logs/best_model.zip\", env=eval_env)\n",
    "\n",
    "# render = True not working. Use function in utils instead.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=2, render=True, fps=30)\n",
    "print(mean_reward, std_reward)\n",
    "\n",
    "#render_env_with_model(eval_env, model, num_steps=40, fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b1e088494fa78e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T15:27:37.950555572Z",
     "start_time": "2024-01-31T15:27:37.940672601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render_modes': ['human', 'rgb_array'], 'render_fps': 1}\n",
      "human\n"
     ]
    }
   ],
   "source": [
    "print(eval_env.metadata)\n",
    "print(eval_env.render_mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
